import os
import numpy as np
from typing import List, Dict
import logging
import time
from ..config import settings

try:
    from pinecone import Pinecone, ServerlessSpec
except ImportError as e:
    raise ImportError("Pinecone v3+ SDK is required. Install with: pip install pinecone==3.*") from e


class PineconeManager:
    def __init__(self):
        if not settings.PINECONE_API_KEY:
            raise ValueError("Pinecone API Key missing. Set PINECONE_API_KEY in .env")

        self.logger = logging.getLogger(__name__)
        self.index_name = settings.INDEX_NAME
        self.dimension = settings.EMBEDDING_DIMENSION

        # Parse cloud/region from legacy environment string like 'us-east-1-aws'
        cloud = 'aws'
        region = 'us-east-1'
        env = getattr(settings, 'PINECONE_ENVIRONMENT', '') or ''
        if env and '-' in env:
            parts = env.split('-')
            if len(parts) >= 3:
                region = '-'.join(parts[0:3]) if len(parts) > 3 else '-'.join(parts[0:2])
                cloud = parts[-1]
            else:
                region = parts[0]
                cloud = parts[-1]

        self.pc = Pinecone(api_key=settings.PINECONE_API_KEY)
        self.cloud = cloud
        self.region = region

    def create_index(self, delete_if_exists: bool = False) -> bool:
        try:
            existing = self.pc.list_indexes()
            exists = self.index_name in existing.names()
            if exists and delete_if_exists:
                self.logger.info(f"Deleting existing index {self.index_name}...")
                self.pc.delete_index(self.index_name)
                time.sleep(5)
                exists = False

            if not exists:
                self.logger.info(f"Creating index {self.index_name} (dim={self.dimension})...")
                self.pc.create_index(
                    name=self.index_name,
                    dimension=self.dimension,
                    metric='cosine',
                    spec=ServerlessSpec(cloud=self.cloud, region=self.region)
                )
                for _ in range(60):
                    try:
                        desc = self.pc.describe_index(self.index_name)
                        st = getattr(desc, 'status', None)
                        if (isinstance(st, dict) and st.get('ready')) or getattr(st, 'ready', False):
                            break
                    except Exception:
                        pass
                    time.sleep(1)
            else:
                self.logger.info(f"Index {self.index_name} already exists")
            return True
        except Exception as e:
            self.logger.error(f"Index creation failed: {e}")
            return False

    def get_index(self):
        try:
            return self.pc.Index(self.index_name)
        except Exception as e:
            self.logger.error(f"Get index failed: {e}")
            raise

    def upload_embeddings(self, embeddings_data: Dict, batch_size: int = 100) -> bool:
        try:
            index = self.get_index()
            embeddings = embeddings_data['embeddings']
            metadata = embeddings_data['metadata']
            ids = embeddings_data['ids']

            self.logger.info(f"Uploading {len(embeddings)} embeddings to Pinecone...")
            for i in range(0, len(embeddings), batch_size):
                batch_end = min(i + batch_size, len(embeddings))
                batch = [
                    {
                        'id': ids[j],
                        'values': embeddings[j],
                        'metadata': metadata[j]
                    } for j in range(i, batch_end)
                ]
                index.upsert(vectors=batch)
                self.logger.info(f"Batch {i//batch_size + 1} upserted: {len(batch)} vectors")
            stats = index.describe_index_stats()
            self.logger.info(f"Upsert complete. Total vectors: {stats.get('total_vector_count', 0)}")
            return True
        except Exception as e:
            self.logger.error(f"Embedding upload failed: {e}")
            return False

    def search_similar_images(self, query_embedding: np.ndarray, top_k: int = 5, viewpoint: str = None) -> List[Dict]:
        try:
            index = self.get_index()

            # 필터 구성
            search_filter = {
                "gender": {"$eq": settings.DEFAULT_GENDER_FILTER}
            }

            # 뷰포인트가 지정된 경우 해당 뷰포인트만 검색
            if viewpoint:
                # 뷰포인트 매핑 (파일명에서 사용되는 형태들)
                viewpoint_mapping = {
                    'top-down': ['Top-Down', 'top-down', 'top_down', 'top'],
                    'front': ['Front', 'front'],
                    'left': ['Left', 'left'],
                    'right': ['Right', 'right'],
                    'back': ['Back', 'back'],
                    'side': ['Side', 'side']
                }

                if viewpoint.lower() in viewpoint_mapping:
                    search_filter["pointview"] = {"$in": viewpoint_mapping[viewpoint.lower()]}
                else:
                    # 직접 매칭 시도
                    search_filter["pointview"] = {"$eq": viewpoint}

            res = index.query(
                vector=query_embedding.tolist(),
                top_k=top_k,
                include_metadata=True,
                filter=search_filter
            )
            similar_images: List[Dict] = []
            matches = res.get('matches', []) if isinstance(res, dict) else getattr(res, 'matches', [])
            for m in matches:
                md = m['metadata'] if isinstance(m, dict) else getattr(m, 'metadata', {})
                similar_images.append({
                    'id': m['id'] if isinstance(m, dict) else getattr(m, 'id', None),
                    'score': m['score'] if isinstance(m, dict) else float(getattr(m, 'score', 0.0)),
                    'stage': md.get('stage'),
                    'filename': md.get('filename'),
                    'path': md.get('path')
                })
            return similar_images
        except Exception as e:
            self.logger.error(f"Similar image search failed: {e}")
            return []

    def predict_hair_loss_stage(self, query_embedding: np.ndarray, top_k: int = 10, viewpoint: str = None) -> Dict:
        try:
            similar_images = self.search_similar_images(query_embedding, top_k, viewpoint)
            if not similar_images:
                return {
                    'predicted_stage': None,
                    'confidence': 0,
                    'stage_scores': {},
                    'similar_images': []
                }
            stage_scores: Dict[int, float] = {}
            total = 0.0
            for img in similar_images:
                st = img['stage']
                w = img['score']
                if st not in stage_scores:
                    stage_scores[st] = 0.0
                stage_scores[st] += w
                total += w
            if total > 0:
                for k in list(stage_scores.keys()):
                    stage_scores[k] /= total
            predicted_stage = max(stage_scores, key=stage_scores.get)
            confidence = stage_scores[predicted_stage]
            return {
                'predicted_stage': predicted_stage,
                'confidence': confidence,
                'stage_scores': stage_scores,
                'similar_images': similar_images[:5]
            }
        except Exception as e:
            self.logger.error(f"Prediction failed: {e}")
            return {
                'predicted_stage': None,
                'confidence': 0,
                'stage_scores': {},
                'similar_images': []
            }

    def get_index_stats(self) -> Dict:
        try:
            index = self.get_index()
            stats = index.describe_index_stats()
            return {
                'success': True,
                'total_vector_count': stats.get('total_vector_count', 0),
                'dimension': stats.get('dimension', 0),
                'namespaces': stats.get('namespaces', {}),
                'index_fullness': stats.get('index_fullness', 0)
            }
        except Exception as e:
            self.logger.error(f"Stats retrieval failed: {e}")
            return {'success': False, 'error': str(e)}

    def index_exists(self) -> bool:
        try:
            return self.index_name in self.pc.list_indexes().names()
        except Exception as e:
            self.logger.error(f"Index existence check failed: {e}")
            return False

    def search_vectors_by_prefix(self, prefix: str, limit: int = 10000) -> List[str]:
        """
        Prefix로 시작하는 벡터 ID들을 검색합니다.

        Args:
            prefix: 검색할 ID prefix (예: "37-Back_jpg")
            limit: 최대 반환할 ID 수

        Returns:
            매칭되는 벡터 ID 리스트
        """
        try:
            index = self.get_index()
            matching_ids = []

            # list() 메소드는 generator를 반환하므로 반복해서 수집
            for ids_batch in index.list(prefix=prefix, limit=min(limit, 10000)):
                matching_ids.extend(ids_batch)
                if len(matching_ids) >= limit:
                    break

            self.logger.info(f"Found {len(matching_ids)} vectors with prefix '{prefix}'")
            return matching_ids[:limit]

        except Exception as e:
            self.logger.error(f"Vector search by prefix failed: {e}")
            return []

    def delete_vectors_by_ids(self, vector_ids: List[str], batch_size: int = 1000) -> bool:
        """
        ID 리스트로 벡터들을 삭제합니다.

        Args:
            vector_ids: 삭제할 벡터 ID 리스트
            batch_size: 배치 삭제 크기

        Returns:
            삭제 성공 여부
        """
        try:
            if not vector_ids:
                self.logger.warning("No vector IDs provided for deletion")
                return True

            index = self.get_index()
            total_deleted = 0

            # 배치 단위로 삭제
            for i in range(0, len(vector_ids), batch_size):
                batch_ids = vector_ids[i:i + batch_size]
                index.delete(ids=batch_ids)
                total_deleted += len(batch_ids)
                self.logger.info(f"Deleted batch: {len(batch_ids)} vectors ({total_deleted}/{len(vector_ids)})")

            self.logger.info(f"Successfully deleted {total_deleted} vectors")
            return True

        except Exception as e:
            self.logger.error(f"Vector deletion failed: {e}")
            return False

    def delete_vectors_by_prefix(self, prefix: str, limit: int = 10000, confirm: bool = False) -> Dict:
        """
        Prefix로 매칭되는 벡터들을 찾아서 삭제합니다.

        Args:
            prefix: 삭제할 벡터 ID prefix
            limit: 최대 삭제할 벡터 수
            confirm: 삭제 확인 (안전장치)

        Returns:
            삭제 결과 정보
        """
        try:
            # 먼저 매칭되는 ID들 찾기
            matching_ids = self.search_vectors_by_prefix(prefix, limit)

            if not matching_ids:
                return {
                    'success': True,
                    'found_count': 0,
                    'deleted_count': 0,
                    'message': f"No vectors found with prefix '{prefix}'"
                }

            if not confirm:
                return {
                    'success': False,
                    'found_count': len(matching_ids),
                    'deleted_count': 0,
                    'message': f"Found {len(matching_ids)} vectors. Set confirm=True to delete",
                    'preview_ids': matching_ids[:10]  # 처음 10개만 미리보기
                }

            # 확인된 경우 삭제 실행
            deletion_success = self.delete_vectors_by_ids(matching_ids)

            return {
                'success': deletion_success,
                'found_count': len(matching_ids),
                'deleted_count': len(matching_ids) if deletion_success else 0,
                'message': f"Successfully deleted {len(matching_ids)} vectors" if deletion_success else "Deletion failed"
            }

        except Exception as e:
            self.logger.error(f"Prefix deletion failed: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def get_reference_images_by_stage(self, predicted_stage: int, gender: str = None, top_k: int = 5) -> Dict[str, str]:
        """
        예측된 stage에 해당하는 참고 이미지를 viewpoint별로 가져옵니다.

        Args:
            predicted_stage: 예측된 탈모 단계
            gender: 성별 필터 (male/female)
            top_k: viewpoint별로 가져올 이미지 수

        Returns:
            viewpoint별 가장 유사한 이미지 경로 딕셔너리
            {"front": "path/to/front.jpg", "side": "path/to/side.jpg"}
        """
        try:
            index = self.get_index()
            reference_images = {}

            # 검색할 viewpoint 목록
            viewpoints = {
                'front': ['Front', 'front'],
                'side': ['Right', 'Left', 'right', 'left', 'Side', 'side']
            }

            for view_key, view_values in viewpoints.items():
                # 필터 구성
                search_filter = {
                    "stage": {"$eq": predicted_stage}
                }

                if gender:
                    search_filter["gender"] = {"$eq": gender}

                search_filter["pointview"] = {"$in": view_values}

                # 더미 벡터로 검색 (메타데이터 필터만 사용)
                dummy_vector = [0.0] * self.dimension

                res = index.query(
                    vector=dummy_vector,
                    top_k=1,  # 가장 대표적인 1개만
                    include_metadata=True,
                    filter=search_filter
                )

                matches = res.get('matches', []) if isinstance(res, dict) else getattr(res, 'matches', [])

                if matches:
                    first_match = matches[0]
                    md = first_match['metadata'] if isinstance(first_match, dict) else getattr(first_match, 'metadata', {})
                    path = md.get('path', '')
                    if path:
                        # 경로 변환: 기존 경로를 실제 파일 위치로 매핑
                        # hair_loss_preprocessed -> male_for_ragdata_final/hair_loss_classification_modified_cleared/male
                        if 'hair_loss_preprocessed' in path:
                            path = path.replace(
                                'C:\\Users\\301\\Desktop\\hair_loss_preprocessed',
                                'C:\\Users\\301\\Desktop\\male_for_ragdata_final\\hair_loss_classification_modified_cleared\\male'
                            )
                        # hair_loss_classification_modified_cleared (male 직접 경로)
                        elif 'C:\\Users\\301\\Desktop\\hair_loss_classification_modified_cleared' in path and '\\male\\' in path:
                            path = path.replace(
                                'C:\\Users\\301\\Desktop\\hair_loss_classification_modified_cleared',
                                'C:\\Users\\301\\Desktop\\male_for_ragdata_final\\hair_loss_classification_modified_cleared'
                            )

                        reference_images[view_key] = path
                        self.logger.info(f"Found reference {view_key} image for stage {predicted_stage}: {path}")

            return reference_images

        except Exception as e:
            self.logger.error(f"Get reference images failed: {e}")
            return {}


