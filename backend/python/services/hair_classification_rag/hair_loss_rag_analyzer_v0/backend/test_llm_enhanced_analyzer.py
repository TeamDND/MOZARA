#!/usr/bin/env python3
"""
LLM í†µí•© Hair Loss Analyzer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from PIL import Image

# ë°±ì—”ë“œ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# í•˜ë“œì½”ë”©ëœ ê²½ë¡œë¡œ config ìš°íšŒ
class HardcodedSettings:
    UPLOAD_DIR = "C:/Users/301/Desktop/main_project/backend/hair_classification/hair_loss_rag_analyzer_v0/backend/uploads"
    INDEX_NAME = "hair-loss-rag-analysis-convnext"
    EMBEDDING_DIMENSION = 1536
    MODEL_NAME = "convnext_large.fb_in22k_ft_in1k_384"

    # íƒˆëª¨ ë‹¨ê³„ ì„¤ëª…
    STAGE_DESCRIPTIONS = {
        2: "ê²½ë¯¸í•œ íƒˆëª¨ - Mì íƒˆëª¨ê°€ ì‹œì‘ë˜ê±°ë‚˜ ì´ë§ˆì„ ì´ ì•½ê°„ í›„í‡´",
        3: "ì´ˆê¸° íƒˆëª¨ - Mì íƒˆëª¨ê°€ ëšœë ·í•´ì§€ê³  ì •ìˆ˜ë¦¬ ë¶€ë¶„ ëª¨ë°œ ë°€ë„ ê°ì†Œ",
        4: "ì¤‘ê¸° íƒˆëª¨ - Mì íƒˆëª¨ ì§„í–‰, ì •ìˆ˜ë¦¬ íƒˆëª¨ ë³¸ê²©í™”",
        5: "ì§„í–‰ëœ íƒˆëª¨ - ì•ë¨¸ë¦¬ì™€ ì •ìˆ˜ë¦¬ íƒˆëª¨ê°€ ì—°ê²°ë˜ê¸° ì‹œì‘",
        6: "ì‹¬í•œ íƒˆëª¨ - ì•ë¨¸ë¦¬ì™€ ì •ìˆ˜ë¦¬ê°€ ì™„ì „íˆ ì—°ê²°ë˜ì–´ í•˜ë‚˜ì˜ í° íƒˆëª¨ ì˜ì—­ í˜•ì„±",
        7: "ë§¤ìš° ì‹¬í•œ íƒˆëª¨ - ì¸¡ë©´ê³¼ ë’·ë¨¸ë¦¬ë¥¼ ì œì™¸í•œ ëŒ€ë¶€ë¶„ì˜ ëª¨ë°œ ì†ì‹¤"
    }

# configë¥¼ ì§ì ‘ ë®ì–´ì“°ê¸°
import app.config
app.config.settings = HardcodedSettings()

from app.services.hair_loss_analyzer import HairLossAnalyzer
settings = HardcodedSettings()


class LLMEnhancedAnalyzerTester:
    def __init__(self, test_data_path: str, base_log_path: str):
        """
        LLM í†µí•© ì• ë„ë¼ì´ì € í…ŒìŠ¤í„° ì´ˆê¸°í™”
        """
        self.test_data_path = Path(test_data_path)
        self.base_log_path = Path(base_log_path)

        # í…ŒìŠ¤íŠ¸ ë²ˆí˜¸ ê²°ì •
        self.test_number = self.get_next_test_number()
        self.result_log_path = self.base_log_path / f"llm_enhanced_test{self.test_number}"

        self.analyzer = None
        self.faiss_results = []  # FAISS ì „ìš© ê²°ê³¼
        self.llm_results = []    # LLM í†µí•© ê²°ê³¼

        # ê²°ê³¼ ë¡œê·¸ ë””ë ‰í„°ë¦¬ ìƒì„±
        self.result_log_path.mkdir(parents=True, exist_ok=True)

        print(f"[INFO] LLM í†µí•© í…ŒìŠ¤íŠ¸ ë²ˆí˜¸: {self.test_number}")
        print(f"[INFO] í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {self.test_data_path}")
        print(f"[INFO] ê²°ê³¼ ë¡œê·¸ ê²½ë¡œ: {self.result_log_path}")

    def get_next_test_number(self) -> int:
        """ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ë²ˆí˜¸ ê²°ì •"""
        if not self.base_log_path.exists():
            return 1

        existing_tests = [d for d in self.base_log_path.iterdir()
                         if d.is_dir() and 'llm_enhanced_test' in d.name]
        if not existing_tests:
            return 1

        test_numbers = []
        for test_dir in existing_tests:
            try:
                num = int(test_dir.name.replace('llm_enhanced_test', ''))
                test_numbers.append(num)
            except ValueError:
                continue

        return max(test_numbers) + 1 if test_numbers else 1

    async def initialize_analyzer(self):
        """HairLossAnalyzer ì´ˆê¸°í™”"""
        try:
            print("[INIT] HairLossAnalyzer ì´ˆê¸°í™” ì¤‘...")
            self.analyzer = HairLossAnalyzer()
            print("[SUCCESS] HairLossAnalyzer ì´ˆê¸°í™” ì™„ë£Œ (LLM í†µí•©)")
            return True
        except Exception as e:
            print(f"[ERROR] HairLossAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def load_test_data(self) -> Dict[int, List[Path]]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        test_data = {}

        # ì¼ë°˜ì ì¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œë“¤ í™•ì¸
        possible_paths = [
            self.test_data_path,
            Path("C:/Users/301/Desktop/test_data_set/test"),
            Path("C:/Users/301/Desktop/hair_loss_rag/hair_rag_dataset_image/hair_rag_dataset_ragging"),
        ]

        actual_test_path = None
        for path in possible_paths:
            if path.exists():
                actual_test_path = path
                print(f"[FOUND] í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ë°œê²¬: {path}")
                break

        if not actual_test_path:
            print("[ERROR] í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        # ë ˆë²¨ë³„ í´ë” ì°¾ê¸°
        for level in range(2, 8):  # ë ˆë²¨ 2-7
            level_patterns = [f"LEVEL_{level}", f"level_{level}", f"{level}", f"level{level}"]

            level_path = None
            for pattern in level_patterns:
                candidate_path = actual_test_path / pattern
                if candidate_path.exists():
                    level_path = candidate_path
                    break

            if not level_path:
                print(f"[WARNING] ë ˆë²¨ {level} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                continue

            # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
            image_files = []
            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                image_files.extend(list(level_path.glob(f"*{ext}")))
                image_files.extend(list(level_path.glob(f"*{ext.upper()}")))

            # ì¤‘ë³µ ì œê±°
            image_files = list(set(image_files))

            # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ê° ë ˆë²¨ë‹¹ ìµœëŒ€ 3ê°œë§Œ ì„ íƒ (ë¹„ìš© ì ˆì•½)
            if len(image_files) > 3:
                image_files = image_files[:3]
                print(f"[INFO] ë ˆë²¨ {level}: {len(image_files)}ê°œ ì´ë¯¸ì§€ ì„ íƒ (ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ì œí•œ)")
            else:
                print(f"[INFO] ë ˆë²¨ {level}: {len(image_files)}ê°œ ì´ë¯¸ì§€")

            test_data[level] = image_files

        total_images = sum(len(files) for files in test_data.values())
        estimated_cost = total_images * 0.003  # GPT-4o-mini ì¶”ì • ë¹„ìš©
        print(f"[INFO] ì´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"[COST] ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.3f}")

        return test_data

    async def test_single_image_both_methods(self, image_path: Path, true_level: int) -> Tuple[Dict, Dict]:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ FAISSì™€ LLM ë‘ ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        """
        try:
            image = Image.open(image_path).convert('RGB')

            # 1. FAISS ì „ìš© ë¶„ì„
            print(f"    [FAISS] ë¶„ì„ ì¤‘...")
            start_time = time.time()
            faiss_result = await self.analyzer.analyze_image(image, image_path.name, use_llm=False)
            faiss_time = time.time() - start_time

            # 2. LLM í†µí•© ë¶„ì„
            print(f"    [LLM] í†µí•© ë¶„ì„ ì¤‘...")
            start_time = time.time()
            llm_result = await self.analyzer.analyze_image(image, image_path.name, use_llm=True)
            llm_time = time.time() - start_time

            # FAISS ê²°ê³¼ ì •ë¦¬
            faiss_test_result = {
                'image_path': str(image_path),
                'filename': image_path.name,
                'true_level': true_level,
                'predicted_level': faiss_result.get('predicted_stage') if faiss_result['success'] else None,
                'confidence': faiss_result.get('confidence', 0.0) if faiss_result['success'] else 0.0,
                'analysis_time': faiss_time,
                'method': 'faiss_only',
                'success': faiss_result['success'],
                'error': faiss_result.get('error') if not faiss_result['success'] else None
            }

            # LLM ê²°ê³¼ ì •ë¦¬
            llm_test_result = {
                'image_path': str(image_path),
                'filename': image_path.name,
                'true_level': true_level,
                'predicted_level': llm_result.get('predicted_stage') if llm_result['success'] else None,
                'confidence': llm_result.get('confidence', 0.0) if llm_result['success'] else 0.0,
                'analysis_time': llm_time,
                'method': llm_result.get('analysis_details', {}).get('method', 'llm_enhanced'),
                'success': llm_result['success'],
                'error': llm_result.get('error') if not llm_result['success'] else None,
                'llm_analysis': llm_result.get('analysis_details', {}).get('llm_analysis', {}),
                'token_usage': llm_result.get('analysis_details', {}).get('token_usage', {})
            }

            return faiss_test_result, llm_test_result

        except Exception as e:
            error_result = {
                'image_path': str(image_path),
                'filename': image_path.name,
                'true_level': true_level,
                'predicted_level': None,
                'confidence': 0.0,
                'analysis_time': 0.0,
                'success': False,
                'error': str(e)
            }
            return error_result, error_result

    async def run_comparative_tests(self):
        """FAISS vs LLM ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("[START] LLM í†µí•© vs FAISS ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_data = self.load_test_data()
        if not test_data:
            print("[ERROR] í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì• ë„ë¼ì´ì € ì´ˆê¸°í™”
        if not await self.initialize_analyzer():
            return

        # ê° ë ˆë²¨ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for level, image_files in test_data.items():
            print(f"\n[LEVEL {level}] í…ŒìŠ¤íŠ¸ ì¤‘... ({len(image_files)}ê°œ ì´ë¯¸ì§€)")

            for i, image_path in enumerate(image_files):
                print(f"  [{i+1}/{len(image_files)}] {image_path.name}")

                faiss_result, llm_result = await self.test_single_image_both_methods(image_path, level)

                self.faiss_results.append(faiss_result)
                self.llm_results.append(llm_result)

                # ê²°ê³¼ ì¶œë ¥
                if faiss_result['success']:
                    print(f"    [FAISS] ì˜ˆì¸¡: {faiss_result['predicted_level']} (ì‹ ë¢°ë„: {faiss_result['confidence']:.3f})")
                else:
                    print(f"    [FAISS] ì‹¤íŒ¨: {faiss_result['error']}")

                if llm_result['success']:
                    print(f"    [LLM] ì˜ˆì¸¡: {llm_result['predicted_level']} (ì‹ ë¢°ë„: {llm_result['confidence']:.3f})")
                    if 'token_usage' in llm_result and llm_result['token_usage']:
                        tokens = llm_result['token_usage'].get('total_tokens', 0)
                        print(f"    [TOKEN] ì‚¬ìš©ëŸ‰: {tokens}")
                else:
                    print(f"    [LLM] ì‹¤íŒ¨: {llm_result['error']}")

        print(f"\n[COMPLETE] ì´ {len(self.faiss_results)}ê°œ ì´ë¯¸ì§€ ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    def calculate_comparative_metrics(self) -> Dict:
        """FAISS vs LLM ë¹„êµ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print("\n[METRICS] ë¹„êµ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")

        # FAISS ê²°ê³¼ ë¶„ì„
        faiss_successful = [r for r in self.faiss_results if r['success'] and r['predicted_level'] is not None]
        faiss_accuracy = 0
        if faiss_successful:
            faiss_y_true = [r['true_level'] for r in faiss_successful]
            faiss_y_pred = [r['predicted_level'] for r in faiss_successful]
            faiss_accuracy = sum(1 for t, p in zip(faiss_y_true, faiss_y_pred) if t == p) / len(faiss_y_true)

        # LLM ê²°ê³¼ ë¶„ì„
        llm_successful = [r for r in self.llm_results if r['success'] and r['predicted_level'] is not None]
        llm_accuracy = 0
        if llm_successful:
            llm_y_true = [r['true_level'] for r in llm_successful]
            llm_y_pred = [r['predicted_level'] for r in llm_successful]
            llm_accuracy = sum(1 for t, p in zip(llm_y_true, llm_y_pred) if t == p) / len(llm_y_true)

        # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
        total_tokens = sum(r.get('token_usage', {}).get('total_tokens', 0) for r in self.llm_results)
        estimated_cost = total_tokens * 0.000001 * 1.5  # GPT-4o-mini ëŒ€ëµì  ë¹„ìš©

        # í‰ê·  ë¶„ì„ ì‹œê°„
        faiss_avg_time = np.mean([r['analysis_time'] for r in faiss_successful]) if faiss_successful else 0
        llm_avg_time = np.mean([r['analysis_time'] for r in llm_successful]) if llm_successful else 0

        metrics = {
            'test_info': {
                'total_tests': len(self.faiss_results),
                'timestamp': datetime.now().isoformat()
            },
            'faiss_performance': {
                'successful_tests': len(faiss_successful),
                'accuracy': faiss_accuracy,
                'avg_analysis_time': faiss_avg_time,
                'success_rate': len(faiss_successful) / len(self.faiss_results) if self.faiss_results else 0
            },
            'llm_performance': {
                'successful_tests': len(llm_successful),
                'accuracy': llm_accuracy,
                'avg_analysis_time': llm_avg_time,
                'success_rate': len(llm_successful) / len(self.llm_results) if self.llm_results else 0,
                'total_tokens_used': total_tokens,
                'estimated_cost_usd': estimated_cost
            },
            'comparison': {
                'accuracy_improvement': llm_accuracy - faiss_accuracy,
                'time_overhead': llm_avg_time - faiss_avg_time,
                'cost_per_analysis': estimated_cost / len(self.llm_results) if self.llm_results else 0
            }
        }

        return metrics

    def save_comparative_results(self, metrics: Dict):
        """ë¹„êµ ê²°ê³¼ ì €ì¥"""
        print("\n[SAVE] ë¹„êµ ê²°ê³¼ ì €ì¥ ì¤‘...")

        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        detailed_results = {
            'test_info': {
                'test_type': 'llm_enhanced_comparative',
                'test_number': self.test_number,
                'timestamp': datetime.now().isoformat(),
                'total_images': len(self.faiss_results)
            },
            'metrics': metrics,
            'faiss_results': self.faiss_results,
            'llm_results': self.llm_results
        }

        json_path = self.result_log_path / "comparative_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)

        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
        report_path = self.result_log_path / "comparison_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write(f"LLM í†µí•© vs FAISS ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ (Test{self.test_number})\n")
            f.write("=" * 70 + "\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {metrics['test_info']['total_tests']}\n\n")

            # FAISS ì„±ëŠ¥
            f.write("ğŸ“Š FAISS ì „ìš© ì„±ëŠ¥\n")
            f.write("-" * 30 + "\n")
            faiss_perf = metrics['faiss_performance']
            f.write(f"ì •í™•ë„: {faiss_perf['accuracy']:.1%}\n")
            f.write(f"ì„±ê³µë¥ : {faiss_perf['success_rate']:.1%}\n")
            f.write(f"í‰ê·  ë¶„ì„ ì‹œê°„: {faiss_perf['avg_analysis_time']:.3f}ì´ˆ\n\n")

            # LLM í†µí•© ì„±ëŠ¥
            f.write("ğŸ¤– LLM í†µí•© ì„±ëŠ¥\n")
            f.write("-" * 30 + "\n")
            llm_perf = metrics['llm_performance']
            f.write(f"ì •í™•ë„: {llm_perf['accuracy']:.1%}\n")
            f.write(f"ì„±ê³µë¥ : {llm_perf['success_rate']:.1%}\n")
            f.write(f"í‰ê·  ë¶„ì„ ì‹œê°„: {llm_perf['avg_analysis_time']:.3f}ì´ˆ\n")
            f.write(f"ì´ í† í° ì‚¬ìš©ëŸ‰: {llm_perf['total_tokens_used']:,}\n")
            f.write(f"ì˜ˆìƒ ë¹„ìš©: ${llm_perf['estimated_cost_usd']:.4f}\n\n")

            # ë¹„êµ ê²°ê³¼
            f.write("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ\n")
            f.write("-" * 30 + "\n")
            comp = metrics['comparison']
            f.write(f"ì •í™•ë„ ê°œì„ : {comp['accuracy_improvement']:+.1%}\n")
            f.write(f"ì‹œê°„ ì˜¤ë²„í—¤ë“œ: {comp['time_overhead']:+.3f}ì´ˆ\n")
            f.write(f"ë¶„ì„ë‹¹ ë¹„ìš©: ${comp['cost_per_analysis']:.5f}\n")

        print(f"[SUCCESS] ìƒì„¸ ê²°ê³¼: {json_path}")
        print(f"[SUCCESS] ìš”ì•½ ë¦¬í¬íŠ¸: {report_path}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("LLM í†µí•© Hair Loss Analyzer ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # ê²½ë¡œ ì„¤ì •
    test_data_path = "C:/Users/301/Desktop/test_data_set/test"
    base_log_path = "C:/Users/301/Desktop/main_project/backend/hair_classification/result_log/log"

    # í…ŒìŠ¤í„° ì´ˆê¸°í™” ë° ì‹¤í–‰
    tester = LLMEnhancedAnalyzerTester(test_data_path, base_log_path)

    try:
        # ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await tester.run_comparative_tests()

        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        metrics = tester.calculate_comparative_metrics()

        if metrics:
            # ê²°ê³¼ ì €ì¥
            tester.save_comparative_results(metrics)

            print("\n" + "=" * 70)
            print(f"LLM í†µí•© ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (Test{tester.test_number})")
            print("=" * 70)

            faiss_acc = metrics['faiss_performance']['accuracy']
            llm_acc = metrics['llm_performance']['accuracy']
            improvement = metrics['comparison']['accuracy_improvement']
            cost = metrics['llm_performance']['estimated_cost_usd']

            print(f"FAISS ì •í™•ë„: {faiss_acc:.1%}")
            print(f"LLM í†µí•© ì •í™•ë„: {llm_acc:.1%}")
            print(f"ì„±ëŠ¥ ê°œì„ : {improvement:+.1%}")
            print(f"ì´ ë¹„ìš©: ${cost:.4f}")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {tester.result_log_path}")
        else:
            print("\n[ERROR] ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")

    except KeyboardInterrupt:
        print("\n\n[STOP] ì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ERROR] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())