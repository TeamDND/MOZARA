#!/usr/bin/python
# -*- encoding: utf-8 -*-

"""
============================================================================
ğŸ§± ResNet18 - BiSeNetì˜ ë°±ë³¸ ë„¤íŠ¸ì›Œí¬
============================================================================

ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
ğŸ“¸ ì´ë¯¸ì§€ â†’ ğŸ” ê³„ì¸µì  íŠ¹ì§• ì¶”ì¶œ â†’ ğŸ“Š 3ê°œ ìŠ¤ì¼€ì¼ ì¶œë ¥

ë¬´ì—‡ì¸ê°€ìš”?
- ResNet18: ë”¥ëŸ¬ë‹ì˜ "í‘œì¤€ ë°±ë³¸"
- 18ê°œ ë ˆì´ì–´ë¡œ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë„¤íŠ¸ì›Œí¬
- BiSeNetì˜ Context Pathë¡œ ì‚¬ìš©ë¨

ResNetì˜ í•µì‹¬: Residual Connection (ì”ì°¨ ì—°ê²°)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì…ë ¥   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ â†˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv   â”‚  â”‚ ë°”ë¡œê°€ê¸° â”‚ â† Shortcut!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ â†™
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì¶œë ¥   â”‚ = Conv(ì…ë ¥) + ì…ë ¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì™œ Residualì¸ê°€ìš”?
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ëŠ” í•™ìŠµì´ ì–´ë ¤ì›€ (ê¸°ìš¸ê¸° ì†Œì‹¤)
- Shortcutìœ¼ë¡œ ì •ë³´ë¥¼ ì§ì ‘ ì „ë‹¬ â†’ í•™ìŠµ ì‰¬ì›Œì§
- ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ê°€ëŠ¥! (18ì¸µ, 50ì¸µ, 101ì¸µ...)

ìš°ë¦¬ì˜ ResNet18 êµ¬ì¡°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì…ë ¥ (3Ã—HÃ—W)     â”‚ RGB ì´ë¯¸ì§€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv1 + MaxPool  â”‚ 1/4 í¬ê¸°
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer1 (64ì±„ë„)  â”‚ 1/4 í¬ê¸° (2 blocks)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer2 (128ì±„ë„) â”‚ 1/8 í¬ê¸° (2 blocks) â†’ feat8 â­
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer3 (256ì±„ë„) â”‚ 1/16 í¬ê¸° (2 blocks) â†’ feat16 â­
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer4 (512ì±„ë„) â”‚ 1/32 í¬ê¸° (2 blocks) â†’ feat32 â­
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì¶œë ¥: feat8, feat16, feat32 (3ê°œ ìŠ¤ì¼€ì¼)
- feat8: ë””í…Œì¼í•œ ì •ë³´ (ë¨¸ë¦¬ì¹´ë½ í…ìŠ¤ì²˜)
- feat16: ì¤‘ê°„ íŒ¨í„´ (ì–¼êµ´ ìœ¤ê³½)
- feat32: ì „ì²´ ë§¥ë½ (ì „ì²´ êµ¬ë„)

============================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.model_zoo as modelzoo

# from modules.bn import InPlaceABNSync as BatchNorm2d

# ì‚¬ì „ í•™ìŠµëœ ResNet18 ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ URL
resnet18_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'


# ============================================================================
# ğŸ”¨ ê¸°ë³¸ ë¸”ë¡ë“¤
# ============================================================================

def conv3x3(in_planes, out_planes, stride=1):
    """
    ğŸ”¹ 3Ã—3 Convolution (padding í¬í•¨)

    ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
    - ê°€ì¥ í”í•˜ê²Œ ì“°ëŠ” 3Ã—3 convolution
    - padding=1ë¡œ í¬ê¸° ìœ ì§€ (stride=1ì¼ ë•Œ)

    ì™œ 3Ã—3ì¸ê°€ìš”?
    - 5Ã—5ë‚˜ 7Ã—7ë³´ë‹¤ ê³„ì‚°ëŸ‰ ì ìŒ
    - ì—¬ëŸ¬ ë²ˆ ìŒ“ìœ¼ë©´ í° í•„í„°ì™€ ê°™ì€ íš¨ê³¼
    - íŒŒë¼ë¯¸í„° íš¨ìœ¨ì !
    """
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)  # BatchNorm ì“°ë¯€ë¡œ bias ë¶ˆí•„ìš”


class BasicBlock(nn.Module):
    """
    ğŸ§± ResNetì˜ ê¸°ë³¸ ë¸”ë¡ (Residual Block)

    ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
    - ì…ë ¥ì„ ë³€í™˜í•˜ë©´ì„œë„ ì›ë³¸ ì •ë³´ë¥¼ ë³´ì¡´
    - "ì›ë³¸ + ë³€í™˜" = Residual Learningì˜ í•µì‹¬!

    êµ¬ì¡°:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ì…ë ¥ x  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ â†˜ (Shortcut)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Conv1   â”‚          â”‚ Identityâ”‚ ë˜ëŠ”
    â”‚   â†“     â”‚          â”‚   or    â”‚
    â”‚ ReLU    â”‚          â”‚ Conv1Ã—1 â”‚ (í¬ê¸° ë‹¤ë¥¼ ë•Œ)
    â”‚   â†“     â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ Conv2   â”‚                â†“
    â”‚   â†“     â”‚                â†“
    â”‚  BN     â”‚ â†’  ë”í•˜ê¸° â†â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  ReLU   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
                  ì¶œë ¥ (ì…ë ¥ + ë³€í™˜)

    ì™œ ì´ë ‡ê²Œ í•˜ë‚˜ìš”?
    - ì¼ë°˜ Conv: F(x)ë¥¼ í•™ìŠµ
    - Residual: F(x) - xë¥¼ í•™ìŠµ (ì”ì°¨ë§Œ í•™ìŠµ)
    - ì”ì°¨ê°€ ë” í•™ìŠµí•˜ê¸° ì‰¬ì›€!

    ì˜ˆì‹œ:
    - ì…ë ¥: [1, 2, 3]
    - Convë¡œ ë³€í™˜: [1.1, 2.2, 2.9]
    - Shortcut: [1, 2, 3]
    - ì¶œë ¥: [2.1, 4.2, 5.9] = ë³€í™˜ + ì›ë³¸
    """
    def __init__(self, in_chan, out_chan, stride=1):
        """
        BasicBlock ì´ˆê¸°í™”

        in_chan: ì…ë ¥ ì±„ë„ ìˆ˜
        out_chan: ì¶œë ¥ ì±„ë„ ìˆ˜
        stride: í¬ê¸° ì¤„ì´ê¸° (1=ìœ ì§€, 2=ì ˆë°˜)
        """
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(in_chan, out_chan, stride)
        self.bn1 = nn.BatchNorm2d(out_chan)
        self.conv2 = conv3x3(out_chan, out_chan)
        self.bn2 = nn.BatchNorm2d(out_chan)
        self.relu = nn.ReLU(inplace=True)

        # Downsample: ì…ì¶œë ¥ í¬ê¸°ê°€ ë‹¤ë¥¼ ë•Œ Shortcut ì¡°ì •
        self.downsample = None
        if in_chan != out_chan or stride != 1:
            # 1Ã—1 Convë¡œ ì±„ë„ ìˆ˜ì™€ í¬ê¸° ë§ì¶¤
            self.downsample = nn.Sequential(
                nn.Conv2d(in_chan, out_chan,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_chan),
                )

    def forward(self, x):
        """
        ìˆœì „íŒŒ: Residual Learning ìˆ˜í–‰

        ê³¼ì •:
        1. ì…ë ¥ â†’ Conv1 â†’ ReLU â†’ Conv2 â†’ BN (ë³€í™˜)
        2. Shortcut ì¤€ë¹„ (í•„ìš”í•˜ë©´ Downsample)
        3. ë³€í™˜ + Shortcut
        4. ReLU
        """
        # ë©”ì¸ ê²½ë¡œ (ë³€í™˜)
        residual = self.conv1(x)
        residual = F.relu(self.bn1(residual))
        residual = self.conv2(residual)
        residual = self.bn2(residual)

        # Shortcut ê²½ë¡œ (ì›ë³¸)
        shortcut = x
        if self.downsample is not None:
            shortcut = self.downsample(x)  # í¬ê¸°/ì±„ë„ ë§ì¶”ê¸°

        # Residual Connection: ë”í•˜ê¸°!
        out = shortcut + residual
        out = self.relu(out)
        return out


def create_layer_basic(in_chan, out_chan, bnum, stride=1):
    """
    ğŸ—ï¸ ResNet Layer ìƒì„± (ì—¬ëŸ¬ BasicBlock ë¬¶ìŒ)

    ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
    - BasicBlockì„ ì—¬ëŸ¬ ê°œ ìŒ“ì•„ì„œ í•˜ë‚˜ì˜ Layer ë§Œë“¦

    ì…ë ¥:
        in_chan: ì…ë ¥ ì±„ë„
        out_chan: ì¶œë ¥ ì±„ë„
        bnum: ë¸”ë¡ ê°œìˆ˜ (ì˜ˆ: 2ê°œ)
        stride: ì²« ë¸”ë¡ì˜ stride (ë‚˜ë¨¸ì§€ëŠ” 1)

    ì¶œë ¥:
        Sequential (BasicBlock ì—¬ëŸ¬ ê°œ)

    ì˜ˆì‹œ:
    - bnum=2, stride=2
    â†’ Block1(stride=2) + Block2(stride=1)
    â†’ í¬ê¸°ëŠ” ì²« ë¸”ë¡ì—ì„œë§Œ ì¤„ì–´ë“¦
    """
    layers = [BasicBlock(in_chan, out_chan, stride=stride)]  # ì²« ë¸”ë¡ (í¬ê¸° ì¡°ì •)
    for i in range(bnum-1):
        layers.append(BasicBlock(out_chan, out_chan, stride=1))  # ë‚˜ë¨¸ì§€ (í¬ê¸° ìœ ì§€)
    return nn.Sequential(*layers)


# ============================================================================
# ğŸ›ï¸ ResNet18 ë©”ì¸ ëª¨ë¸
# ============================================================================

class Resnet18(nn.Module):
    """
    ğŸ† ResNet18 - 18ì¸µ ì”ì°¨ ë„¤íŠ¸ì›Œí¬

    ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
    - ì´ë¯¸ì§€ì—ì„œ ê³„ì¸µì  íŠ¹ì§•ì„ ì¶”ì¶œ
    - 3ê°œ ìŠ¤ì¼€ì¼ ì¶œë ¥: ë””í…Œì¼ â†’ ì¤‘ê°„ â†’ ì „ì²´

    êµ¬ì¡° ìƒì„¸:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ì…ë ¥ (B, 3, H, W)                    â”‚ ì›ë³¸ RGB
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Conv1 (7Ã—7, stride=2)                â”‚
    â”‚ + BN + ReLU                          â”‚ 1/2 í¬ê¸°
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ MaxPool (3Ã—3, stride=2)              â”‚ 1/4 í¬ê¸°
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Layer1: 64 ì±„ë„, 2 blocks            â”‚ 1/4 í¬ê¸°
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Layer2: 128 ì±„ë„, 2 blocks           â”‚ 1/8 í¬ê¸° â†’ feat8 â­
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Layer3: 256 ì±„ë„, 2 blocks           â”‚ 1/16 í¬ê¸° â†’ feat16 â­
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Layer4: 512 ì±„ë„, 2 blocks           â”‚ 1/32 í¬ê¸° â†’ feat32 â­
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ì¶œë ¥:
        feat8: (B, 128, H/8, W/8) - ë””í…Œì¼ (ë¨¸ë¦¬ì¹´ë½ í…ìŠ¤ì²˜)
        feat16: (B, 256, H/16, W/16) - íŒ¨í„´ (ì–¼êµ´ êµ¬ì¡°)
        feat32: (B, 512, H/32, W/32) - ë§¥ë½ (ì „ì²´ êµ¬ë„)

    ì´ íŒŒë¼ë¯¸í„°: ì•½ 11Mê°œ
    ì´ ë ˆì´ì–´: 18ê°œ (Conv1 + Layer1~4 + FC)
    """
    def __init__(self):
        """ResNet18 ì´ˆê¸°í™” ë° ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        super(Resnet18, self).__init__()

        # Stem: ì´ˆê¸° Convolution
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # 4ê°œ Layer (ê° 2ê°œ ë¸”ë¡)
        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)    # 64 ì±„ë„
        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)   # 128 ì±„ë„
        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)  # 256 ì±„ë„
        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)  # 512 ì±„ë„

        self.init_weight()  # ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ

    def forward(self, x):
        """
        ìˆœì „íŒŒ: 3ê°œ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ

        ê³¼ì •:
        1. Stem (Conv1 + MaxPool) â†’ 1/4 í¬ê¸°
        2. Layer1 â†’ 1/4 í¬ê¸°
        3. Layer2 â†’ 1/8 í¬ê¸° (feat8 ì¶œë ¥)
        4. Layer3 â†’ 1/16 í¬ê¸° (feat16 ì¶œë ¥)
        5. Layer4 â†’ 1/32 í¬ê¸° (feat32 ì¶œë ¥)

        ì…ë ¥: (B, 3, H, W)
        ì¶œë ¥: (feat8, feat16, feat32) 3ê°œ í…ì„œ
        """
        # Stem
        x = self.conv1(x)
        x = F.relu(self.bn1(x))
        x = self.maxpool(x)

        # 4ê°œ Layer
        x = self.layer1(x)           # 1/4 í¬ê¸°
        feat8 = self.layer2(x)       # 1/8 í¬ê¸° â­
        feat16 = self.layer3(feat8)  # 1/16 í¬ê¸° â­
        feat32 = self.layer4(feat16) # 1/32 í¬ê¸° â­

        return feat8, feat16, feat32

    def init_weight(self):
        """
        ğŸ”§ ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ (Transfer Learning)

        ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
        - ImageNetìœ¼ë¡œ í•™ìŠµëœ ResNet18 ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì˜´
        - FC(ë¶„ë¥˜) ë ˆì´ì–´ëŠ” ì œì™¸í•˜ê³  íŠ¹ì§• ì¶”ì¶œ ë¶€ë¶„ë§Œ ì‚¬ìš©

        ì™œ í•„ìš”í•œê°€ìš”?
        - ì²˜ìŒë¶€í„° í•™ìŠµí•˜ë©´ ì˜¤ë˜ ê±¸ë¦¼
        - ImageNet í•™ìŠµ ê°€ì¤‘ì¹˜ëŠ” ì¼ë°˜ì ì¸ íŠ¹ì§• ì˜ ì¡ì•„ëƒ„
        - Transfer Learningìœ¼ë¡œ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ!

        ê³¼ì •:
        1. PyTorch Hubì—ì„œ ResNet18 ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ
        2. FC ë ˆì´ì–´ ì œì™¸ (ìš°ë¦¬ëŠ” íŠ¹ì§• ì¶”ì¶œë§Œ í•„ìš”)
        3. ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ ëª¨ë¸ì— ë³µì‚¬
        """
        # ImageNet ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ
        state_dict = modelzoo.load_url(resnet18_url)
        self_state_dict = self.state_dict()

        # FC ë ˆì´ì–´ ë¹¼ê³  ë³µì‚¬
        for k, v in state_dict.items():
            if 'fc' in k:
                continue  # FC ë ˆì´ì–´ëŠ” ìŠ¤í‚µ (ë¶„ë¥˜ ë ˆì´ì–´, ìš°ë¦¬ëŠ” íŠ¹ì§•ë§Œ í•„ìš”)
            self_state_dict.update({k: v})

        self.load_state_dict(self_state_dict)

    def get_params(self):
        """
        ğŸ“Š í•™ìŠµ íŒŒë¼ë¯¸í„° ë¶„ë¦¬ (Weight Decay ì ìš© ì—¬ë¶€)

        ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
        - Weight Decayë¥¼ ì ìš©í•  íŒŒë¼ë¯¸í„°ì™€ ì•ˆ í•  íŒŒë¼ë¯¸í„° êµ¬ë¶„

        ì™œ ë¶„ë¦¬í•˜ë‚˜ìš”?
        - Conv/Linearì˜ weight: Weight Decay ì ìš© (ê³¼ì í•© ë°©ì§€)
        - BatchNorm, bias: Weight Decay ì ìš© ì•ˆ í•¨ (ì„±ëŠ¥ ì €í•˜ ë°©ì§€)

        ì¶œë ¥:
            wd_params: Weight Decay ì ìš©í•  íŒŒë¼ë¯¸í„° (Conv, Linear weight)
            nowd_params: Weight Decay ì ìš© ì•ˆ í•  íŒŒë¼ë¯¸í„° (BatchNorm, bias)
        """
        wd_params, nowd_params = [], []
        for name, module in self.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                wd_params.append(module.weight)  # WeightëŠ” Decay ì ìš©
                if not module.bias is None:
                    nowd_params.append(module.bias)  # BiasëŠ” Decay ì•ˆ í•¨
            elif isinstance(module,  nn.BatchNorm2d):
                nowd_params += list(module.parameters())  # BatchNormì€ Decay ì•ˆ í•¨
        return wd_params, nowd_params


if __name__ == "__main__":
    net = Resnet18()
    x = torch.randn(16, 3, 224, 224)
    out = net(x)
    print(out[0].size())
    print(out[1].size())
    print(out[2].size())
    net.get_params()
