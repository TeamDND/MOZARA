"""
============================================================================
ğŸ§  Swin Transformer ê¸°ë°˜ íƒˆëª¨ ë¶„ë¥˜ AI ëª¨ë¸
============================================================================

ğŸ“Œ ì£¼ìš” íŠ¹ì§•:
- 6ì±„ë„ ì…ë ¥: RGB ì´ë¯¸ì§€(3ì±„ë„) + í—¤ì–´ ë§ˆìŠ¤í¬(3ì±„ë„)
- 4ë‹¨ê³„ íƒˆëª¨ ë¶„ë¥˜: 0(ì •ìƒ) â†’ 1(ê²½ë¯¸) â†’ 2(ì¤‘ë“±ë„) â†’ 3(ì‹¬ê°)
- Shifted Window Attentionìœ¼ë¡œ íš¨ìœ¨ì ì¸ íŠ¹ì§• ì¶”ì¶œ

ğŸ“Š ëª¨ë¸ êµ¬ì¡° ìš”ì•½:
    ì…ë ¥(224x224) â†’ Patch Embed â†’ Stage1(56x56) â†’ Stage2(28x28)
    â†’ Stage3(14x14) â†’ Stage4(7x7) â†’ ë¶„ë¥˜(4í´ë˜ìŠ¤)

ğŸ“„ ë…¼ë¬¸: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
============================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
import math


# ============================================================================
# ğŸ”§ ë³´ì¡° ëª¨ë“ˆ (Helper Modules)
# ============================================================================

class Mlp(nn.Module):
    """
    ğŸ”¹ Multi-Layer Perceptron (MLP) ë¸”ë¡

    ì—­í• : Transformerì˜ Feed-Forward Network
    ë™ì‘: ì…ë ¥ â†’ í™•ì¥(4ë°°) â†’ í™œì„±í™” â†’ ì¶•ì†Œ â†’ ì¶œë ¥

    ì˜ˆì‹œ:
        ì…ë ¥: (ë°°ì¹˜, í† í°, 96ì°¨ì›)
        íˆë“ : (ë°°ì¹˜, í† í°, 384ì°¨ì›)  â† 4ë°° í™•ì¥
        ì¶œë ¥: (ë°°ì¹˜, í† í°, 96ì°¨ì›)
    """

    def __init__(self, in_features, hidden_features=None, out_features=None,
                 act_layer=nn.GELU, drop=0.):
        """
        íŒŒë¼ë¯¸í„°:
            in_features (int): ì…ë ¥ ì°¨ì› (ì˜ˆ: 96)
            hidden_features (int): íˆë“  ì°¨ì› (ì˜ˆ: 384, ê¸°ë³¸ê°’=ì…ë ¥*4)
            out_features (int): ì¶œë ¥ ì°¨ì› (ê¸°ë³¸ê°’=ì…ë ¥ê³¼ ë™ì¼)
            act_layer: í™œì„±í™” í•¨ìˆ˜ (GELU ì‚¬ìš©)
            drop (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        """
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        """
        ìˆœì „íŒŒ ê²½ë¡œ:
        x â†’ Linear1 â†’ GELU â†’ Dropout â†’ Linear2 â†’ Dropout â†’ output
        """
        x = self.fc1(x)      # ì°¨ì› í™•ì¥
        x = self.act(x)      # ë¹„ì„ í˜• í™œì„±í™”
        x = self.drop(x)     # ì •ê·œí™”
        x = self.fc2(x)      # ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
        x = self.drop(x)     # ì •ê·œí™”
        return x


def window_partition(x, window_size):
    """
    ğŸªŸ ì´ë¯¸ì§€ë¥¼ ì‘ì€ ìœˆë„ìš°ë¡œ ë¶„í• 

    í•µì‹¬ ì•„ì´ë””ì–´:
    - ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•´ Attention ê³„ì‚°í•˜ë©´ O(NÂ²)ë¡œ ë„ˆë¬´ ëŠë¦¼
    - ì‘ì€ ìœˆë„ìš°(7x7)ë¡œ ë‚˜ëˆ ì„œ ê°ê° ê³„ì‚°í•˜ë©´ O(MÂ²)ë¡œ ë¹ ë¦„!

    ì‹œê°í™”:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”  â”‚  224x224 ì´ë¯¸ì§€ë¥¼
    â”‚  â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤  â”‚  7x7 ìœˆë„ìš°ë¡œ ë¶„í• 
    â”‚  â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤  â”‚  â†’ 32x32 = 1024ê°œì˜ ìœˆë„ìš°
    â”‚  â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Args:
        x: ì…ë ¥ í…ì„œ (ë°°ì¹˜, ë†’ì´, ë„ˆë¹„, ì±„ë„)
           ì˜ˆ: (2, 56, 56, 96)
        window_size (int): ìœˆë„ìš° í¬ê¸° (ë³´í†µ 7)

    Returns:
        windows: (ìœˆë„ìš°ê°œìˆ˜*ë°°ì¹˜, 7, 7, ì±„ë„)
                 ì˜ˆ: (128, 7, 7, 96)
    """
    B, H, W, C = x.shape

    # ì´ë¯¸ì§€ë¥¼ ìœˆë„ìš° ë‹¨ìœ„ë¡œ ì¬ë°°ì—´
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    ğŸ”„ ìœˆë„ìš°ë“¤ì„ ë‹¤ì‹œ ì›ë³¸ ì´ë¯¸ì§€ë¡œ ë³µì›

    window_partitionì˜ ë°˜ëŒ€ ì—°ì‚°

    Args:
        windows: ìœˆë„ìš° ë°°ì¹˜ (ìœˆë„ìš°ê°œìˆ˜*ë°°ì¹˜, 7, 7, ì±„ë„)
        window_size (int): ìœˆë„ìš° í¬ê¸°
        H, W (int): ì›ë³¸ ì´ë¯¸ì§€ì˜ ë†’ì´ì™€ ë„ˆë¹„

    Returns:
        x: ë³µì›ëœ ì´ë¯¸ì§€ (ë°°ì¹˜, ë†’ì´, ë„ˆë¹„, ì±„ë„)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


# ============================================================================
# ğŸ’¡ Attention ëª¨ë“ˆ (í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜)
# ============================================================================

class WindowAttention(nn.Module):
    """
    ğŸ¯ Window ê¸°ë°˜ Multi-Head Self Attention (W-MSA)

    âœ¨ Swin Transformerì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜!

    ê¸°ì¡´ Vision Transformer ë¬¸ì œ:
        - ì „ì²´ ì´ë¯¸ì§€ì— Attention â†’ ê³„ì‚°ëŸ‰ í­ë°œ ğŸ’¥
        - 224x224 ì´ë¯¸ì§€ = 50,176ê°œ í† í° â†’ O(NÂ²) = 2.5ì–µë²ˆ ì—°ì‚°!

    Swinì˜ í•´ê²°ì±…:
        - 7x7 ìœˆë„ìš°ë¡œ ë¶„í•  â†’ 49ê°œ í† í°ë§Œ ê³„ì‚° â†’ O(49Â²) = 2,401ë²ˆ!
        - 100ë°° ì´ìƒ ë¹ ë¦„! ğŸš€

    íŠ¹ì§•:
        1. Window ë‹¨ìœ„ë¡œ Attention ê³„ì‚° (ê³„ì‚° íš¨ìœ¨ â†‘)
        2. Relative Position Bias ì‚¬ìš© (ìœ„ì¹˜ ì •ë³´ í•™ìŠµ)
        3. Multi-Headë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True,
                 qk_scale=None, attn_drop=0., proj_drop=0.):
        """
        íŒŒë¼ë¯¸í„°:
            dim (int): ì…ë ¥ ì±„ë„ ìˆ˜ (ì˜ˆ: 96)
            window_size (tuple): ìœˆë„ìš° í¬ê¸° (7, 7)
            num_heads (int): Attention Head ê°œìˆ˜ (ì˜ˆ: 3)
            qkv_bias (bool): Query/Key/Valueì— bias ì‚¬ìš© ì—¬ë¶€
            attn_drop (float): Attention ë“œë¡­ì•„ì›ƒ
            proj_drop (float): ì¶œë ¥ í”„ë¡œì ì…˜ ë“œë¡­ì•„ì›ƒ
        """
        super().__init__()
        self.dim = dim
        self.window_size = window_size  # (Wh, Ww)
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5  # Attention ìŠ¤ì¼€ì¼

        # ğŸ“ Relative Position Bias í…Œì´ë¸”
        # ìœ„ì¹˜ ì •ë³´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì €ì¥
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))

        # ìœˆë„ìš° ë‚´ ê° í† í° ìŒì˜ ìƒëŒ€ ìœ„ì¹˜ ê³„ì‚°
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer("relative_position_index", relative_position_index)

        # Query, Key, Valueë¥¼ í•œë²ˆì— ê³„ì‚°
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        ğŸ”„ Window Attention ìˆœì „íŒŒ

        ë‹¨ê³„ë³„ ë™ì‘:
        1ï¸âƒ£ Query, Key, Value ê³„ì‚°
        2ï¸âƒ£ Attention ìŠ¤ì½”ì–´ = Q @ K^T (ë‚´ì )
        3ï¸âƒ£ Relative Position Bias ì¶”ê°€ (ìœ„ì¹˜ ì •ë³´)
        4ï¸âƒ£ Softmaxë¡œ í™•ë¥  ë¶„í¬ ê³„ì‚°
        5ï¸âƒ£ Valueì™€ ê³±í•´ì„œ ìµœì¢… ì¶œë ¥

        Args:
            x: ì…ë ¥ (ìœˆë„ìš°*ë°°ì¹˜, 49í† í°, ì±„ë„)
            mask: Shifted Windowìš© ë§ˆìŠ¤í¬ (ì„ íƒì )

        Returns:
            ì¶œë ¥ (ìœˆë„ìš°*ë°°ì¹˜, 49í† í°, ì±„ë„)
        """
        B_, N, C = x.shape

        # QKV ê³„ì‚° (í•œë²ˆì— ê³„ì‚° í›„ ë¶„ë¦¬)
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Scaled Dot-Product Attention
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))  # (B_, heads, N, N)

        # Relative Position Bias ì¶”ê°€ (Swinì˜ í•µì‹¬!)
        relative_position_bias = self.relative_position_bias_table[
            self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1],
            self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)

        # Shifted Window Masking (í•„ìš”ì‹œ)
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        # Attention ê°€ì¤‘ì¹˜ë¡œ Value ì§‘ê³„
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


# ============================================================================
# ğŸ—ï¸ Transformer ë¸”ë¡
# ============================================================================

class SwinTransformerBlock(nn.Module):
    """
    ğŸ§± Swin Transformer Block (ê¸°ë³¸ êµ¬ì„± ë‹¨ìœ„)

    ë™ì‘ ìˆœì„œ:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ì…ë ¥                                â”‚
    â”‚   â†“                                 â”‚
    â”‚  LayerNorm                          â”‚
    â”‚   â†“                                 â”‚
    â”‚  Window Partition (ìœˆë„ìš° ë¶„í• )      â”‚
    â”‚   â†“                                 â”‚
    â”‚  Window Attention (ì£¼ëª©!)           â”‚
    â”‚   â†“                                 â”‚
    â”‚  Window Merge (ìœˆë„ìš° ë³‘í•©)         â”‚
    â”‚   â†“                                 â”‚
    â”‚  Residual Connection (+)            â”‚
    â”‚   â†“                                 â”‚
    â”‚  LayerNorm                          â”‚
    â”‚   â†“                                 â”‚
    â”‚  MLP (Feed Forward)                 â”‚
    â”‚   â†“                                 â”‚
    â”‚  Residual Connection (+)            â”‚
    â”‚   â†“                                 â”‚
    â”‚  ì¶œë ¥                                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    W-MSA vs SW-MSA:
    - W-MSA (shift=0): ì¼ë°˜ ìœˆë„ìš° Attention
    - SW-MSA (shift>0): ìœˆë„ìš°ë¥¼ ì´ë™ì‹œì¼œ ê²½ê³„ ì •ë³´ êµí™˜
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        """
        íŒŒë¼ë¯¸í„°:
            dim (int): ì±„ë„ ìˆ˜
            input_resolution (tuple): ì…ë ¥ í•´ìƒë„ (H, W)
            num_heads (int): Attention head ê°œìˆ˜
            window_size (int): ìœˆë„ìš° í¬ê¸° (7)
            shift_size (int): ìœˆë„ìš° ì´ë™ í¬ê¸° (0 ë˜ëŠ” window_size//2)
            mlp_ratio (float): MLP í™•ì¥ ë¹„ìœ¨ (4ë°°)
            drop_path (float): Stochastic Depth ë¹„ìœ¨
        """
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio

        # ì…ë ¥ì´ ìœˆë„ìš°ë³´ë‹¤ ì‘ìœ¼ë©´ ìœˆë„ìš° ë¶„í•  ìŠ¤í‚µ
        if min(self.input_resolution) <= self.window_size:
            self.shift_size = 0
            self.window_size = min(self.input_resolution)

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)

    def forward(self, x):
        """
        ìˆœì „íŒŒ (Shifted Windowê°€ í•µì‹¬!)

        SW-MSAì˜ ìœˆë„ìš° ì´ë™ ê°œë…:
        â”Œâ”€â”€â”¬â”€â”€â”    shift    â”Œâ”€â”¬â”€â”€â”€â”¬â”€â”
        â”œâ”€â”€â”¼â”€â”€â”¤    â”€â”€â”€â”€â†’    â”œâ”€â”¼â”€â”€â”€â”¼â”€â”¤  ì¸ì ‘ ìœˆë„ìš° ê°„
        â”œâ”€â”€â”¼â”€â”€â”¤             â”œâ”€â”¼â”€â”€â”€â”¼â”€â”¤  ì •ë³´ êµí™˜!
        â””â”€â”€â”´â”€â”€â”˜             â””â”€â”´â”€â”€â”€â”´â”€â”˜
        """
        H, W = self.input_resolution
        B, L, C = x.shape

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # Cyclic Shift (SW-MSAì—ì„œë§Œ)
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # ìœˆë„ìš° ë¶„í• 
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        # Window Attention
        attn_windows = self.attn(x_windows)

        # ìœˆë„ìš° ë³‘í•©
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)

        # Shift ë³µì›
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # Residual Connection
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x


# ============================================================================
# ğŸ”½ ë‹¤ìš´ìƒ˜í”Œë§ ëª¨ë“ˆ
# ============================================================================

class PatchMerging(nn.Module):
    """
    ğŸ“‰ Patch Merging Layer (ë‹¤ìš´ìƒ˜í”Œë§)

    ë™ì‘: 2x2 íŒ¨ì¹˜ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨

    ì‹œê°í™”:
    â”Œâ”€â”€â”¬â”€â”€â”
    â”‚ 1â”‚ 2â”‚        â”Œâ”€â”€â”€â”€â”
    â”œâ”€â”€â”¼â”€â”€â”¤  â”€â”€â”€â”€â†’ â”‚1234â”‚  í•´ìƒë„ 1/2, ì±„ë„ 2ë°°
    â”‚ 3â”‚ 4â”‚        â””â”€â”€â”€â”€â”˜
    â””â”€â”€â”´â”€â”€â”˜

    ë³€í™”:
        - í•´ìƒë„: (H, W) â†’ (H/2, W/2)
        - ì±„ë„: C â†’ 2C
        - í† í° ìˆ˜: H*W â†’ H/2*W/2 (1/4ë¡œ ê°ì†Œ)

    CNNì˜ Poolingê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì •ë³´ ì†ì‹¤ ì ìŒ!
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        """
        íŒŒë¼ë¯¸í„°:
            input_resolution (tuple): ì…ë ¥ í•´ìƒë„ (H, W)
            dim (int): ì…ë ¥ ì±„ë„ ìˆ˜
        """
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        2x2 íŒ¨ì¹˜ë¥¼ ë³‘í•©í•˜ì—¬ ë‹¤ìš´ìƒ˜í”Œë§

        ì˜ˆì‹œ:
            ì…ë ¥: (B, 56*56, 96)
            ì¶œë ¥: (B, 28*28, 192)
        """
        H, W = self.input_resolution
        B, L, C = x.shape

        x = x.view(B, H, W, C)

        # 2x2 íŒ¨ì¹˜ì˜ 4ê°œ ìœ„ì¹˜ ì¶”ì¶œ
        x0 = x[:, 0::2, 0::2, :]  # ì¢Œìƒë‹¨
        x1 = x[:, 1::2, 0::2, :]  # ì¢Œí•˜ë‹¨
        x2 = x[:, 0::2, 1::2, :]  # ìš°ìƒë‹¨
        x3 = x[:, 1::2, 1::2, :]  # ìš°í•˜ë‹¨
        x = torch.cat([x0, x1, x2, x3], -1)  # ì±„ë„ ë°©í–¥ ê²°í•© (4C)
        x = x.view(B, -1, 4 * C)

        x = self.norm(x)
        x = self.reduction(x)  # 4C â†’ 2C ì°¨ì› ì¶•ì†Œ

        return x


# ============================================================================
# ğŸ­ Stage êµ¬ì„±
# ============================================================================

class BasicLayer(nn.Module):
    """
    ğŸª Swin Transformer Stage (ì—¬ëŸ¬ ë¸”ë¡ì˜ ì§‘í•©)

    êµ¬ì¡°:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Block 0 (W-MSA)                â”‚
    â”‚  Block 1 (SW-MSA)  â† shift!     â”‚
    â”‚  Block 2 (W-MSA)                â”‚
    â”‚  Block 3 (SW-MSA)  â† shift!     â”‚
    â”‚  ...                            â”‚
    â”‚  Patch Merging (ë‹¤ìš´ìƒ˜í”Œë§)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    íŠ¹ì§•:
        - W-MSAì™€ SW-MSAë¥¼ ë²ˆê°ˆì•„ ì‚¬ìš©
        - ì¸ì ‘ ìœˆë„ìš° ê°„ ì •ë³´ êµí™˜ ê°€ëŠ¥!
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):
        """
        íŒŒë¼ë¯¸í„°:
            dim (int): ì±„ë„ ìˆ˜
            input_resolution (tuple): ì…ë ¥ í•´ìƒë„
            depth (int): ì´ Stageì˜ ë¸”ë¡ ê°œìˆ˜
            num_heads (int): Attention head ê°œìˆ˜
            window_size (int): ìœˆë„ìš° í¬ê¸°
            downsample: ë‹¤ìš´ìƒ˜í”Œë§ ë ˆì´ì–´ (PatchMerging)
        """
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth

        # Transformer Block ìƒì„±
        # ì§ìˆ˜: W-MSA, í™€ìˆ˜: SW-MSA
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim, input_resolution=input_resolution,
                num_heads=num_heads, window_size=window_size,
                shift_size=0 if (i % 2 == 0) else window_size // 2,  # êµëŒ€ë¡œ!
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop, attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                norm_layer=norm_layer)
            for i in range(depth)])

        # ë‹¤ìš´ìƒ˜í”Œë§ ë ˆì´ì–´
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        """ëª¨ë“  ë¸”ë¡ í†µê³¼ í›„ ë‹¤ìš´ìƒ˜í”Œë§"""
        for blk in self.blocks:
            x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x


# ============================================================================
# ğŸ–¼ï¸ íŒ¨ì¹˜ ì„ë² ë”©
# ============================================================================

class PatchEmbed(nn.Module):
    """
    ğŸ§© Image to Patch Embedding (ì´ë¯¸ì§€ â†’ íŒ¨ì¹˜ ì„ë² ë”©)

    ë™ì‘:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  224x224 ì´ë¯¸ì§€      â”‚
    â”‚  (6ì±„ë„)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ Convolution (stride=4)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  56x56 íŒ¨ì¹˜          â”‚
    â”‚  (96ì±„ë„)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    íŠ¹ì§•:
        - 6ì±„ë„ ì…ë ¥ ì§€ì› (RGB 3ì±„ë„ + ë§ˆìŠ¤í¬ 3ì±„ë„)
        - 4x4 íŒ¨ì¹˜ë¡œ ë¶„í•  (224/4 = 56)
        - ì´ 3,136ê°œ íŒ¨ì¹˜ ìƒì„± (56*56)
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=6, embed_dim=96, norm_layer=None):
        """
        íŒŒë¼ë¯¸í„°:
            img_size (int): ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (224)
            patch_size (int): íŒ¨ì¹˜ í¬ê¸° (4)
            in_chans (int): ì…ë ¥ ì±„ë„ (6: RGB + Mask)
            embed_dim (int): ì„ë² ë”© ì°¨ì› (96)
        """
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]

        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]  # 3136
        self.in_chans = in_chans
        self.embed_dim = embed_dim

        # Convolutionìœ¼ë¡œ íŒ¨ì¹˜ ì¶”ì¶œ
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        """
        ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜

        ë³€í™˜ ê³¼ì •:
            (B, 6, 224, 224) â†’ Conv2d â†’ (B, 96, 56, 56)
            â†’ Flatten â†’ (B, 3136, 96)
        """
        B, C, H, W = x.shape

        x = self.proj(x).flatten(2).transpose(1, 2)
        if self.norm is not None:
            x = self.norm(x)
        return x


# ============================================================================
# ğŸ¨ ë©”ì¸ ëª¨ë¸
# ============================================================================

class SwinHairClassifier(nn.Module):
    """
    ===================================================================
    ğŸ† Swin Transformer ê¸°ë°˜ íƒˆëª¨ ë¶„ë¥˜ ëª¨ë¸ (ë©”ì¸ ëª¨ë¸)
    ===================================================================

    ğŸ“Š ì „ì²´ ì•„í‚¤í…ì²˜:

    ì…ë ¥: (B, 6, 224, 224)  â† RGB + í—¤ì–´ ë§ˆìŠ¤í¬
      â†“
    [Patch Embed] 56x56, 96ì±„ë„
      â†“
    [Stage 1] 56x56, 96ì±„ë„, 2ë¸”ë¡   â† W-MSA, SW-MSA
      â†“ Patch Merging
    [Stage 2] 28x28, 192ì±„ë„, 2ë¸”ë¡  â† W-MSA, SW-MSA
      â†“ Patch Merging
    [Stage 3] 14x14, 384ì±„ë„, 6ë¸”ë¡  â† ê°€ì¥ ë§ì€ ì—°ì‚°!
      â†“ Patch Merging
    [Stage 4] 7x7, 768ì±„ë„, 2ë¸”ë¡    â† W-MSA, SW-MSA
      â†“ Global Average Pooling
    [Classification] 768 â†’ 256 â†’ 4í´ë˜ìŠ¤
      â†“
    ì¶œë ¥: (B, 4)  â† íƒˆëª¨ ë ˆë²¨ 0~3

    ===================================================================

    ğŸ“ˆ ëª¨ë¸ í¬ê¸°:
        - Tiny: ~28M íŒŒë¼ë¯¸í„°, ë¹ ë¥¸ ì¶”ë¡ 
        - Small: ~50M íŒŒë¼ë¯¸í„°, ë” ì •í™•í•œ ì˜ˆì¸¡

    ğŸ¯ íƒˆëª¨ ë ˆë²¨:
        - Level 0: ì •ìƒ (íƒˆëª¨ ì—†ìŒ)
        - Level 1: ê²½ë¯¸ (ì´ˆê¸° ë‹¨ê³„)
        - Level 2: ì¤‘ë“±ë„ (ì§„í–‰ ì¤‘)
        - Level 3: ì‹¬ê° (ìƒë‹¹í•œ íƒˆëª¨)
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=6, num_classes=4,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        """
        íŒŒë¼ë¯¸í„°:
            img_size (int): ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (224)
            in_chans (int): ì…ë ¥ ì±„ë„ (6: RGB + Mask)
            num_classes (int): ë¶„ë¥˜ í´ë˜ìŠ¤ (4: ë ˆë²¨ 0~3)
            embed_dim (int): ì´ˆê¸° ì„ë² ë”© ì°¨ì› (96)
            depths (list): ê° Stageì˜ ë¸”ë¡ ê°œìˆ˜ [2,2,6,2]
            num_heads (list): ê° Stageì˜ head ê°œìˆ˜ [3,6,12,24]
            window_size (int): Attention ìœˆë„ìš° í¬ê¸° (7)
        """
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)  # 4ê°œ Stage
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))  # 768
        self.mlp_ratio = mlp_ratio

        # ğŸ§© Patch Embedding
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans,
            embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # ğŸ“ Position Embedding (ì„ íƒì )
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # ğŸ“‰ Stochastic Depth (ê¹Šì–´ì§ˆìˆ˜ë¡ í™•ë¥  ì¦ê°€)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

        # ğŸ­ 4ê°œ Stage ìƒì„±
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(
                dim=int(embed_dim * 2 ** i_layer),  # 96, 192, 384, 768
                input_resolution=(
                    patches_resolution[0] // (2 ** i_layer),
                    patches_resolution[1] // (2 ** i_layer)
                ),  # 56, 28, 14, 7
                depth=depths[i_layer],  # 2, 2, 6, 2
                num_heads=num_heads[i_layer],  # 3, 6, 12, 24
                window_size=window_size,
                mlp_ratio=self.mlp_ratio,
                qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate,
                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                norm_layer=norm_layer,
                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)

        # ğŸ¯ Classification Head
        self.head = nn.Sequential(
            nn.Linear(self.num_features, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

        self.apply(self._init_weights)

    def _init_weights(self, m):
        """
        ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        - Linear: Truncated Normal
        - LayerNorm: bias=0, weight=1
        """
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        """
        ğŸ” íŠ¹ì§• ì¶”ì¶œ íŒŒì´í”„ë¼ì¸

        íë¦„:
        1. Patch Embedding (224x224 â†’ 56x56 íŒ¨ì¹˜)
        2. Stage 1~4 í†µê³¼ (ê³„ì¸µì  íŠ¹ì§• ì¶”ì¶œ)
        3. Global Average Pooling (ê³µê°„ ì •ë³´ í†µí•©)

        Args:
            x: (B, 6, 224, 224)

        Returns:
            íŠ¹ì§• ë²¡í„° (B, 768)
        """
        x = self.patch_embed(x)  # (B, 3136, 96)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        # 4ê°œ Stage í†µê³¼
        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)
        x = self.avgpool(x.transpose(1, 2))  # Global Average Pooling
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        """
        ğŸš€ ì „ì²´ ìˆœì „íŒŒ

        ì…ë ¥ â†’ íŠ¹ì§• ì¶”ì¶œ â†’ ë¶„ë¥˜ â†’ ì¶œë ¥

        Args:
            x: (B, 6, 224, 224) - RGB + í—¤ì–´ ë§ˆìŠ¤í¬

        Returns:
            (B, 4) - íƒˆëª¨ ë ˆë²¨ ë¡œì§“
        """
        x = self.forward_features(x)  # íŠ¹ì§• ì¶”ì¶œ
        x = self.head(x)  # ë¶„ë¥˜
        return x


# ============================================================================
# ğŸ­ ëª¨ë¸ íŒ©í† ë¦¬ í•¨ìˆ˜
# ============================================================================

def swin_tiny_patch4_window7_224_hair(num_classes=4, **kwargs):
    """
    ğŸ”¹ Swin-Tiny ëª¨ë¸ (ê²½ëŸ‰ ë²„ì „)

    ì‚¬ì–‘:
        - íŒŒë¼ë¯¸í„°: ~28M
        - Stage ë¸”ë¡: [2, 2, 6, 2]
        - ì¶”ë¡  ì†ë„: âš¡âš¡âš¡ ë¹ ë¦„
        - ì •í™•ë„: â­â­â­ ì¢‹ìŒ

    ìš©ë„: ì‹¤ì‹œê°„ ë¶„ì„, ëª¨ë°”ì¼ ë°°í¬
    """
    model = SwinHairClassifier(
        patch_size=4,
        window_size=7,
        embed_dim=96,
        depths=[2, 2, 6, 2],  # Tiny
        num_heads=[3, 6, 12, 24],
        num_classes=num_classes,
        **kwargs
    )
    return model


def swin_small_patch4_window7_224_hair(num_classes=4, **kwargs):
    """
    ğŸ”¸ Swin-Small ëª¨ë¸ (ê³ ì„±ëŠ¥ ë²„ì „)

    ì‚¬ì–‘:
        - íŒŒë¼ë¯¸í„°: ~50M
        - Stage ë¸”ë¡: [2, 2, 18, 2]  â† Stage 3ì´ 18ë¸”ë¡!
        - ì¶”ë¡  ì†ë„: âš¡âš¡ ì¤‘ê°„
        - ì •í™•ë„: â­â­â­â­ ë§¤ìš° ì¢‹ìŒ

    ìš©ë„: ì„œë²„ ê¸°ë°˜ ë¶„ì„, ê³ ì •ë°€ ì§„ë‹¨
    """
    model = SwinHairClassifier(
        patch_size=4,
        window_size=7,
        embed_dim=96,
        depths=[2, 2, 18, 2],  # Small (Stage 3ì´ 18ë¸”ë¡)
        num_heads=[3, 6, 12, 24],
        num_classes=num_classes,
        **kwargs
    )
    return model


# ============================================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================================================

if __name__ == "__main__":
    """
    ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

    í…ŒìŠ¤íŠ¸ ë‚´ìš©:
    1. ëª¨ë¸ ìƒì„±
    2. 6ì±„ë„ ì…ë ¥ í…ŒìŠ¤íŠ¸
    3. ì¶œë ¥ shape í™•ì¸
    4. í´ë˜ìŠ¤ë³„ í™•ë¥  ì¶œë ¥
    """
    print("=" * 60)
    print("ğŸ§ª Swin Transformer íƒˆëª¨ ë¶„ë¥˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # Swin-Tiny ëª¨ë¸ ìƒì„±
    model = swin_tiny_patch4_window7_224_hair(num_classes=4)

    # ë”ë¯¸ ì…ë ¥ ìƒì„±
    batch_size = 2
    rgb_image = torch.randn(batch_size, 3, 224, 224)    # RGB ì´ë¯¸ì§€
    mask_image = torch.randn(batch_size, 3, 224, 224)   # í—¤ì–´ ë§ˆìŠ¤í¬
    dual_input = torch.cat([rgb_image, mask_image], dim=1)  # 6ì±„ë„ë¡œ ê²°í•©

    print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"   - ì…ë ¥ shape: {dual_input.shape}")
    print(f"   - íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")

    # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    with torch.no_grad():
        output = model(dual_input)
        print(f"\nâœ… ìˆœì „íŒŒ ì„±ê³µ!")
        print(f"   - ì¶œë ¥ shape: {output.shape}")
        print(f"   - ì¶œë ¥ ë¡œì§“: {output[0]}")

        # Softmaxë¡œ í™•ë¥  ë³€í™˜
        probs = torch.softmax(output, dim=1)
        print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ í™•ë¥  (ìƒ˜í”Œ 1):")
        for i, prob in enumerate(probs[0]):
            level_name = ["ì •ìƒ", "ê²½ë¯¸", "ì¤‘ë“±ë„", "ì‹¬ê°"][i]
            print(f"   Level {i} ({level_name}): {prob.item():.2%}")

    print("\n" + "=" * 60)
    print("âœ¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
