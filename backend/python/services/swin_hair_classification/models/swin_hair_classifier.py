"""
============================================================================
ğŸ§  íƒˆëª¨ AI ë¶„ì„ ëª¨ë¸ - ì´ˆë³´ìë¥¼ ìœ„í•œ ì‰¬ìš´ ì„¤ëª…
============================================================================

ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
ğŸ“¸ ì‚¬ì§„ â†’ ğŸ¤– AI ë¶„ì„ â†’ ğŸ“Š íƒˆëª¨ ë‹¨ê³„ íŒë‹¨ (0~3ë‹¨ê³„)

ì‰½ê²Œ ë§í•˜ë©´:
- ì •ìˆ˜ë¦¬ ì‚¬ì§„ + ì¸¡ë©´ ì‚¬ì§„ì„ ë°›ì•„ì„œ
- "ì´ ì‚¬ëŒì˜ íƒˆëª¨ê°€ ì–¼ë§ˆë‚˜ ì§„í–‰ë˜ì—ˆëŠ”ì§€" íŒë‹¨í•˜ëŠ” AIì…ë‹ˆë‹¤

ì…ë ¥: RGB ì‚¬ì§„(ì¼ë°˜ ì‚¬ì§„) + í—¤ì–´ ë§ˆìŠ¤í¬(ë¨¸ë¦¬ì¹´ë½ë§Œ ê°•ì¡°í•œ ì‚¬ì§„) = ì´ 6ì±„ë„
ì¶œë ¥: 0(ì •ìƒ), 1(ê²½ë¯¸), 2(ì¤‘ë“±ë„), 3(ì‹¬ê°) ì¤‘ í•˜ë‚˜

============================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
import math


# ============================================================================
# ğŸ“¦ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ (ë ˆê³  ë¸”ë¡ ê°™ì€ ê²ƒë“¤)
# ============================================================================

class Mlp(nn.Module):
    """
    ğŸ”¹ MLP = Multi-Layer Perceptron (ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ )

    ì‰½ê²Œ ì„¤ëª…:
    - "ê³„ì‚°ê¸°" ì—­í• ì„ í•˜ëŠ” ë¸”ë¡ì…ë‹ˆë‹¤
    - ì…ë ¥ë°›ì€ ìˆ«ìë“¤ì„ ì—¬ëŸ¬ ë²ˆ ê³„ì‚°í•´ì„œ ì˜ë¯¸ìˆëŠ” ìˆ«ìë¡œ ë°”ê¿‰ë‹ˆë‹¤

    ë¹„ìœ :
    - ë§ˆì¹˜ ìš”ë¦¬ì‚¬ê°€ ì¬ë£Œë¥¼ ì—¬ëŸ¬ ë²ˆ ì†ì§ˆí•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤
    - ìƒê³ ê¸°(ì…ë ¥) â†’ ì†ì§ˆ(ê³„ì‚°1) â†’ ì–‘ë…(ê³„ì‚°2) â†’ ì™„ì„±ëœ ìš”ë¦¬(ì¶œë ¥)

    ì‹¤ì œ ë™ì‘:
    1. ìˆ«ìë¥¼ 4ë°°ë¡œ ëŠ˜ë¦½ë‹ˆë‹¤ (ë” ë§ì€ ì •ë³´ ì¶”ì¶œ)
    2. ë¹„ì„ í˜• ë³€í™˜ (ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ)
    3. ë‹¤ì‹œ ì›ë˜ í¬ê¸°ë¡œ ì¤„ì…ë‹ˆë‹¤
    """

    def __init__(self, in_features, hidden_features=None, out_features=None,
                 act_layer=nn.GELU, drop=0.):
        """
        ì„¤ì •:
            in_features: ì…ë ¥ ìˆ«ì ê°œìˆ˜ (ì˜ˆ: 96ê°œ)
            hidden_features: ì¤‘ê°„ ê³„ì‚°ìš© ìˆ«ì ê°œìˆ˜ (ì˜ˆ: 384ê°œ, ë³´í†µ 4ë°°)
            drop: ê³¼ì í•© ë°©ì§€ìš© (0.1 = 10%ë¥¼ ëœë¤í•˜ê²Œ ë”)
        """
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features

        self.fc1 = nn.Linear(in_features, hidden_features)  # ì²« ë²ˆì§¸ ê³„ì‚°
        self.act = act_layer()                               # ë¹„ì„ í˜• ë³€í™˜
        self.fc2 = nn.Linear(hidden_features, out_features) # ë‘ ë²ˆì§¸ ê³„ì‚°
        self.drop = nn.Dropout(drop)                         # ê³¼ì í•© ë°©ì§€

    def forward(self, x):
        """
        ì‹¤ì œ ê³„ì‚° ê³¼ì •:
        ì…ë ¥ â†’ í™•ì¥ â†’ í™œì„±í™” â†’ ì¶•ì†Œ â†’ ì¶œë ¥
        """
        x = self.fc1(x)      # ìˆ«ì ê°œìˆ˜ë¥¼ 4ë°°ë¡œ ëŠ˜ë¦¼
        x = self.act(x)      # ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë³€í™˜
        x = self.drop(x)     # ëœë¤í•˜ê²Œ ì¼ë¶€ë¥¼ 0ìœ¼ë¡œ (ê³¼ì í•© ë°©ì§€)
        x = self.fc2(x)      # ë‹¤ì‹œ ì›ë˜ ìˆ«ì ê°œìˆ˜ë¡œ
        x = self.drop(x)     # í•œë²ˆ ë” ê³¼ì í•© ë°©ì§€
        return x


def window_partition(x, window_size):
    """
    ğŸªŸ ì‚¬ì§„ì„ ì‘ì€ ì¡°ê°(ìœˆë„ìš°)ìœ¼ë¡œ ë‚˜ëˆ„ê¸°

    ì™œ ë‚˜ëˆ„ë‚˜ìš”?
    - ì‚¬ì§„ ì „ì²´ë¥¼ í•œë²ˆì— ë³´ë©´ ê³„ì‚°ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤
    - ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ ì„œ ê°ê° ë³´ë©´ ë¹ ë¦…ë‹ˆë‹¤!

    ë¹„ìœ :
    - í° í¼ì¦ì„ í•œë²ˆì— ë§ì¶”ê¸° vs ì‘ì€ ì˜ì—­ì”© ë§ì¶”ê¸°
    - ì‘ì€ ì˜ì—­ì”© ë³´ëŠ”ê²Œ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤!

    ê·¸ë¦¼ìœ¼ë¡œ ë³´ë©´:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ì „ì²´ ì‚¬ì§„ 224x224â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ 7x7ì”© ìë¥´ê¸°
    â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”
    â”‚ 1â”‚ 2â”‚ 3â”‚ 4â”‚  ê°ê° 7x7 í¬ê¸°
    â”œâ”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¤  ì´ 32x32 = 1024ì¡°ê°
    â”‚ 5â”‚ 6â”‚ 7â”‚ 8â”‚
    â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜

    ì…ë ¥: í° ì‚¬ì§„ (B, H, W, C)
          ì˜ˆ: (2ì¥, 56í”½ì…€, 56í”½ì…€, 96ì±„ë„)
    ì¶œë ¥: ì‘ì€ ì¡°ê°ë“¤ (ì¡°ê°ê°œìˆ˜*ì‚¬ì§„ê°œìˆ˜, 7, 7, ì±„ë„)
          ì˜ˆ: (128ì¡°ê°, 7í”½ì…€, 7í”½ì…€, 96ì±„ë„)
    """
    B, H, W, C = x.shape

    # ì‚¬ì§„ì„ 7x7 ì¡°ê°ìœ¼ë¡œ ìë¥´ê¸°
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    ğŸ”„ ì‘ì€ ì¡°ê°ë“¤ì„ ë‹¤ì‹œ í° ì‚¬ì§„ìœ¼ë¡œ í•©ì¹˜ê¸°

    window_partitionì˜ ë°˜ëŒ€ ì‘ì—…ì…ë‹ˆë‹¤.
    ì¡°ê°ë‚œ í¼ì¦ì„ ë‹¤ì‹œ ì™„ì„±ëœ ê·¸ë¦¼ìœ¼ë¡œ ë§ì¶”ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


# ============================================================================
# ğŸ‘ï¸ Attention ë©”ì»¤ë‹ˆì¦˜ (AIê°€ "ì£¼ëª©"í•˜ëŠ” ë°©ë²•)
# ============================================================================

class WindowAttention(nn.Module):
    """
    ğŸ¯ Window Attention - AIê°€ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ê¸°ëŠ¥

    "Attention"ì´ ë­”ê°€ìš”?
    - ì‚¬ëŒì´ ì‚¬ì§„ì„ ë³¼ ë•Œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì‹œì„ ì´ ê°€ëŠ” ê²ƒì²˜ëŸ¼
    - AIë„ ì´ë¯¸ì§€ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì— "ì£¼ëª©"í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤

    ì˜ˆì‹œ:
    - íƒˆëª¨ ì§„ë‹¨í•  ë•Œ:
      âœ“ í—¤ì–´ë¼ì¸ì˜ í›„í‡´ ì •ë„ â† ì—¬ê¸° ì§‘ì¤‘!
      âœ“ ì •ìˆ˜ë¦¬ì˜ ë¨¸ë¦¬ì¹´ë½ ë°€ë„ â† ì—¬ê¸°ë„ ì§‘ì¤‘!
      âœ— ë°°ê²½ì€ ë³„ë¡œ ì•ˆ ì¤‘ìš” â† ë¬´ì‹œ

    ì™œ "Window" Attentionì¸ê°€ìš”?
    - ì‚¬ì§„ ì „ì²´ë¥¼ ë³´ë©´ ê³„ì‚°ì´ ë„ˆë¬´ ë§ì•„ì§‘ë‹ˆë‹¤ (50,000ê°œ í”½ì…€ ì „ë¶€ ë¹„êµ = 25ì–µë²ˆ ê³„ì‚°!)
    - 7x7 ì‘ì€ ìœˆë„ìš°ë¡œ ë‚˜ëˆ„ë©´ (49ê°œë§Œ ë¹„êµ = 2,400ë²ˆ ê³„ì‚°!)
    - 1000ë°° ì´ìƒ ë¹ ë¦…ë‹ˆë‹¤! ğŸš€

    Multi-HeadëŠ” ë­”ê°€ìš”?
    - "ì—¬ëŸ¬ ê´€ì ì—ì„œ ë³´ê¸°"ì…ë‹ˆë‹¤
    - í—¤ë“œ1: ë¨¸ë¦¬ì¹´ë½ ë°€ë„ ê´€ì 
    - í—¤ë“œ2: í—¤ì–´ë¼ì¸ ëª¨ì–‘ ê´€ì 
    - í—¤ë“œ3: ë‘í”¼ ìƒíƒœ ê´€ì 
    - ì´ë ‡ê²Œ ì—¬ëŸ¬ ê´€ì ì„ ì¢…í•©í•´ì„œ íŒë‹¨í•©ë‹ˆë‹¤
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True,
                 qk_scale=None, attn_drop=0., proj_drop=0.):
        """
        ì„¤ì •:
            dim: ì±„ë„ ìˆ˜ (ì˜ˆ: 96)
            window_size: ìœˆë„ìš° í¬ê¸° (7x7)
            num_heads: ê´€ì  ê°œìˆ˜ (ì˜ˆ: 3ê°œ ê´€ì )
            attn_drop: ê³¼ì í•© ë°©ì§€ ë¹„ìœ¨
        """
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # ğŸ“ Relative Position Bias (ìƒëŒ€ì  ìœ„ì¹˜ ì •ë³´)
        # "í—¤ì–´ë¼ì¸ì€ ìœ„ìª½ì— ìˆê³ , ì •ìˆ˜ë¦¬ëŠ” ì¤‘ì•™ì— ìˆë‹¤"ëŠ” ìœ„ì¹˜ ì •ë³´ë¥¼ í•™ìŠµ
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))

        # ê° í”½ì…€ ê°„ì˜ ìƒëŒ€ì  ìœ„ì¹˜ ê³„ì‚°
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer("relative_position_index", relative_position_index)

        # Query, Key, Value ê³„ì‚° ë ˆì´ì–´
        # Q: "ë‚˜ëŠ” ë¬´ì—‡ì„ ì°¾ê³  ìˆë‚˜?"
        # K: "ë‚˜ëŠ” ì–´ë–¤ íŠ¹ì§•ì„ ê°€ì¡Œë‚˜?"
        # V: "ë‚˜ì˜ ì‹¤ì œ ê°’ì€ ë¬´ì—‡ì¸ê°€?"
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        ğŸ”„ Attention ê³„ì‚° ê³¼ì •

        ì‰¬ìš´ ë¹„ìœ :
        1. ê° í”½ì…€ì´ "ë‚˜ëŠ” ì´ëŸ° ê²ƒì„ ì°¾ê³  ìˆì–´"ë¼ê³  ì§ˆë¬¸(Q)
        2. ë‹¤ë¥¸ í”½ì…€ë“¤ì´ "ë‚˜ëŠ” ì´ëŸ° íŠ¹ì§•ì´ ìˆì–´"ë¼ê³  ëŒ€ë‹µ(K)
        3. ë¹„ìŠ·í•œ ê²ƒë“¤ë¼ë¦¬ ì ìˆ˜ë¥¼ ë†’ê²Œ ë§¤ê¹€ (Qì™€ Kì˜ ìœ ì‚¬ë„)
        4. ì ìˆ˜ê°€ ë†’ì€ ê²ƒë“¤ì˜ ê°’(V)ì„ ë§ì´ ë°˜ì˜

        ì‹¤ì œ ë™ì‘:
        1ï¸âƒ£ Query, Key, Value ê³„ì‚°
        2ï¸âƒ£ Qì™€ Kë¥¼ ë¹„êµí•´ì„œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        3ï¸âƒ£ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€ (ì–´ë””ì— ìˆëŠ”ì§€ë„ ì¤‘ìš”!)
        4ï¸âƒ£ ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (í•©ì´ 100%ê°€ ë˜ë„ë¡)
        5ï¸âƒ£ ì ìˆ˜ì— ë”°ë¼ Valueë¥¼ ê°€ì¤‘í‰ê· 

        ì…ë ¥: (ìœˆë„ìš°ê°œìˆ˜, 49í”½ì…€, ì±„ë„)
        ì¶œë ¥: (ìœˆë„ìš°ê°œìˆ˜, 49í”½ì…€, ì±„ë„) - ì£¼ëª©í•œ ê²°ê³¼
        """
        B_, N, C = x.shape

        # Q, K, Vë¥¼ í•œë²ˆì— ê³„ì‚° í›„ ë¶„ë¦¬
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Attention ì ìˆ˜ ê³„ì‚° (ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€)
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        # ìœ„ì¹˜ ì •ë³´ ì¶”ê°€ (ê°™ì€ íŠ¹ì§•ì´ë¼ë„ ìœ„ì¹˜ì— ë”°ë¼ ì˜ë¯¸ê°€ ë‹¤ë¦„)
        relative_position_bias = self.relative_position_bias_table[
            self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1],
            self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)

        # ë§ˆìŠ¤í‚¹ (í•„ìš”í•œ ê²½ìš°)
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        # Attention ì ìˆ˜ë¡œ Value ê°€ì¤‘í‰ê· 
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


# ============================================================================
# ğŸ§± Transformer ë¸”ë¡ (ë©”ì¸ ì²˜ë¦¬ ë‹¨ìœ„)
# ============================================================================

class SwinTransformerBlock(nn.Module):
    """
    ğŸ§± Swin Transformer Block - ì´ë¯¸ì§€ ì²˜ë¦¬ì˜ ê¸°ë³¸ ë‹¨ìœ„

    ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
    - ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì¤‘ìš”í•œ íŒ¨í„´ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤
    - "ì´ ë¶€ë¶„ì— ë¨¸ë¦¬ì¹´ë½ì´ ì ë„¤", "ì—¬ê¸° í—¤ì–´ë¼ì¸ì´ í›„í‡´í–ˆë„¤" ê°™ì€ ì •ë³´ ì¶”ì¶œ

    ì²˜ë¦¬ ê³¼ì • (ìˆœì„œëŒ€ë¡œ):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ì…ë ¥ ì´ë¯¸ì§€ ì¡°ê°         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. ì •ê·œí™” (ê°’ ì•ˆì •í™”)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 2. ìœˆë„ìš°ë¡œ ë‚˜ëˆ„ê¸°       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 3. Attention (ì£¼ëª©!)     â”‚
    â”‚    ì¤‘ìš”í•œ ë¶€ë¶„ ì°¾ê¸°      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 4. ë‹¤ì‹œ í•©ì¹˜ê¸°           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 5. ì›ë³¸ê³¼ í•©ì¹˜ê¸° (+)     â”‚
    â”‚    (ì •ë³´ ë³´ì¡´)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 6. MLP (ì¶”ê°€ ê³„ì‚°)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 7. ë˜ í•©ì¹˜ê¸° (+)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ì¶œë ¥ (íŒ¨í„´ ì •ë³´ í¬í•¨)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    W-MSA vs SW-MSA:
    - W-MSA: ì¼ë°˜ ìœˆë„ìš°ë¡œ ë‚˜ëˆ”
    - SW-MSA: ìœˆë„ìš°ë¥¼ ì¡°ê¸ˆ ì´ë™ì‹œì¼œì„œ ê²½ê³„ ì •ë³´ë„ ë´„

    ë¹„ìœ :
    - W-MSA: í¼ì¦ ì¡°ê°ì„ ê·¸ëŒ€ë¡œ ë³´ê¸°
    - SW-MSA: í¼ì¦ ì¡°ê°ì„ ì¡°ê¸ˆ ë°€ì–´ì„œ ì—°ê²° ë¶€ë¶„ë„ ë³´ê¸°
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        """
        ì„¤ì •:
            dim: ì±„ë„ ìˆ˜ (ì •ë³´ëŸ‰)
            input_resolution: ì´ë¯¸ì§€ í¬ê¸° (ë†’ì´, ë„ˆë¹„)
            num_heads: ê´€ì  ê°œìˆ˜
            window_size: ìœˆë„ìš° í¬ê¸° (ë³´í†µ 7x7)
            shift_size: ìœˆë„ìš° ì´ë™ í¬ê¸° (0ì´ë©´ W-MSA, >0ì´ë©´ SW-MSA)
            mlp_ratio: MLP í™•ì¥ ë¹„ìœ¨ (ë³´í†µ 4ë°°)
        """
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio

        # ì´ë¯¸ì§€ê°€ ìœˆë„ìš°ë³´ë‹¤ ì‘ìœ¼ë©´ ìœˆë„ìš° ë‚˜ëˆ„ê¸° ìŠ¤í‚µ
        if min(self.input_resolution) <= self.window_size:
            self.shift_size = 0
            self.window_size = min(self.input_resolution)

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)

    def forward(self, x):
        """
        ì‹¤ì œ ì²˜ë¦¬ ê³¼ì •

        SW-MSAì˜ ìœˆë„ìš° ì´ë™:
        â”Œâ”€â”€â”¬â”€â”€â”     ì´ë™     â”Œâ”€â”¬â”€â”€â”€â”¬â”€â”
        â”œâ”€â”€â”¼â”€â”€â”¤    â”€â”€â”€â”€â†’    â”œâ”€â”¼â”€â”€â”€â”¼â”€â”¤  ê²½ê³„ ë¶€ë¶„ë„
        â”œâ”€â”€â”¼â”€â”€â”¤             â”œâ”€â”¼â”€â”€â”€â”¼â”€â”¤  ë³¼ ìˆ˜ ìˆìŒ!
        â””â”€â”€â”´â”€â”€â”˜             â””â”€â”´â”€â”€â”€â”´â”€â”˜
        """
        H, W = self.input_resolution
        B, L, C = x.shape

        shortcut = x  # ì›ë³¸ ì €ì¥ (ë‚˜ì¤‘ì— ë”í•˜ê¸° ìœ„í•´)
        x = self.norm1(x)  # ê°’ ì•ˆì •í™”
        x = x.view(B, H, W, C)

        # Cyclic Shift (SW-MSAì¸ ê²½ìš° ìœˆë„ìš° ì´ë™)
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # ìœˆë„ìš°ë¡œ ë‚˜ëˆ„ê¸°
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        # Attention ìˆ˜í–‰ (ì¤‘ìš”í•œ ë¶€ë¶„ ì°¾ê¸°)
        attn_windows = self.attn(x_windows)

        # ìœˆë„ìš° ë‹¤ì‹œ í•©ì¹˜ê¸°
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)

        # Shift ë³µì›
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # ì›ë³¸ê³¼ í•©ì¹˜ê¸° (Residual Connection)
        # ì´ìœ : ì›ë³¸ ì •ë³´ë¥¼ ìƒì§€ ì•Šê¸° ìœ„í•´
        x = shortcut + self.drop_path(x)

        # MLPë¡œ ì¶”ê°€ ì²˜ë¦¬ í›„ ë˜ í•©ì¹˜ê¸°
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x


# ============================================================================
# ğŸ“‰ ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸° (ë‹¤ìš´ìƒ˜í”Œë§)
# ============================================================================

class PatchMerging(nn.Module):
    """
    ğŸ“‰ Patch Merging - ì´ë¯¸ì§€ë¥¼ ì‘ê²Œ ë§Œë“¤ê¸°

    ì™œ ì‘ê²Œ ë§Œë“œë‚˜ìš”?
    - í° ì´ë¯¸ì§€: ì‘ì€ ë””í…Œì¼(ë¨¸ë¦¬ì¹´ë½ í•œ ì˜¬)ì„ ë³¼ ìˆ˜ ìˆìŒ
    - ì‘ì€ ì´ë¯¸ì§€: ì „ì²´ì ì¸ íŒ¨í„´(í—¤ì–´ë¼ì¸ ëª¨ì–‘)ì„ ë³¼ ìˆ˜ ìˆìŒ
    - ë‘˜ ë‹¤ í•„ìš”í•˜ë¯€ë¡œ ì ì  ì‘ê²Œ ë§Œë“¤ë©´ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤!

    ë™ì‘:
    â”Œâ”€â”€â”¬â”€â”€â”
    â”‚ 1â”‚ 2â”‚           â”Œâ”€â”€â”€â”€â”
    â”œâ”€â”€â”¼â”€â”€â”¤  í•©ì¹˜ê¸°â†’  â”‚1234â”‚
    â”‚ 3â”‚ 4â”‚           â””â”€â”€â”€â”€â”˜
    â””â”€â”€â”´â”€â”€â”˜

    2x2 í”½ì…€ 4ê°œë¥¼ 1ê°œë¡œ í•©ì¹©ë‹ˆë‹¤

    ë³€í™”:
    - í¬ê¸°: (56x56) â†’ (28x28) = 1/2
    - ì •ë³´ëŸ‰: 96 â†’ 192 = 2ë°°
    - ì´ ë°ì´í„°ëŸ‰ì€ ë¹„ìŠ· (56*56*96 â‰ˆ 28*28*192)

    ë¹„ìœ :
    - êµ¬ê¸€ ë§µì—ì„œ ì¤Œì•„ì›ƒí•˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·
    - ê°€ê¹Œì´ì„œ ë³´ë‹¤ê°€ â†’ ë©€ë¦¬ì„œ ì „ì²´ ë³´ê¸°
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        """
        ì„¤ì •:
            input_resolution: ì…ë ¥ í¬ê¸° (H, W)
            dim: ì±„ë„ ìˆ˜
        """
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        2x2 í”½ì…€ì„ 1ê°œë¡œ í•©ì¹˜ê¸°

        ì˜ˆì‹œ:
        ì…ë ¥: (B, 56*56, 96)  - 56x56 ì´ë¯¸ì§€, ì±„ë„ 96
        ì¶œë ¥: (B, 28*28, 192) - 28x28 ì´ë¯¸ì§€, ì±„ë„ 192
        """
        H, W = self.input_resolution
        B, L, C = x.shape

        x = x.view(B, H, W, C)

        # 2x2 ì˜ì—­ì˜ 4ê°œ í”½ì…€ì„ ê°ê° ì¶”ì¶œ
        x0 = x[:, 0::2, 0::2, :]  # ì¢Œìƒë‹¨
        x1 = x[:, 1::2, 0::2, :]  # ì¢Œí•˜ë‹¨
        x2 = x[:, 0::2, 1::2, :]  # ìš°ìƒë‹¨
        x3 = x[:, 1::2, 1::2, :]  # ìš°í•˜ë‹¨
        x = torch.cat([x0, x1, x2, x3], -1)  # 4ê°œë¥¼ ì´ì–´ë¶™ì„
        x = x.view(B, -1, 4 * C)

        x = self.norm(x)
        x = self.reduction(x)  # 4C â†’ 2Cë¡œ ì••ì¶•

        return x


# ============================================================================
# ğŸª Stage (ì—¬ëŸ¬ ë¸”ë¡ì˜ ë¬¶ìŒ)
# ============================================================================

class BasicLayer(nn.Module):
    """
    ğŸª Stage - ì—¬ëŸ¬ Transformer ë¸”ë¡ì˜ ì§‘í•©

    ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
    - ì—¬ëŸ¬ ê°œì˜ Transformer Blockì„ ì—°ì†ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤
    - W-MSAì™€ SW-MSAë¥¼ ë²ˆê°ˆì•„ê°€ë©° ì‹¤í–‰ (ì§ìˆ˜ë²ˆì§¸, í™€ìˆ˜ë²ˆì§¸)

    êµ¬ì¡°:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Block 0 (ì¼ë°˜ ìœˆë„ìš°)     â”‚
    â”‚ Block 1 (ì´ë™ëœ ìœˆë„ìš°)   â”‚  â† ë²ˆê°ˆì•„ê°€ë©°
    â”‚ Block 2 (ì¼ë°˜ ìœˆë„ìš°)     â”‚
    â”‚ Block 3 (ì´ë™ëœ ìœˆë„ìš°)   â”‚
    â”‚ ...                      â”‚
    â”‚ Patch Merging (ì‘ê²Œ)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ì™œ ë²ˆê°ˆì•„ê°€ë©° í•˜ë‚˜ìš”?
    - ì¼ë°˜: ê° ì˜ì—­ì„ ì§‘ì¤‘í•´ì„œ ë´„
    - ì´ë™: ì˜ì—­ ê°„ ì—°ê²° ê´€ê³„ë„ ë´„
    - ë‘˜ ë‹¤ í•´ì•¼ ì™„ë²½í•œ ë¶„ì„!
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):
        """
        ì„¤ì •:
            dim: ì±„ë„ ìˆ˜
            input_resolution: ì´ë¯¸ì§€ í¬ê¸°
            depth: ì´ Stageì— ë¸”ë¡ì´ ëª‡ ê°œì¸ì§€ (ì˜ˆ: 2ê°œ, 6ê°œ)
            num_heads: ê´€ì  ê°œìˆ˜
            window_size: ìœˆë„ìš° í¬ê¸°
            downsample: ë§ˆì§€ë§‰ì— í¬ê¸° ì¤„ì´ê¸° (PatchMerging)
        """
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth

        # Transformer Block ì—¬ëŸ¬ ê°œ ë§Œë“¤ê¸°
        # ì§ìˆ˜ ë²ˆì§¸: shift_size=0 (ì¼ë°˜)
        # í™€ìˆ˜ ë²ˆì§¸: shift_size=window_size//2 (ì´ë™)
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(
                dim=dim, input_resolution=input_resolution,
                num_heads=num_heads, window_size=window_size,
                shift_size=0 if (i % 2 == 0) else window_size // 2,  # ë²ˆê°ˆì•„ê°€ë©°!
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop, attn_drop=attn_drop,
                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                norm_layer=norm_layer)
            for i in range(depth)])

        # ë§ˆì§€ë§‰ì— ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸°
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        """ëª¨ë“  ë¸”ë¡ ì‹¤í–‰ í›„ í¬ê¸° ì¤„ì´ê¸°"""
        for blk in self.blocks:
            x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x


# ============================================================================
# ğŸ§© ì‚¬ì§„ì„ AIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
# ============================================================================

class PatchEmbed(nn.Module):
    """
    ğŸ§© Patch Embedding - ì‚¬ì§„ì„ AI ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜

    ë¬´ì—‡ì„ í•˜ë‚˜ìš”?
    - ì¼ë°˜ ì‚¬ì§„ (224x224 í”½ì…€) â†’ AIê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜

    ë™ì‘:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  224x224 ì‚¬ì§„        â”‚  ë³´í†µ ì‚¬ì§„
    â”‚  (6ì±„ë„)             â”‚  RGB + ë§ˆìŠ¤í¬
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ 4x4ì”© ë¬¶ê¸°
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  56x56 íŒ¨ì¹˜          â”‚  56*56 = 3,136ê°œ ì¡°ê°
    â”‚  (96ì±„ë„)            â”‚  ê° ì¡°ê°ì´ 4x4 ì˜ì—­ì„ ëŒ€í‘œ
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ì™œ ì´ë ‡ê²Œ í•˜ë‚˜ìš”?
    - í”½ì…€ í•˜ë‚˜í•˜ë‚˜ ë³´ë©´ ë„ˆë¬´ ëŠë¦¼
    - 4x4 ì˜ì—­ì”© ë¬¶ì–´ì„œ ë³´ë©´ ë¹ ë¥´ë©´ì„œë„ ì •ë³´ ì†ì‹¤ ì ìŒ

    ë¹„ìœ :
    - ê¸€ì í•˜ë‚˜ì”© ì½ê¸° vs ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì½ê¸°
    - ë‹¨ì–´ë¡œ ì½ëŠ” ê²Œ ë¹ ë¥´ë©´ì„œë„ ì˜ë¯¸ íŒŒì•… ì˜ ë¨!
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=6, embed_dim=96, norm_layer=None):
        """
        ì„¤ì •:
            img_size: ì…ë ¥ ì‚¬ì§„ í¬ê¸° (224x224)
            patch_size: íŒ¨ì¹˜ í¬ê¸° (4x4)
            in_chans: ì…ë ¥ ì±„ë„ ìˆ˜ (6: RGB 3ê°œ + ë§ˆìŠ¤í¬ 3ê°œ)
            embed_dim: ì¶œë ¥ ì±„ë„ ìˆ˜ (96)
        """
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]

        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]  # 56*56 = 3136
        self.in_chans = in_chans
        self.embed_dim = embed_dim

        # Convolutionìœ¼ë¡œ íŒ¨ì¹˜ ë§Œë“¤ê¸°
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        """
        ì‚¬ì§„ì„ íŒ¨ì¹˜ë¡œ ë³€í™˜

        ë³€í™˜ ê³¼ì •:
        (B, 6, 224, 224)  ì‚¬ì§„ 6ì±„ë„
            â†“ Conv2dë¡œ 4x4ì”© ë¬¶ê¸°
        (B, 96, 56, 56)   56x56 í¬ê¸°, 96ì±„ë„
            â†“ í¼ì¹˜ê¸°
        (B, 3136, 96)     3136ê°œ íŒ¨ì¹˜, ê° 96ì°¨ì›
        """
        B, C, H, W = x.shape

        x = self.proj(x).flatten(2).transpose(1, 2)
        if self.norm is not None:
            x = self.norm(x)
        return x


# ============================================================================
# ğŸ¨ ë©”ì¸ ëª¨ë¸ (ëª¨ë“  ê²ƒì„ í•©ì¹œ ìµœì¢… ëª¨ë¸)
# ============================================================================

class SwinHairClassifier(nn.Module):
    """
    ===================================================================
    ğŸ† íƒˆëª¨ AI ë¶„ì„ ë©”ì¸ ëª¨ë¸ - ì „ì²´ êµ¬ì¡°
    ===================================================================

    ì´ ëª¨ë¸ì´ í•˜ëŠ” ì¼:
    ğŸ“¸ ì •ìˆ˜ë¦¬ + ì¸¡ë©´ ì‚¬ì§„ â†’ ğŸ¤– AI ë¶„ì„ â†’ ğŸ“Š íƒˆëª¨ ë‹¨ê³„ (0~3)

    ì „ì²´ íë¦„ (4ë‹¨ê³„):

    ğŸ“¥ ì…ë ¥: 224x224 ì‚¬ì§„ (RGB 3ì±„ë„ + ë§ˆìŠ¤í¬ 3ì±„ë„ = 6ì±„ë„)
    â†“
    ğŸ§© [Patch Embed] 56x56 íŒ¨ì¹˜ë¡œ ë³€í™˜, ì±„ë„ 96
    â†“
    ğŸª [Stage 1] 56x56, ì±„ë„ 96, ë¸”ë¡ 2ê°œ
         - ì‘ì€ ë””í…Œì¼ ì°¾ê¸° (ë¨¸ë¦¬ì¹´ë½ í…ìŠ¤ì²˜, ëª¨ê³µ ë“±)
    â†“ (í¬ê¸° 1/2ë¡œ)
    ğŸª [Stage 2] 28x28, ì±„ë„ 192, ë¸”ë¡ 2ê°œ
         - ì¤‘ê°„ íŒ¨í„´ ì°¾ê¸° (í—¤ì–´ë¼ì¸ ëª¨ì–‘, ë°€ë„ ë³€í™” ë“±)
    â†“ (í¬ê¸° 1/2ë¡œ)
    ğŸª [Stage 3] 14x14, ì±„ë„ 384, ë¸”ë¡ 6ê°œ â­ ê°€ì¥ ë§ì€ ê³„ì‚°!
         - ì¤‘ìš”í•œ íŠ¹ì§• ì¶”ì¶œ (íƒˆëª¨ íŒ¨í„´ ì „ë°˜)
    â†“ (í¬ê¸° 1/2ë¡œ)
    ğŸª [Stage 4] 7x7, ì±„ë„ 768, ë¸”ë¡ 2ê°œ
         - ì „ì²´ì ì¸ íŠ¹ì§• í†µí•©
    â†“
    ğŸ“Š [ë¶„ë¥˜ê¸°] 768ì°¨ì› â†’ 256ì°¨ì› â†’ 4í´ë˜ìŠ¤
         - ìµœì¢… íŒë‹¨: 0(ì •ìƒ), 1(ê²½ë¯¸), 2(ì¤‘ë“±ë„), 3(ì‹¬ê°)
    â†“
    ğŸ“¤ ì¶œë ¥: íƒˆëª¨ ë‹¨ê³„ + í™•ë¥ 

    ===================================================================

    ë¹„ìœ :
    - Stage 1: ë‚˜ë¬´ ìì‚¬ê·€ ë³´ê¸° (ë””í…Œì¼)
    - Stage 2: ë‚˜ë¬´ ê°€ì§€ ë³´ê¸° (íŒ¨í„´)
    - Stage 3: ë‚˜ë¬´ ì „ì²´ ë³´ê¸° (êµ¬ì¡°)
    - Stage 4: ìˆ² ì „ì²´ ë³´ê¸° (ì „ë°˜ì  ìƒí™©)

    ğŸ“Š ëª¨ë¸ í¬ê¸°:
    - Tiny: 28M íŒŒë¼ë¯¸í„° (ë¹ ë¦„, ê°€ë²¼ì›€)
    - Small: 50M íŒŒë¼ë¯¸í„° (ì •í™•í•¨, ë¬´ê±°ì›€)

    ğŸ¯ ì¶œë ¥ í´ë˜ìŠ¤:
    - Level 0: ì •ìƒ - íƒˆëª¨ ì—†ìŒ
    - Level 1: ê²½ë¯¸ - ì´ˆê¸° ë‹¨ê³„, ëˆˆì— ì˜ ì•ˆë”
    - Level 2: ì¤‘ë“±ë„ - ëˆˆì— ë³´ì´ê¸° ì‹œì‘, ê´€ë¦¬ í•„ìš”
    - Level 3: ì‹¬ê° - ëšœë ·í•˜ê²Œ ë³´ì„, ì ê·¹ì  ì¹˜ë£Œ í•„ìš”
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=6, num_classes=4,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        """
        ëª¨ë¸ ì„¤ì •

        ì£¼ìš” íŒŒë¼ë¯¸í„°:
            img_size: ì…ë ¥ ì‚¬ì§„ í¬ê¸° (224x224)
            in_chans: ì…ë ¥ ì±„ë„ (6 = RGB 3 + ë§ˆìŠ¤í¬ 3)
            num_classes: ì¶œë ¥ í´ë˜ìŠ¤ ê°œìˆ˜ (4 = ë ˆë²¨ 0~3)
            embed_dim: ì‹œì‘ ì±„ë„ ìˆ˜ (96)
            depths: ê° Stageì˜ ë¸”ë¡ ê°œìˆ˜ [2, 2, 6, 2]
            num_heads: ê° Stageì˜ ê´€ì  ê°œìˆ˜ [3, 6, 12, 24]
            window_size: ìœˆë„ìš° í¬ê¸° (7x7)
        """
        super().__init__()

        self.num_classes = num_classes  # 4ê°œ í´ë˜ìŠ¤
        self.num_layers = len(depths)   # 4ê°œ Stage
        self.embed_dim = embed_dim      # 96
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))  # 96*8 = 768
        self.mlp_ratio = mlp_ratio

        # ğŸ§© 1ë‹¨ê³„: ì‚¬ì§„ì„ íŒ¨ì¹˜ë¡œ ë³€í™˜
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans,
            embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # ğŸ“ ìœ„ì¹˜ ì •ë³´ (ì„ íƒì )
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # ğŸ² Stochastic Depth (ê³¼ì í•© ë°©ì§€)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

        # ğŸª 2ë‹¨ê³„: 4ê°œ Stage ìƒì„±
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(
                dim=int(embed_dim * 2 ** i_layer),  # 96 â†’ 192 â†’ 384 â†’ 768
                input_resolution=(
                    patches_resolution[0] // (2 ** i_layer),
                    patches_resolution[1] // (2 ** i_layer)
                ),  # 56 â†’ 28 â†’ 14 â†’ 7
                depth=depths[i_layer],          # 2, 2, 6, 2
                num_heads=num_heads[i_layer],   # 3, 6, 12, 24
                window_size=window_size,
                mlp_ratio=self.mlp_ratio,
                qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate,
                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                norm_layer=norm_layer,
                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)  # ì „ì²´ í‰ê· 

        # ğŸ¯ 3ë‹¨ê³„: ë¶„ë¥˜ê¸° (ìµœì¢… íŒë‹¨)
        self.head = nn.Sequential(
            nn.Linear(self.num_features, 256),  # 768 â†’ 256
            nn.ReLU(inplace=True),              # í™œì„±í™”
            nn.Dropout(0.3),                    # ê³¼ì í•© ë°©ì§€
            nn.Linear(256, num_classes)         # 256 â†’ 4 (ìµœì¢… í´ë˜ìŠ¤)
        )

        self.apply(self._init_weights)

    def _init_weights(self, m):
        """
        ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        - í•™ìŠµ ì‹œì‘ ì „ ì´ˆê¸°ê°’ ì„¤ì •
        """
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        """í•™ìŠµ ì‹œ decay ì ìš©í•˜ì§€ ì•Šì„ íŒŒë¼ë¯¸í„°"""
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        """í•™ìŠµ ì‹œ decay ì ìš©í•˜ì§€ ì•Šì„ í‚¤ì›Œë“œ"""
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        """
        ğŸ” íŠ¹ì§• ì¶”ì¶œ (1~2ë‹¨ê³„)

        ê³¼ì •:
        1. ì‚¬ì§„ â†’ íŒ¨ì¹˜ë¡œ ë³€í™˜
        2. Stage 1~4 í†µê³¼í•˜ë©° íŠ¹ì§• ì¶”ì¶œ
        3. ì „ì²´ í‰ê· ìœ¼ë¡œ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ

        ì…ë ¥: (B, 6, 224, 224) - 6ì±„ë„ ì‚¬ì§„
        ì¶œë ¥: (B, 768) - 768ì°¨ì› íŠ¹ì§• ë²¡í„°
        """
        x = self.patch_embed(x)  # (B, 3136, 96)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        # 4ê°œ Stage í†µê³¼
        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)                         # ì •ê·œí™”
        x = self.avgpool(x.transpose(1, 2))      # í‰ê·  (7x7 â†’ 1ê°œ ê°’)
        x = torch.flatten(x, 1)                  # í¼ì¹˜ê¸°
        return x

    def forward(self, x):
        """
        ğŸš€ ì „ì²´ ì‹¤í–‰ (1~3ë‹¨ê³„ ëª¨ë‘)

        íë¦„:
        ì…ë ¥ ì‚¬ì§„ â†’ íŠ¹ì§• ì¶”ì¶œ â†’ ë¶„ë¥˜ â†’ íƒˆëª¨ ë‹¨ê³„ ì¶œë ¥

        ì…ë ¥: (B, 6, 224, 224)
              - B: ë°°ì¹˜ í¬ê¸° (í•œë²ˆì— ëª‡ ì¥ ì²˜ë¦¬)
              - 6: RGB 3ì±„ë„ + ë§ˆìŠ¤í¬ 3ì±„ë„
              - 224x224: ì‚¬ì§„ í¬ê¸°

        ì¶œë ¥: (B, 4)
              - 4ê°œ í´ë˜ìŠ¤ì˜ ì ìˆ˜ (ë¡œì§“)
              - Softmax ê±°ì¹˜ë©´ í™•ë¥ ë¡œ ë³€í™˜ ê°€ëŠ¥

        ì˜ˆì‹œ:
        ì…ë ¥: ì •ìˆ˜ë¦¬ ì‚¬ì§„ 1ì¥
        ì¶œë ¥: [0.1, 0.2, 0.5, 0.2]
               â†‘    â†‘    â†‘    â†‘
              ì •ìƒ  ê²½ë¯¸ ì¤‘ë“±ë„ ì‹¬ê°
        â†’ ì¤‘ë“±ë„(50% í™•ë¥ )ë¡œ íŒë‹¨!
        """
        x = self.forward_features(x)  # íŠ¹ì§• ì¶”ì¶œ (768ì°¨ì›)
        x = self.head(x)               # ë¶„ë¥˜ (4í´ë˜ìŠ¤)
        return x


# ============================================================================
# ğŸ­ ëª¨ë¸ ìƒì„± í•¨ìˆ˜ (ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ)
# ============================================================================

def swin_tiny_patch4_window7_224_hair(num_classes=4, **kwargs):
    """
    ğŸ”¹ Swin-Tiny ëª¨ë¸ (ê°€ë²¼ìš´ ë²„ì „)

    ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?
    - ë¹ ë¥¸ ë¶„ì„ì´ í•„ìš”í•  ë•Œ
    - ì„œë²„ ì„±ëŠ¥ì´ ë‚®ì„ ë•Œ
    - ëª¨ë°”ì¼ ì•±ì— ë„£ì„ ë•Œ

    ìŠ¤í™:
    - íŒŒë¼ë¯¸í„°: 28Mê°œ (2800ë§Œ ê°œ)
    - ë¸”ë¡ êµ¬ì„±: [2, 2, 6, 2]
    - ì†ë„: âš¡âš¡âš¡ ë§¤ìš° ë¹ ë¦„
    - ì •í™•ë„: â­â­â­ ì¢‹ìŒ
    - ë©”ëª¨ë¦¬: ì ê²Œ ì‚¬ìš©

    ì¶”ë¡  ì‹œê°„:
    - GPU: ~0.5ì´ˆ
    - CPU: ~3ì´ˆ
    """
    model = SwinHairClassifier(
        patch_size=4,
        window_size=7,
        embed_dim=96,
        depths=[2, 2, 6, 2],        # Tiny ë²„ì „ ë¸”ë¡ ê°œìˆ˜
        num_heads=[3, 6, 12, 24],
        num_classes=num_classes,
        **kwargs
    )
    return model


def swin_small_patch4_window7_224_hair(num_classes=4, **kwargs):
    """
    ğŸ”¸ Swin-Small ëª¨ë¸ (ê°•ë ¥í•œ ë²„ì „)

    ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?
    - ìµœê³ ì˜ ì •í™•ë„ê°€ í•„ìš”í•  ë•Œ
    - ì„œë²„ ì„±ëŠ¥ì´ ì¢‹ì„ ë•Œ
    - ì˜ë£Œ ì§„ë‹¨ ë“± ì •ë°€ë„ê°€ ì¤‘ìš”í•  ë•Œ

    ìŠ¤í™:
    - íŒŒë¼ë¯¸í„°: 50Mê°œ (5000ë§Œ ê°œ)
    - ë¸”ë¡ êµ¬ì„±: [2, 2, 18, 2] â† Stage 3ì´ 18ê°œ!
    - ì†ë„: âš¡âš¡ ë³´í†µ
    - ì •í™•ë„: â­â­â­â­ ë§¤ìš° ì¢‹ìŒ
    - ë©”ëª¨ë¦¬: ë§ì´ ì‚¬ìš©

    ì¶”ë¡  ì‹œê°„:
    - GPU: ~1.5ì´ˆ
    - CPU: ~8ì´ˆ

    Tiny vs Small:
    - Tiny: ë¹ ë¥´ì§€ë§Œ ì•½ê°„ ëœ ì •í™•
    - Small: ëŠë¦¬ì§€ë§Œ ë” ì •í™•
    """
    model = SwinHairClassifier(
        patch_size=4,
        window_size=7,
        embed_dim=96,
        depths=[2, 2, 18, 2],       # Small ë²„ì „ (Stage 3ì´ 18ë¸”ë¡)
        num_heads=[3, 6, 12, 24],
        num_classes=num_classes,
        **kwargs
    )
    return model


# ============================================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ (ëª¨ë¸ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸)
# ============================================================================

if __name__ == "__main__":
    """
    ëª¨ë¸ í…ŒìŠ¤íŠ¸

    í™•ì¸ ì‚¬í•­:
    1. ëª¨ë¸ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ëŠ”ì§€
    2. ì…ë ¥ì´ ì œëŒ€ë¡œ ë“¤ì–´ê°€ëŠ”ì§€
    3. ì¶œë ¥ì´ ì œëŒ€ë¡œ ë‚˜ì˜¤ëŠ”ì§€
    4. ì¶œë ¥ì„ í™•ë¥ ë¡œ ë³€í™˜í•˜ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€
    """
    print("=" * 60)
    print("ğŸ§ª íƒˆëª¨ AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    # 1ï¸âƒ£ ëª¨ë¸ ìƒì„± (Tiny ë²„ì „)
    print("\n1ï¸âƒ£ ëª¨ë¸ ìƒì„± ì¤‘...")
    model = swin_tiny_patch4_window7_224_hair(num_classes=4)
    print("   âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!")

    # 2ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
    print("\n2ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± ì¤‘...")
    batch_size = 2  # 2ì¥ì˜ ì‚¬ì§„
    rgb_image = torch.randn(batch_size, 3, 224, 224)    # RGB ì´ë¯¸ì§€
    mask_image = torch.randn(batch_size, 3, 224, 224)   # í—¤ì–´ ë§ˆìŠ¤í¬
    dual_input = torch.cat([rgb_image, mask_image], dim=1)  # 6ì±„ë„ë¡œ í•©ì¹˜ê¸°
    print(f"   âœ… ì…ë ¥ ìƒì„± ì™„ë£Œ: {dual_input.shape}")

    # 3ï¸âƒ£ ëª¨ë¸ ì •ë³´ ì¶œë ¥
    print("\n3ï¸âƒ£ ëª¨ë¸ ì •ë³´:")
    param_count = sum(p.numel() for p in model.parameters())
    print(f"   - íŒŒë¼ë¯¸í„° ê°œìˆ˜: {param_count:,}ê°œ")
    print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~{param_count * 4 / 1024 / 1024:.1f} MB")

    # 4ï¸âƒ£ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    print("\n4ï¸âƒ£ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸ ì¤‘...")
    with torch.no_grad():  # í•™ìŠµ ì•„ë‹˜, ì¶”ë¡ ë§Œ
        output = model(dual_input)
        print(f"   âœ… ì¶œë ¥ ìƒì„± ì™„ë£Œ: {output.shape}")
        print(f"\n   ğŸ“Š ì¶œë ¥ ê°’ (ë¡œì§“):")
        print(f"      ìƒ˜í”Œ 1: {output[0].numpy()}")

        # 5ï¸âƒ£ í™•ë¥ ë¡œ ë³€í™˜
        probs = torch.softmax(output, dim=1)
        print(f"\n5ï¸âƒ£ í™•ë¥ ë¡œ ë³€í™˜ (Softmax):")
        print(f"   ìƒ˜í”Œ 1ì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ :")
        level_names = ["ì •ìƒ (Level 0)", "ê²½ë¯¸ (Level 1)",
                      "ì¤‘ë“±ë„ (Level 2)", "ì‹¬ê° (Level 3)"]
        for i, (prob, name) in enumerate(zip(probs[0], level_names)):
            bar = "â–ˆ" * int(prob.item() * 50)  # ë§‰ëŒ€ ê·¸ë˜í”„
            print(f"      {name}: {prob.item():.1%} {bar}")

        # 6ï¸âƒ£ ìµœì¢… ì˜ˆì¸¡
        predicted = torch.argmax(probs[0])
        print(f"\n6ï¸âƒ£ ìµœì¢… ì˜ˆì¸¡: {level_names[predicted]}")
        print(f"   ì‹ ë¢°ë„: {probs[0][predicted].item():.1%}")

    print("\n" + "=" * 60)
    print("âœ¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
    print("=" * 60)

    # ğŸ’¡ ì‚¬ìš© íŒ
    print("\nğŸ’¡ ì‚¬ìš© íŒ:")
    print("   1. ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” RGB + í—¤ì–´ ë§ˆìŠ¤í¬ë¥¼ 6ì±„ë„ë¡œ í•©ì³ì„œ ì…ë ¥")
    print("   2. ì¶œë ¥ì€ torch.softmax()ë¡œ í™•ë¥ ë¡œ ë³€í™˜")
    print("   3. torch.argmax()ë¡œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ")
    print("   4. GPU ì‚¬ìš© ì‹œ: model.cuda(), input.cuda()")
    print("   5. ì¶”ë¡  ì‹œ: torch.no_grad() ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)")
