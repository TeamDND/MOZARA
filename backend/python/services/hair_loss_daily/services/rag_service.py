"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤ - CLIP ì•™ìƒë¸” ê¸°ë°˜
"""
from typing import List, Dict, Optional, Any
from .clip_ensemble_service import clip_ensemble_service
from .pinecone_service import get_pinecone_service
from .image_preprocessing_service import image_preprocessing_service
from ..utils.image_path_utils import get_image_path_from_metadata, get_available_image_paths
import statistics
from collections import Counter
import numpy as np

class RAGService:
    """RAG ê¸°ë°˜ ë¨¸ë¦¬ì‚¬ì§„ ë¶„ì„ ì„œë¹„ìŠ¤ - CLIP ì•™ìƒë¸” ê¸°ë°˜"""
    
    def __init__(self):
        self.clip_service = clip_ensemble_service
        self._pinecone_service = None
    
    @property
    def pinecone_service(self):
        """Pinecone ì„œë¹„ìŠ¤ ì§€ì—° ë¡œë”©"""
        if self._pinecone_service is None:
            self._pinecone_service = get_pinecone_service()
        return self._pinecone_service
    
    def analyze_hair_image(self, image_bytes: bytes, top_k: int = 10, use_preprocessing: bool = True) -> Dict[str, Any]:
        """ë¨¸ë¦¬ì‚¬ì§„ ë¶„ì„ (CLIP ì•™ìƒë¸” RAG ë°©ì‹)"""
        try:
            # 1. ì‚¬ìš©ì ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì„ íƒì )
            if use_preprocessing:
                print("ğŸ–¼ï¸ ì‚¬ìš©ì ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘...")
                processed_image_bytes = image_preprocessing_service.preprocess_for_medical_analysis(image_bytes)
                print("[OK] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ (ë¹› ë°˜ì‚¬ ì²˜ë¦¬ í¬í•¨)")
            else:
                print("ğŸ–¼ï¸ ì „ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©...")
                processed_image_bytes = image_bytes
            
            # 2. CLIP ì•™ìƒë¸”ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
            print("ğŸ” CLIP ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
            
            # CLIP ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ
            hybrid_features = self.clip_service.extract_hybrid_features(processed_image_bytes)
            query_vector = hybrid_features["combined"]
            print(f"[OK] CLIP ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(query_vector)}ì°¨ì›")
            
            if query_vector is None or len(query_vector) == 0:
                return {
                    "success": False,
                    "error": "ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                }
            
            # 2. Pineconeì—ì„œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ê²€ìƒ‰ (ë¹„ë“¬/íƒˆëª¨ ì œì™¸)
            print("ğŸ” ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ ì¤‘ (ë¹„ë“¬/íƒˆëª¨ ì œì™¸)...")
            # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            query_vector_list = query_vector.tolist() if hasattr(query_vector, 'tolist') else query_vector
            
            # ë¹„ë“¬ê³¼ íƒˆëª¨ë¥¼ ì œì™¸í•˜ëŠ” í•„í„°
            exclude_filter = {
                "category": {"$nin": ["5.ë¹„ë“¬", "ë¹„ë“¬", "íƒˆëª¨"]}
            }
            
            similar_cases = self.pinecone_service.search_similar_vectors(
                query_vector_list, top_k=top_k, filter_dict=exclude_filter
            )
            
            if similar_cases is None or len(similar_cases) == 0:
                return {
                    "success": False,
                    "error": "ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # 3. ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
            print("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘...")
            analysis_result = self._analyze_search_results(similar_cases)
            
            # 4. ìœ ì‚¬ ì¼€ì´ìŠ¤ì— ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ ì¶”ê°€ (íƒˆëª¨ ì œì™¸)
            enhanced_similar_cases = []
            for case in similar_cases[:10]:  # ìƒìœ„ 10ê°œì—ì„œ í•„í„°ë§
                enhanced_case = case.copy()
                metadata = case.get("metadata", {})
                
                # íƒˆëª¨ ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš° ì œì™¸
                category = metadata.get("category", "")
                if category == "íƒˆëª¨":
                    continue
                
                image_path = get_image_path_from_metadata(metadata)
                if image_path:
                    enhanced_case["image_path"] = image_path
                enhanced_similar_cases.append(enhanced_case)
                
                # íƒˆëª¨ ì œì™¸ í›„ 5ê°œë§Œ ë°˜í™˜
                if len(enhanced_similar_cases) >= 5:
                    break
            
            # 5. ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "analysis": analysis_result,
                "similar_cases": enhanced_similar_cases,
                "total_similar_cases": len(similar_cases),
                "model_info": self.clip_service.get_model_info(),
                "preprocessing_used": use_preprocessing,
                "preprocessing_info": {
                    "enabled": use_preprocessing,
                    "description": "ë¹› ë°˜ì‚¬ ì²˜ë¦¬ ê°•í™” (ë¹„ë“¬ ì˜¤ì¸ ë°©ì§€)" if use_preprocessing else "ì „ì²˜ë¦¬ ì—†ìŒ"
                }
            }
            
            return result
            
        except Exception as e:
            import traceback
            print(f"[ERROR] RAG ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            print(f"[ERROR] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def test_similarity_consistency(self, image_bytes: bytes, test_rounds: int = 3) -> Dict[str, Any]:
        """ìœ ì‚¬ë„ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ - ê°™ì€ ì´ë¯¸ì§€ë¡œ ì—¬ëŸ¬ ë²ˆ ê²€ìƒ‰í•˜ì—¬ ì¼ê´€ì„± í™•ì¸"""
        try:
            print(f"ğŸ§ª ìœ ì‚¬ë„ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì´ {test_rounds}íšŒ)")
            
            results = []
            for i in range(test_rounds):
                print(f"ğŸ”„ í…ŒìŠ¤íŠ¸ {i+1}/{test_rounds} ì‹¤í–‰ ì¤‘...")
                
                # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¡œ íŠ¹ì§• ì¶”ì¶œ
                preprocessed_image_bytes = image_preprocessing_service.preprocess_for_medical_analysis(image_bytes)
                hybrid_features = self.clip_service.extract_hybrid_features(preprocessed_image_bytes)
                query_vector = hybrid_features["combined"]
                
                # ê²€ìƒ‰ ì‹¤í–‰
                query_vector_list = query_vector.tolist() if hasattr(query_vector, 'tolist') else query_vector
                similar_cases = self.pinecone_service.search_similar_vectors(query_vector_list, top_k=5)
                
                # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì €ì¥
                top_results = []
                for case in similar_cases[:3]:
                    top_results.append({
                        "id": case.get("id", ""),
                        "score": case.get("score", 0),
                        "category": case.get("metadata", {}).get("category", "")
                    })
                
                results.append(top_results)
            
            # ì¼ê´€ì„± ë¶„ì„
            consistency_analysis = self._analyze_consistency(results)
            
            return {
                "success": True,
                "test_rounds": test_rounds,
                "results": results,
                "consistency_analysis": consistency_analysis
            }
            
        except Exception as e:
            import traceback
            print(f"[ERROR] ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
            print(f"[ERROR] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def _analyze_consistency(self, results: List[List[Dict]]) -> Dict[str, Any]:
        """ì¼ê´€ì„± ë¶„ì„"""
        if not results or len(results) < 2:
            return {"error": "ë¶„ì„í•  ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."}
        
        # ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
        baseline = results[0]
        consistency_scores = []
        
        for i in range(1, len(results)):
            current = results[i]
            matches = 0
            
            # ê° ê²°ê³¼ì—ì„œ ê°™ì€ IDê°€ ìˆëŠ”ì§€ í™•ì¸
            for baseline_item in baseline:
                for current_item in current:
                    if baseline_item["id"] == current_item["id"]:
                        matches += 1
                        break
            
            consistency_score = matches / len(baseline) if baseline else 0
            consistency_scores.append(consistency_score)
        
        # ì ìˆ˜ ì¼ê´€ì„± ë¶„ì„
        score_variations = []
        for i in range(len(baseline)):
            scores = [result[i]["score"] for result in results if i < len(result)]
            if len(scores) > 1:
                score_variation = max(scores) - min(scores)
                score_variations.append(score_variation)
        
        return {
            "average_consistency": sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0,
            "consistency_scores": consistency_scores,
            "average_score_variation": sum(score_variations) / len(score_variations) if score_variations else 0,
            "score_variations": score_variations,
            "is_consistent": all(score > 0.8 for score in consistency_scores) if consistency_scores else False
        }
    
    def test_weighted_ensemble(self, image_bytes: bytes, model_weights: Dict[str, float] = None, top_k: int = 10) -> Dict[str, Any]:
        """ê°€ì¤‘ì¹˜ ì¡°ì •ëœ ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ (Pinecone ë°ì´í„° ì¬ì—…ë¡œë“œ ì—†ì´)"""
        try:
            print(f"ğŸ” ê°€ì¤‘ì¹˜ ì¡°ì • ì•™ìƒë¸” í…ŒìŠ¤íŠ¸")
            
            # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¡œ íŠ¹ì§• ì¶”ì¶œ
            preprocessed_image_bytes = image_preprocessing_service.preprocess_for_medical_analysis(image_bytes)
            
            # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
            if model_weights is None:
                model_weights = {
                    "ViT-B-32": 0.6,  # ê¸°ë³¸ ëª¨ë¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
                    "ViT-B-16": 0.2,  # ê³ í•´ìƒë„ ëª¨ë¸ ê°€ì¤‘ì¹˜ ê°ì†Œ
                    "RN50": 0.2       # ResNet ëª¨ë¸ ê°€ì¤‘ì¹˜ ê°ì†Œ
                }
            
            # ê°€ì¤‘ì¹˜ ì¡°ì •ëœ ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ
            weighted_features = self.clip_service.extract_weighted_ensemble_features(
                preprocessed_image_bytes, model_weights
            )
            
            # ê²€ìƒ‰ ì‹¤í–‰
            query_vector_list = weighted_features.tolist() if hasattr(weighted_features, 'tolist') else weighted_features
            similar_cases = self.pinecone_service.search_similar_vectors(query_vector_list, top_k=top_k)
            
            if similar_cases is None or len(similar_cases) == 0:
                return {
                    "success": False,
                    "error": "ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "model_weights": model_weights,
                "similar_cases": similar_cases,
                "total_similar_cases": len(similar_cases),
                "preprocessing_used": True
            }
            
            return result
            
        except Exception as e:
            import traceback
            print(f"[ERROR] ê°€ì¤‘ì¹˜ ì¡°ì • ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
            print(f"[ERROR] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def test_without_preprocessing(self, image_bytes: bytes, top_k: int = 10) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ì—†ì´ ì§ì ‘ ë¶„ì„ (ë””ë²„ê¹…ìš©)"""
        try:
            print("ğŸ” ì „ì²˜ë¦¬ ì—†ì´ ì§ì ‘ ë¶„ì„ ì¤‘...")
            
            # CLIP ì•™ìƒë¸”ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©)
            hybrid_features = self.clip_service.extract_hybrid_features(image_bytes)
            query_vector = hybrid_features["combined"]
            print(f"[OK] CLIP ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(query_vector)}ì°¨ì›")
            
            if query_vector is None or len(query_vector) == 0:
                return {
                    "success": False,
                    "error": "ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                }
            
            # Pineconeì—ì„œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ê²€ìƒ‰
            query_vector_list = query_vector.tolist() if hasattr(query_vector, 'tolist') else query_vector
            similar_cases = self.pinecone_service.search_similar_vectors(
                query_vector_list, top_k=top_k
            )
            
            if similar_cases is None or len(similar_cases) == 0:
                return {
                    "success": False,
                    "error": "ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "similar_cases": similar_cases,
                "total_similar_cases": len(similar_cases),
                "preprocessing_used": False
            }
            
            return result
            
        except Exception as e:
            import traceback
            print(f"[ERROR] ì „ì²˜ë¦¬ ì—†ì´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            print(f"[ERROR] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {
                "success": False,
                "error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def _analyze_search_results(self, similar_cases: List[Dict]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì§„ë‹¨ ì •ë³´ ì¶”ì¶œ (ê°€ì¤‘ì¹˜ ì ìš©)"""
        try:
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
            weighted_categories = {}
            weighted_severities = {}
            values = {
                "value_1": [],  # ë¯¸ì„¸ê°ì§ˆ
                "value_2": [],  # í”¼ì§€ê³¼ë‹¤
                "value_3": [],  # ëª¨ë‚­ì‚¬ì´í™ë°˜
                "value_4": [],  # ëª¨ë‚­í™ë°˜ë†í¬
                "value_5": []   # ë¹„ë“¬ (ë°ì´í„°ëŠ” ìˆ˜ì§‘í•˜ì§€ë§Œ ê²°ê³¼ì—ì„œ ì œì™¸)
                # "value_6": []   # íƒˆëª¨ - ì œì™¸
            }
            scores = []
            
            for case in similar_cases:
                metadata = case.get("metadata", {})
                score = case.get("score", 0)
                
                # íƒˆëª¨ ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš° ë¶„ì„ì—ì„œ ì œì™¸
                category = metadata.get("category", "")
                if category == "íƒˆëª¨":
                    continue
                
                scores.append(score)
                print(f"[DEBUG] ì¼€ì´ìŠ¤: {category}, ì‹¬ê°ë„: {metadata.get('severity', '')}, ìœ ì‚¬ë„: {score:.3f}")
                
                # ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
                severity = metadata.get("severity", "")
                
                if category:
                    weighted_categories[category] = weighted_categories.get(category, 0) + score
                if severity:
                    weighted_severities[severity] = weighted_severities.get(severity, 0) + score
                
                # ê° ì¹´í…Œê³ ë¦¬ë³„ ê°’ ìˆ˜ì§‘ (ê°€ì¤‘ì¹˜ ì ìš©)
                for key in values.keys():
                    val = metadata.get(key, "0")
                    try:
                        weighted_val = int(val) * score  # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ê°€ì¤‘ì¹˜ ì ìš©
                        values[key].append(weighted_val)
                    except ValueError:
                        values[key].append(0)
            
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì£¼ìš” ì¹´í…Œê³ ë¦¬/ì‹¬ê°ë„ ê²°ì • (ì„ê³„ê°’ ì ìš©)
            primary_category = self._get_primary_category_with_threshold(weighted_categories, scores)
            primary_severity = self._get_primary_severity_with_threshold(weighted_severities, scores)
            
            # ì§„ë‹¨ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
            diagnosis_scores = self._calculate_weighted_diagnosis_scores(values, scores)
            print(f"[DEBUG] ê³„ì‚°ëœ diagnosis_scores: {diagnosis_scores}")
            
            # ì¤‘ë³µ ë°ì´í„° ì œê±° (ê°™ì€ ì´ë¯¸ì§€ IDì— ëŒ€í•´ ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë§Œ ìœ ì§€)
            unique_cases = {}
            for case in similar_cases:
                image_id = case.get("metadata", {}).get("image_id", "")
                if image_id not in unique_cases or case.get("score", 0) > unique_cases[image_id].get("score", 0):
                    unique_cases[image_id] = case
            
            # ì¤‘ë³µ ì œê±°ëœ ì¼€ì´ìŠ¤ë¡œ ë‹¤ì‹œ ë¶„ì„
            filtered_cases = list(unique_cases.values())
            
            # í†µê³„ ë¶„ì„ (ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ì‚¬ìš©)
            analysis = {
                "primary_category": primary_category,
                "primary_severity": primary_severity,
                "average_confidence": statistics.mean(scores) if scores else 0,
                "category_distribution": dict(Counter([case.get("metadata", {}).get("category", "") for case in filtered_cases])),
                "severity_distribution": dict(Counter([case.get("metadata", {}).get("severity", "") for case in filtered_cases])),
                "diagnosis_scores": diagnosis_scores,
                "recommendations": self._generate_recommendations(values, [primary_category]),
                "weighted_analysis": {
                    "weighted_categories": weighted_categories,
                    "weighted_severities": weighted_severities
                }
            }
            
            # scalp_score ê³„ì‚° ë° ì¶”ê°€
            try:
                scalp_score = self._calculate_scalp_score(
                    primary_category, 
                    primary_severity, 
                    diagnosis_scores, 
                    statistics.mean(scores) if scores else 0
                )
                analysis["scalp_score"] = scalp_score
                print(f"[DEBUG] analysisì— scalp_score ì¶”ê°€ ì™„ë£Œ: {scalp_score}")
                print(f"[DEBUG] analysis í‚¤ ëª©ë¡: {list(analysis.keys())}")
            except Exception as e:
                print(f"[ERROR] scalp_score ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
                import traceback
                traceback.print_exc()
                analysis["scalp_score"] = 100  # ê¸°ë³¸ê°’
            
            return analysis
            
        except Exception as e:
            import traceback
            print(f"[WARN] ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            print(f"[WARN] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {"error": str(e)}
    
    def _get_most_common(self, items: List[str]) -> str:
        """ê°€ì¥ ë¹ˆë²ˆí•œ í•­ëª© ë°˜í™˜"""
        if not items:
            return ""
        
        counter = Counter(items)
        return counter.most_common(1)[0][0]
    
    def _calculate_scalp_score(self, primary_category: str, primary_severity: str, 
                               diagnosis_scores: Dict[str, float], avg_confidence: float) -> int:
        """ë‘í”¼ ì ìˆ˜ ê³„ì‚° (0-100ì )"""
        print(f"\n[DEBUG] ë‘í”¼ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
        print(f"[DEBUG] primary_category: {primary_category}")
        print(f"[DEBUG] primary_severity: {primary_severity}")
        print(f"[DEBUG] diagnosis_scores: {diagnosis_scores}")
        print(f"[DEBUG] avg_confidence: {avg_confidence}")
        
        # primary_severity íŒŒì‹± ë””ë²„ê¹…
        print(f"[DEBUG] primary_severity ì›ë³¸: '{primary_severity}'")
        print(f"[DEBUG] '.' í¬í•¨ ì—¬ë¶€: {'.' in primary_severity if primary_severity else False}")
        
        base_score = 100
        
        # ì‹¬ê°ë„ ì¶”ì¶œ (0.ì–‘í˜¸=0, 1.ê²½ì¦=1, 2.ì¤‘ë“±ë„=2, 3.ì¤‘ì¦=3)
        severity_level = 0
        if primary_severity:
            severity_level = int(primary_severity.split('.')[0]) if '.' in primary_severity else 0
        
        print(f"[DEBUG] severity_level: {severity_level}")
        
        # ì‹¬ê°ë„ì— ë”°ë¥¸ ê°ì  (ì¡°ì •)
        severity_penalty = severity_level * 15  # 25 â†’ 15ë¡œ ê°ì†Œ
        base_score -= severity_penalty
        print(f"[DEBUG] ì‹¬ê°ë„ ê°ì : -{severity_penalty}, í˜„ì¬ ì ìˆ˜: {base_score}")
        
        # ì§„ë‹¨ ì ìˆ˜ ê¸°ë°˜ ê°ì  (0~3 ë²”ìœ„)
        if diagnosis_scores:
            scores = list(diagnosis_scores.values())
            avg_diagnosis_score = sum(scores) / len(scores)
            diagnosis_penalty = avg_diagnosis_score * 8  # 15 â†’ 8ë¡œ ê°ì†Œ
            base_score -= diagnosis_penalty
            print(f"[DEBUG] í‰ê·  ì§„ë‹¨ ì ìˆ˜: {avg_diagnosis_score:.2f}")
            print(f"[DEBUG] ì§„ë‹¨ ì ìˆ˜ ê°ì : -{diagnosis_penalty:.2f}, í˜„ì¬ ì ìˆ˜: {base_score:.2f}")
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ì • (ë‚®ì€ ì‹ ë¢°ë„ë©´ ëœ ê°ì )
        confidence_adjustment = (avg_confidence - 0.5) * 10
        base_score += confidence_adjustment
        print(f"[DEBUG] ì‹ ë¢°ë„ ë³´ì •: {confidence_adjustment:+.2f}, í˜„ì¬ ì ìˆ˜: {base_score:.2f}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ê°€ ê°ì 
        category_penalty = 0
        category_lower = primary_category.lower()
        
        # í™ë°˜/ë†í¬ ê°ì  (RGB ê¸°ë°˜ ì¡°ì •)
        if 'í™ë°˜' in category_lower or 'ë†í¬' in category_lower:
            # RGB ê¸°ë°˜ í™ë°˜ ì‹¬ê°ë„ë³„ ê°ì 
            erythema_penalty = self._calculate_erythema_penalty(diagnosis_scores)
            category_penalty = erythema_penalty
        elif 'í”¼ì§€ê³¼ë‹¤' in category_lower:
            category_penalty = 8
        elif 'ë¯¸ì„¸ê°ì§ˆ' in category_lower:
            category_penalty = 5
        
        base_score -= category_penalty
        print(f"[DEBUG] ì¹´í…Œê³ ë¦¬ ê°ì : -{category_penalty}, í˜„ì¬ ì ìˆ˜: {base_score:.2f}")
        
        # ìµœì¢… ì ìˆ˜ëŠ” 0~100 ë²”ìœ„ë¡œ ì œí•œ
        final_score = max(0, min(100, round(base_score)))
        
        print(f"[DEBUG] ìµœì¢… ë‘í”¼ ì ìˆ˜: {final_score}\n")
        
        return final_score
    
    def _calculate_erythema_penalty(self, diagnosis_scores: Dict[str, float]) -> int:
        """RGB ê¸°ë°˜ í™ë°˜ ì‹¬ê°ë„ë³„ ê°ì  ê³„ì‚°"""
        # RGB í™ë°˜ ê¸°ì¤€ê°’ (ì¤‘ì¦, ì¤‘ë“±ë„, ê²½ì¦)
        erythema_rgb_standards = {
            "severe": {"r": 177, "g": 114, "b": 125, "penalty": 15},      # ì¤‘ì¦: ë†’ì€ ê°ì 
            "moderate": {"r": 196, "g": 135, "b": 143, "penalty": 10},    # ì¤‘ë“±ë„: ì¤‘ê°„ ê°ì   
            "mild": {"r": 173, "g": 152, "b": 125, "penalty": 5}          # ê²½ì¦: ë‚®ì€ ê°ì 
        }
        
        # ëª¨ë‚­ì‚¬ì´í™ë°˜ ì ìˆ˜ í™•ì¸
        erythema_score = diagnosis_scores.get("ëª¨ë‚­ì‚¬ì´í™ë°˜", 0)
        
        # ì ìˆ˜ ê¸°ë°˜ ì‹¬ê°ë„ íŒì •
        if erythema_score >= 2.0:
            # ì¤‘ì¦: 2.0 ì´ìƒ
            penalty = erythema_rgb_standards["severe"]["penalty"]
            severity = "ì¤‘ì¦"
        elif erythema_score >= 1.0:
            # ì¤‘ë“±ë„: 1.0-1.9
            penalty = erythema_rgb_standards["moderate"]["penalty"]
            severity = "ì¤‘ë“±ë„"
        elif erythema_score >= 0.5:
            # ê²½ì¦: 0.5-0.9
            penalty = erythema_rgb_standards["mild"]["penalty"]
            severity = "ê²½ì¦"
        else:
            # ì–‘í˜¸: 0.5 ë¯¸ë§Œ
            penalty = 0
            severity = "ì–‘í˜¸"
        
        print(f"[DEBUG] í™ë°˜ RGB ë¶„ì„: ì ìˆ˜={erythema_score:.2f}, ì‹¬ê°ë„={severity}, ê°ì ={penalty}")
        
        return penalty
    
    def _calculate_diagnosis_scores(self, values: Dict[str, List[int]]) -> Dict[str, float]:
        """ê° ì§„ë‹¨ ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°"""
        scores = {}
        
        category_names = {
            "value_1": "ë¯¸ì„¸ê°ì§ˆ",
            "value_2": "í”¼ì§€ê³¼ë‹¤", 
            "value_3": "ëª¨ë‚­ì‚¬ì´í™ë°˜",
            "value_4": "ëª¨ë‚­í™ë°˜ë†í¬",
            "value_5": "ë¹„ë“¬"  # ë°ì´í„°ëŠ” ìˆ˜ì§‘í•˜ì§€ë§Œ ê²°ê³¼ì—ì„œ ì œì™¸
            # "value_6": "íƒˆëª¨" - ì œì™¸
        }
        
        for key, vals in values.items():
            if vals:
                avg_score = statistics.mean(vals)
                category_name = category_names.get(key, key)
                # ë¹„ë“¬ì€ ê²°ê³¼ì—ì„œ ì œì™¸ (ë¹›ë°˜ì‚¬ ì˜¤ì¸ ë¬¸ì œ)
                if category_name != "ë¹„ë“¬":
                    scores[category_name] = round(avg_score, 2)
        
        return scores
    
    def _calculate_weighted_diagnosis_scores(self, values: Dict[str, List[float]], scores: List[float]) -> Dict[str, float]:
        """ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì§„ë‹¨ ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ê°•í™”)"""
        weighted_scores = {}
        
        category_names = {
            "value_1": "ë¯¸ì„¸ê°ì§ˆ",
            "value_2": "í”¼ì§€ê³¼ë‹¤", 
            "value_3": "ëª¨ë‚­ì‚¬ì´í™ë°˜",
            "value_4": "ëª¨ë‚­í™ë°˜ë†í¬",
            "value_5": "ë¹„ë“¬"  # ë°ì´í„°ëŠ” ìˆ˜ì§‘í•˜ì§€ë§Œ ê²°ê³¼ì—ì„œ ì œì™¸
            # "value_6": "íƒˆëª¨" - ì œì™¸
        }
        
        for key, weighted_vals in values.items():
            if weighted_vals is not None and len(weighted_vals) > 0 and scores:
                # ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ë¥¼ ë” ê°•í•˜ê²Œ ì ìš© (0.7)
                similarity_weight = 0.7
                severity_weight = 0.3
                
                # ìœ ì‚¬ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°
                weighted_sum = 0
                total_weight = 0
                
                for i, (val, score) in enumerate(zip(weighted_vals, scores)):
                    # ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ë” í° ê°€ì¤‘ì¹˜
                    weight = (score * similarity_weight) + (val * severity_weight)
                    weighted_sum += val * weight
                    total_weight += weight
                
                if total_weight > 0:
                    normalized_score = weighted_sum / total_weight
                    category_name = category_names.get(key, key)
                    # ë¹„ë“¬ì€ ê²°ê³¼ì—ì„œ ì œì™¸ (ë¹›ë°˜ì‚¬ ì˜¤ì¸ ë¬¸ì œ)
                    if category_name != "ë¹„ë“¬":
                        weighted_scores[category_name] = round(normalized_score, 2)
        
        return weighted_scores
    
    def _get_primary_category_with_threshold(self, weighted_categories: Dict[str, float], scores: List[float]) -> str:
        """ì„ê³„ê°’ì„ ì ìš©í•œ ì£¼ìš” ì¹´í…Œê³ ë¦¬ ê²°ì • (ë¹„ë“¬/íƒˆëª¨ ì œì™¸)"""
        if not weighted_categories:
            return "0.ì–‘í˜¸"
        
        # ë¹„ë“¬ê³¼ íƒˆëª¨ë¥¼ ì œì™¸í•œ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
        filtered_categories = {
            k: v for k, v in weighted_categories.items() 
            if 'ë¹„ë“¬' not in k and 'íƒˆëª¨' not in k
        }
        
        if not filtered_categories:
            print("[DEBUG] ë¹„ë“¬/íƒˆëª¨ ì œì™¸ í›„ ë‚¨ì€ ì¹´í…Œê³ ë¦¬ ì—†ìŒ -> ì–‘í˜¸")
            return "0.ì–‘í˜¸"
        
        # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        avg_score = statistics.mean(scores) if scores else 0
        
        # ì„ê³„ê°’: í‰ê·  ìœ ì‚¬ë„ê°€ 0.5 ë¯¸ë§Œì´ë©´ "ì–‘í˜¸" ë°˜í™˜
        if avg_score < 0.5:
            return "0.ì–‘í˜¸"
        
        # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ì¹´í…Œê³ ë¦¬ ì„ íƒ (ë¹„ë“¬/íƒˆëª¨ ì œì™¸ëœ ì¹´í…Œê³ ë¦¬ì—ì„œ)
        primary_category = max(filtered_categories.items(), key=lambda x: float(x[1]))[0]
        category_score = float(filtered_categories[primary_category])
        
        print(f"[DEBUG] ë¹„ë“¬/íƒˆëª¨ ì œì™¸ í›„ primary_category: {primary_category}, score: {category_score}")
        
        # ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
        if category_score < 0.6:
            # ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì¹´í…Œê³ ë¦¬ í™•ì¸
            sorted_categories = sorted(filtered_categories.items(), key=lambda x: float(x[1]), reverse=True)
            if len(sorted_categories) > 1 and float(sorted_categories[1][1]) >= 0.6:
                primary_category = sorted_categories[1][0]
            else:
                return "0.ì–‘í˜¸"
        
        return primary_category
    
    def _get_primary_severity_with_threshold(self, weighted_severities: Dict[str, float], scores: List[float]) -> str:
        """ì„ê³„ê°’ì„ ì ìš©í•œ ì£¼ìš” ì‹¬ê°ë„ ê²°ì • (ì—„ê²©í•œ ê¸°ì¤€)"""
        if not weighted_severities:
            return "0.ì–‘í˜¸"
        
        # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        avg_score = statistics.mean(scores) if scores else 0
        
        # ì„ê³„ê°’: í‰ê·  ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ ì–‘í˜¸ë¡œ íŒë‹¨ (ë” ì—„ê²©í•˜ê²Œ)
        if avg_score < 0.5:  # 0.3 â†’ 0.5ë¡œ ê°•í™”
            return "0.ì–‘í˜¸"
        
        # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ì‹¬ê°ë„ ì„ íƒ
        primary_severity = max(weighted_severities.items(), key=lambda x: float(x[1]))[0]
        
        # ì¤‘ë“±ë„/ì¤‘ì¦ ì§„ë‹¨ì„ ìœ„í•´ì„œëŠ” ë” ë†’ì€ ìœ ì‚¬ë„ í•„ìš” (ë” ì—„ê²©í•˜ê²Œ)
        if primary_severity in ["2.ì¤‘ë“±ë„", "3.ì¤‘ì¦"]:
            severity_score = float(weighted_severities[primary_severity])
            if severity_score < 0.8:  # 0.6 â†’ 0.8ë¡œ ê°•í™”
                # ê²½ì¦ìœ¼ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ
                if "1.ê²½ì¦" in weighted_severities:
                    primary_severity = "1.ê²½ì¦"
                else:
                    primary_severity = "0.ì–‘í˜¸"
        
        # ê²½ì¦ ì§„ë‹¨ë„ ë” ì—„ê²©í•œ ê¸°ì¤€ ì ìš©
        elif primary_severity == "1.ê²½ì¦":
            severity_score = float(weighted_severities[primary_severity])
            if severity_score < 0.6:  # ê²½ì¦ë„ 0.6 ì´ìƒ í•„ìš”
                primary_severity = "0.ì–‘í˜¸"
        
        return primary_severity
    
    def _generate_recommendations(self, values: Dict[str, List[int]], categories: List[str]) -> List[str]:
        """ì§„ë‹¨ ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œì‚¬í•­
        category_recommendations = {
            "ë¯¸ì„¸ê°ì§ˆ": [
                "ë¶€ë“œëŸ¬ìš´ ê°ì§ˆ ì œê±° ì œí’ˆ ì‚¬ìš©",
                "ê³¼ë„í•œ ë§ˆì‚¬ì§€ í”¼í•˜ê¸°",
                "ìˆ˜ë¶„ ê³µê¸‰ì— ì§‘ì¤‘"
            ],
            "í”¼ì§€ê³¼ë‹¤": [
                "ê¹¨ë—í•œ ìƒ´í‘¸ ì‚¬ìš©",
                "ë‘í”¼ ë§ˆì‚¬ì§€ë¡œ í˜ˆì•¡ìˆœí™˜ ê°œì„ ",
                "ì§€ì„± ë‘í”¼ ì „ìš© ì œí’ˆ ì‚¬ìš©"
            ],
            "ëª¨ë‚­ì‚¬ì´í™ë°˜": [
                "ìê·¹ì´ ì ì€ ìƒ´í‘¸ ì‚¬ìš©",
                "ë‘í”¼ ë³´ìŠµ ê´€ë¦¬",
                "ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬"
            ],
            "ëª¨ë‚­í™ë°˜ë†í¬": [
                "ì˜ë£Œì§„ ìƒë‹´ ê¶Œì¥",
                "í•­ê·  ìƒ´í‘¸ ì‚¬ìš©",
                "ë‘í”¼ ì²­ê²° ìœ ì§€"
            ],
            "ë¹„ë“¬": [  # ë°ì´í„°ëŠ” ìœ ì§€í•˜ì§€ë§Œ ê²°ê³¼ì—ì„œ ì œì™¸
                "í•­ì§„ê·  ìƒ´í‘¸ ì‚¬ìš©",
                "ê·œì¹™ì ì¸ ìƒ´í‘¸",
                "ë‘í”¼ ê±´ì¡° ë°©ì§€"
            ],
            "íƒˆëª¨": [
                "ë‘í”¼ ë§ˆì‚¬ì§€",
                "ì˜ì–‘ ê· í˜• ì¡íŒ ì‹ë‹¨",
                "ì¶©ë¶„í•œ ìˆ˜ë©´",
                "ì˜ë£Œì§„ ìƒë‹´ ê¶Œì¥"
            ]
        }
        
        # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œì‚¬í•­ ì¶”ê°€
        primary_category = self._get_most_common(categories)
        if primary_category in category_recommendations:
            recommendations.extend(category_recommendations[primary_category])
        
        # ë†’ì€ ì ìˆ˜ ì¹´í…Œê³ ë¦¬ë³„ ì¶”ê°€ ì¶”ì²œ
        for key, vals in values.items():
            if vals is not None and len(vals) > 0 and statistics.mean(vals) > 1.5:  # ì¤‘ë“±ë„ ì´ìƒ
                category_name = {
                    "value_1": "ë¯¸ì„¸ê°ì§ˆ",
                    "value_2": "í”¼ì§€ê³¼ë‹¤",
                    "value_3": "ëª¨ë‚­ì‚¬ì´í™ë°˜", 
                    "value_4": "ëª¨ë‚­í™ë°˜ë†í¬",
                    "value_5": "ë¹„ë“¬"  # ë°ì´í„°ëŠ” ìˆ˜ì§‘í•˜ì§€ë§Œ ê²°ê³¼ì—ì„œ ì œì™¸
                    # "value_6": "íƒˆëª¨" - ì œì™¸
                }.get(key, "")
                
                if category_name and category_name != primary_category and category_name != "ë¹„ë“¬":
                    recommendations.extend(category_recommendations.get(category_name, []))
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
        unique_recommendations = list(dict.fromkeys(recommendations))
        return unique_recommendations[:5]
    
    def search_by_specific_condition(self, 
                                   image_bytes: bytes, 
                                   category: str, 
                                   top_k: int = 5) -> Dict[str, Any]:
        """íŠ¹ì • ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰"""
        try:
            # ì‚¬ìš©ì ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì˜ë£Œìš© ì´ë¯¸ì§€ ìˆ˜ì¤€ìœ¼ë¡œ)
            preprocessed_image_bytes = image_preprocessing_service.preprocess_for_medical_analysis(image_bytes)
            
            # CLIP ì•™ìƒë¸”ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì‚¬ìš©)
            hybrid_features = self.clip_service.extract_hybrid_features(preprocessed_image_bytes)
            query_vector = hybrid_features["combined"]
            
            if query_vector is None or len(query_vector) == 0:
                return {"success": False, "error": "ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨"}
            
            # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰
            # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            query_vector_list = query_vector.tolist() if hasattr(query_vector, 'tolist') else query_vector
            similar_cases = self.pinecone_service.search_by_category(
                query_vector_list, category, top_k
            )
            
            if similar_cases is None or len(similar_cases) == 0:
                return {
                    "success": False,
                    "error": f"{category} ì¹´í…Œê³ ë¦¬ì˜ ìœ ì‚¬ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ìœ ì‚¬ ì¼€ì´ìŠ¤ì— ì´ë¯¸ì§€ ê²½ë¡œ ì •ë³´ ì¶”ê°€ (íƒˆëª¨ ì œì™¸)
            enhanced_similar_cases = []
            for case in similar_cases:
                enhanced_case = case.copy()
                metadata = case.get("metadata", {})
                
                # íƒˆëª¨ ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš° ì œì™¸
                case_category = metadata.get("category", "")
                if case_category == "íƒˆëª¨":
                    continue
                
                image_path = get_image_path_from_metadata(metadata)
                if image_path:
                    enhanced_case["image_path"] = image_path
                enhanced_similar_cases.append(enhanced_case)
            
            return {
                "success": True,
                "category": category,
                "similar_cases": enhanced_similar_cases,
                "total_cases": len(similar_cases)
            }
            
        except Exception as e:
            import traceback
            print(f"[ERROR] ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            print(f"[ERROR] ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return {"success": False, "error": str(e)}

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rag_service = RAGService()
