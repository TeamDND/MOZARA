"""
CLIP ì•™ìƒë¸” ì„œë¹„ìŠ¤ - ë‹¤ì¤‘ CLIP ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ íŠ¹ì§• ì¶”ì¶œ
"""
import torch
import open_clip
import numpy as np
from PIL import Image
import io
from typing import List, Dict, Optional, Tuple
import logging
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CLIPEnsembleService:
    """CLIP ì•™ìƒë¸” ì„œë¹„ìŠ¤ - ë‹¤ì¤‘ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ ì¡°í•©"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.models = {}
        self.prompt_sets = {}
        self.model_weights = {}
        self._initialize_models()
        self._initialize_prompts()
        logger.info(f"âœ… CLIP ì•™ìƒë¸” ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def _initialize_models(self):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™” (open_clip_torch ì‚¬ìš©) - ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì•™ìƒë¸”"""
        model_configs = [
            ("ViT-B-32", "openai", 0.4, "ê¸°ë³¸ ëª¨ë¸ - ë¹ ë¥´ê³  ì•ˆì •ì "),
            ("ViT-B-16", "openai", 0.3, "ê³ í•´ìƒë„ ëª¨ë¸ - ì„¸ë°€í•œ íŠ¹ì§•"),
            ("RN50", "openai", 0.3, "ResNet ê¸°ë°˜ - ë‹¤ë¥¸ ì•„í‚¤í…ì²˜")
        ]
        
        for model_name, pretrained, weight, description in model_configs:
            try:
                model, _, preprocess = open_clip.create_model_and_transforms(
                    model_name, 
                    pretrained=pretrained, 
                    device=self.device
                )
                tokenizer = open_clip.get_tokenizer(model_name)
                
                # ë©”ëª¨ë¦¬ ìµœì í™”: ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •
                model.eval()
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                
                self.models[model_name] = {
                    "model": model,
                    "preprocess": preprocess,
                    "tokenizer": tokenizer,
                    "weight": weight,
                    "description": description
                }
                logger.info(f"âœ… {model_name} ë¡œë“œ ì™„ë£Œ ({description})")
            except Exception as e:
                logger.warning(f"âš ï¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.device == "cuda":
                    torch.cuda.empty_cache()
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        if self.models:
            total_weight = sum(config["weight"] for config in self.models.values())
            for model_name in self.models:
                self.models[model_name]["weight"] /= total_weight
    
    def _initialize_prompts(self):
        """ë‹¤ì–‘í•œ ê´€ì ì˜ í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ ì´ˆê¸°í™”"""
        self.prompt_sets = {
            "medical": {
                "prompts": [
                    "healthy scalp with clean skin",
                    "scalp with dandruff flakes",
                    "oily scalp with excess sebum production",
                    "scalp with red inflammation",
                    "scalp with clogged hair follicles"
                ],
                "weight": 0.4,
                "description": "ì˜ë£Œì  ê´€ì  (íƒˆëª¨ ì œì™¸)"
            },
            "descriptive": {
                "prompts": [
                    "scalp with white spots and flakes",
                    "shiny oily scalp surface",
                    "red inflamed scalp area",
                    "scalp with visible hair follicles",
                    "scalp with skin irritation",
                    "clean healthy scalp skin"
                ],
                "weight": 0.3,
                "description": "ì‹œê°ì  ì„¤ëª…"
            },
            "symptom_based": {
                "prompts": [
                    "scalp showing signs of dandruff",
                    "scalp with excessive oil production",
                    "scalp with inflammation signs",
                    "scalp with skin problems",
                    "normal healthy scalp condition"
                ],
                "weight": 0.3,
                "description": "ì¦ìƒ ê¸°ë°˜ (íƒˆëª¨ ì œì™¸)"
            }
        }
    
    def _get_image_hash(self, image_bytes: bytes) -> str:
        """ì´ë¯¸ì§€ í•´ì‹œ ìƒì„± (ìºì‹±ìš©)"""
        return hashlib.md5(image_bytes).hexdigest()
    
    def _extract_single_model_features(self, image_bytes: bytes, model_name: str) -> Tuple[np.ndarray, np.ndarray]:
        """ë‹¨ì¼ ëª¨ë¸ íŠ¹ì§• ì¶”ì¶œ"""
        if model_name not in self.models:
            raise ValueError(f"ëª¨ë¸ {model_name}ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        config = self.models[model_name]
        model = config["model"]
        preprocess = config["preprocess"]
        
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        image_input = preprocess(image).unsqueeze(0).to(self.device)
        
        # íŠ¹ì§• ì¶”ì¶œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        with torch.no_grad():
            image_features = model.encode_image(image_input)
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # CPUë¡œ ì´ë™í•˜ì—¬ GPU ë©”ëª¨ë¦¬ í•´ì œ
            result = image_features.cpu().numpy().flatten()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            del image_features, image_input
            if self.device == "cuda":
                torch.cuda.empty_cache()
        
        return result, None
    
    def extract_single_model_features(self, image_bytes: bytes, model_name: str) -> np.ndarray:
        """ë‹¨ì¼ ëª¨ë¸ë¡œ íŠ¹ì§• ì¶”ì¶œ"""
        features, _ = self._extract_single_model_features(image_bytes, model_name)
        return features.flatten()
    
    def extract_ensemble_features(self, image_bytes: bytes) -> np.ndarray:
        """ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ (ë‹¤ì¤‘ ëª¨ë¸)"""
        all_features = []
        weights = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ëª¨ë¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
        with ThreadPoolExecutor(max_workers=len(self.models)) as executor:
            futures = {}
            
            for model_name, config in self.models.items():
                future = executor.submit(
                    self.extract_single_model_features, 
                    image_bytes, 
                    model_name
                )
                futures[future] = (model_name, config["weight"])
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in futures:
                model_name, weight = futures[future]
                try:
                    features = future.result()
                    all_features.append(features)
                    weights.append(weight)
                    logger.debug(f"âœ… {model_name} íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"âŒ {model_name} íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        
        if not all_features:
            raise RuntimeError("ëª¨ë“  ëª¨ë¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸”
        # ëª¨ë“  íŠ¹ì§• ë²¡í„°ë¥¼ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ ë§ì¶¤
        min_dim = min(features.shape[0] for features in all_features)
        normalized_features = []
        
        for features in all_features:
            if features.shape[0] > min_dim:
                # ì°¨ì›ì´ í° ê²½ìš° ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                normalized_features.append(features[:min_dim])
            else:
                # ì°¨ì›ì´ ì‘ì€ ê²½ìš° íŒ¨ë”©
                padded = np.zeros(min_dim)
                padded[:features.shape[0]] = features
                normalized_features.append(padded)
        
        # íŠ¹ì§• ê²°í•© (concatenation) ë°©ì‹ìœ¼ë¡œ ì•™ìƒë¸”
        # ëª¨ë“  ëª¨ë¸ì˜ íŠ¹ì§•ì„ ì—°ê²°í•˜ì—¬ ì°¨ì› ì¦ê°€
        ensemble_features = np.concatenate(normalized_features, axis=0)
        
        # L2 ì •ê·œí™”
        ensemble_features = ensemble_features / (np.linalg.norm(ensemble_features) + 1e-8)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del normalized_features, all_features, weights
        if self.device == "cuda":
            torch.cuda.empty_cache()
        
        logger.info(f"ğŸ” ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(ensemble_features)}ì°¨ì›, {len(self.models)}ê°œ ëª¨ë¸ ì‚¬ìš©")
        return ensemble_features
    
    def extract_prompt_ensemble_features(self, image_bytes: bytes) -> Dict[str, np.ndarray]:
        """í”„ë¡¬í”„íŠ¸ ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ"""
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        prompt_features = {}
        
        for category, config in self.prompt_sets.items():
            prompts = config["prompts"]
            weight = config["weight"]
            
            # ê° ëª¨ë¸ì—ì„œ í”„ë¡¬í”„íŠ¸ë³„ íŠ¹ì§• ì¶”ì¶œ
            category_features = []
            
            for model_name, model_config in self.models.items():
                model = model_config["model"]
                preprocess = model_config["preprocess"]
                tokenizer = model_config["tokenizer"]
                model_weight = model_config["weight"]
                
                # í…ìŠ¤íŠ¸ í† í°í™” (open_clip_torch ì‚¬ìš©)
                text_inputs = tokenizer(prompts).to(self.device)
                
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                image_input = preprocess(image).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ
                    image_features = model.encode_image(image_input)
                    text_features = model.encode_text(text_inputs)
                    
                    # ì •ê·œí™”
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                    
                    # ìœ ì‚¬ë„ ê³„ì‚°
                    similarities = torch.matmul(image_features, text_features.T)
                    
                    # ê°€ì¤‘ í‰ê· 
                    weighted_similarities = similarities * model_weight
                    category_features.append(weighted_similarities.cpu().numpy())
            
            # ëª¨ë¸ë³„ ê²°ê³¼ ê²°í•©
            if category_features:
                combined_features = np.mean(category_features, axis=0)
                prompt_features[category] = combined_features.flatten()
                logger.debug(f"âœ… {category} í”„ë¡¬í”„íŠ¸ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
        
        return prompt_features
    
    def extract_hybrid_features(self, image_bytes: bytes) -> Dict[str, np.ndarray]:
        """í•˜ì´ë¸Œë¦¬ë“œ íŠ¹ì§• ì¶”ì¶œ (ëª¨ë¸ ì•™ìƒë¸” + í”„ë¡¬í”„íŠ¸ ì•™ìƒë¸”)"""
        # ëª¨ë¸ ì•™ìƒë¸” íŠ¹ì§•
        model_features = self.extract_ensemble_features(image_bytes)
        
        # í”„ë¡¬í”„íŠ¸ ì•™ìƒë¸” íŠ¹ì§•
        prompt_features = self.extract_prompt_ensemble_features(image_bytes)
        
        # ëª¨ë“  íŠ¹ì§• ê²°í•©
        hybrid_features = {
            "model_ensemble": model_features,
            **prompt_features
        }
        
        # í†µí•© íŠ¹ì§• ë²¡í„° ìƒì„±
        all_feature_vectors = [model_features]
        for features in prompt_features.values():
            all_feature_vectors.append(features)
        
        # ê²°í•©ëœ íŠ¹ì§• ë²¡í„° (ì°¨ì› í™•ì¸ í›„ ê²°í•©)
        try:
            combined_features = np.concatenate(all_feature_vectors)
        except ValueError as e:
            # ì°¨ì›ì´ ë§ì§€ ì•ŠëŠ” ê²½ìš°, ëª¨ë“  ë²¡í„°ë¥¼ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ ë§ì¶¤
            min_dim = min(len(fv) for fv in all_feature_vectors)
            normalized_vectors = []
            
            for fv in all_feature_vectors:
                if len(fv) > min_dim:
                    normalized_vectors.append(fv[:min_dim])
                else:
                    padded = np.zeros(min_dim)
                    padded[:len(fv)] = fv
                    normalized_vectors.append(padded)
            
            combined_features = np.concatenate(normalized_vectors)
        hybrid_features["combined"] = combined_features
        
        logger.info(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(combined_features)}ì°¨ì›")
        return hybrid_features
    
    def extract_weighted_ensemble_features(self, image_bytes: bytes, model_weights: Dict[str, float]) -> np.ndarray:
        """ê°€ì¤‘ì¹˜ ì¡°ì •ëœ ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ (Pinecone ë°ì´í„° ì¬ì—…ë¡œë“œ ì—†ì´)"""
        all_features = []
        weights = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ëª¨ë¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
        with ThreadPoolExecutor(max_workers=len(self.models)) as executor:
            futures = {}
            
            for model_name, config in self.models.items():
                # ì‚¬ìš©ì ì§€ì • ê°€ì¤‘ì¹˜ ì‚¬ìš©
                weight = model_weights.get(model_name, config["weight"])
                future = executor.submit(
                    self.extract_single_model_features, 
                    image_bytes, 
                    model_name
                )
                futures[future] = (model_name, weight)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in futures:
                model_name, weight = futures[future]
                try:
                    features = future.result()
                    all_features.append(features)
                    weights.append(weight)
                    logger.debug(f"âœ… {model_name} íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ (ê°€ì¤‘ì¹˜: {weight})")
                except Exception as e:
                    logger.error(f"âŒ {model_name} íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        
        if not all_features:
            raise RuntimeError("ëª¨ë“  ëª¨ë¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸” (ê¸°ì¡´ concatenation ëŒ€ì‹ )
        # ëª¨ë“  íŠ¹ì§• ë²¡í„°ë¥¼ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ ë§ì¶¤
        min_dim = min(features.shape[0] for features in all_features)
        normalized_features = []
        
        for features in all_features:
            if features.shape[0] > min_dim:
                # ì°¨ì›ì´ í° ê²½ìš° ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                normalized_features.append(features[:min_dim])
            else:
                # ì°¨ì›ì´ ì‘ì€ ê²½ìš° íŒ¨ë”©
                padded = np.zeros(min_dim)
                padded[:features.shape[0]] = features
                normalized_features.append(padded)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_ensemble = np.zeros(min_dim)
        total_weight = sum(weights)
        
        for features, weight in zip(normalized_features, weights):
            weighted_ensemble += features * (weight / total_weight)
        
        # L2 ì •ê·œí™”
        weighted_ensemble = weighted_ensemble / (np.linalg.norm(weighted_ensemble) + 1e-8)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del normalized_features, all_features, weights
        if self.device == "cuda":
            torch.cuda.empty_cache()
        
        logger.info(f"ğŸ” ê°€ì¤‘ì¹˜ ì¡°ì • ì•™ìƒë¸” íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(weighted_ensemble)}ì°¨ì›")
        return weighted_ensemble
    
    def get_model_info(self) -> Dict[str, any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "service_name": "CLIP Ensemble Service",
            "device": self.device,
            "models": {
                name: {
                    "weight": config["weight"],
                    "description": config["description"]
                } for name, config in self.models.items()
            },
            "prompt_sets": {
                name: {
                    "weight": config["weight"],
                    "description": config["description"],
                    "prompt_count": len(config["prompts"])
                } for name, config in self.prompt_sets.items()
            },
            "feature_dimensions": {
                "model_ensemble": 512,  # CLIP ê¸°ë³¸ ì°¨ì›
                "prompt_ensemble": 512 * len(self.prompt_sets),
                "combined": 512 * (1 + len(self.prompt_sets))
            }
        }
    
    def health_check(self) -> Dict[str, any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        return {
            "status": "healthy" if self.models else "unavailable",
            "loaded_models": list(self.models.keys()),
            "prompt_sets": list(self.prompt_sets.keys()),
            "device": self.device,
            "cache_size": self._cached_single_model_features.cache_info().currsize
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
clip_ensemble_service = CLIPEnsembleService()
