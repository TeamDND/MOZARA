"""
RAG ê¸°ë°˜ íƒˆëª¨ ì „ë¬¸ ì±—ë´‡ ì„œë¹„ìŠ¤ (ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬)
LangChain + ì‚¬ìš©ìë³„ ëŒ€í™” ê¸°ì–µ ê¸°ëŠ¥
"""
import os
import logging
from typing import List, Dict, Optional
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path

# ë¡œê¹… ì„¤ì • (ë¨¼ì € ì„¤ì •)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ - í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ì°¾ê¸°
current_dir = Path(__file__).resolve().parent  # services/rag_chatbot/
project_root = current_dir.parent.parent.parent.parent  # MOZARA/
env_path = project_root / ".env"

if env_path.exists():
    load_dotenv(str(env_path))
    logger.info(f"âœ… .env íŒŒì¼ ë¡œë“œ: {env_path}")
else:
    logger.warning(f"âš ï¸ .env íŒŒì¼ ì—†ìŒ: {env_path}")

# LangChain imports
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.retrievers import MergerRetriever
from langchain.schema import Document

# Pinecone imports
from pinecone import Pinecone

class HairLossRAGChatbotWithMemory:
    """ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” RAG ì±—ë´‡"""

    def __init__(self):
        """RAG ì±—ë´‡ ì´ˆê¸°í™”"""
        self.setup_apis()
        self.setup_vectorstores()
        self.setup_llm()

        # ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (user_id: memory)
        self.user_memories = {}
        # ì‚¬ìš©ìë³„ ì²´ì¸ ì €ì¥ì†Œ (user_id: chain)
        self.user_chains = {}

        logger.info("âœ… RAG ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ (ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬)")

    def setup_apis(self):
        """API í‚¤ ì„¤ì •"""
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        if not self.pinecone_api_key:
            raise ValueError("PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # GOOGLE_API_KEY ì‚¬ìš©
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        if not self.google_api_key:
            raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info(f"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ")

    def setup_vectorstores(self):
        """Pinecone ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •"""
        try:
            logger.info("ğŸ”„ Pinecone ì´ˆê¸°í™” ì‹œì‘...")
            pc = Pinecone(api_key=self.pinecone_api_key)

            self.embeddings = OpenAIEmbeddings(
                openai_api_key=self.openai_api_key,
                model="text-embedding-ada-002"
            )

            index_names = {
                'papers': os.getenv("PINECONE_INDEX_NAME1", "hair-loss-papers"),
                'encyclopedia': os.getenv("PINECONE_INDEX_NAME3", "hair-encyclopedia")
            }

            self.vectorstores = {}

            # list_indexes() í˜¸ì¶œ ì œê±°í•˜ê³  ì§ì ‘ ì—°ê²° ì‹œë„
            for name, index_name in index_names.items():
                try:
                    logger.info(f"ğŸ”„ {name} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì‹œë„: {index_name}")
                    vectorstore = PineconeVectorStore(
                        index_name=index_name,
                        embedding=self.embeddings,
                        pinecone_api_key=self.pinecone_api_key
                    )
                    self.vectorstores[name] = vectorstore
                    logger.info(f"âœ… {name} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ: {index_name}")
                except Exception as e:
                    logger.warning(f"âš ï¸  {name} ì—°ê²° ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {e}")
                    continue

            if not self.vectorstores:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # MergerRetriever
            retrievers = [
                vs.as_retriever(search_kwargs={"k": 5})
                for vs in self.vectorstores.values()
            ]
            self.retriever = MergerRetriever(retrievers=retrievers)

            logger.info("âœ… ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def setup_llm(self):
        """LLM ì„¤ì •"""
        # í™˜ê²½ë³€ìˆ˜ë¡œë„ ì„¤ì •
        os.environ["GOOGLE_API_KEY"] = self.google_api_key

        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=self.google_api_key,
            temperature=0.3,
            convert_system_message_to_human=False
        )
        logger.info("âœ… Gemini LLM ì„¤ì • ì™„ë£Œ (model: gemini-2.5-flash)")

    def get_or_create_chain(self, user_id: str) -> ConversationalRetrievalChain:
        """ì‚¬ìš©ìë³„ ì²´ì¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""

        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì²´ì¸ì´ë©´ ë°˜í™˜
        if user_id in self.user_chains:
            logger.info(f"ğŸ”„ ê¸°ì¡´ ì²´ì¸ ì‚¬ìš©: {user_id}")
            return self.user_chains[user_id]

        # ìƒˆ ë©”ëª¨ë¦¬ ìƒì„±
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        self.user_memories[user_id] = memory

        # Condense Question Prompt - ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•˜ì—¬ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
        condense_template = """ì´ì „ ëŒ€í™” ê¸°ë¡ê³¼ í›„ì† ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë…ë¦½ì ì´ê³  ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ì¤‘ìš”:
- ëŒ€í™” ê¸°ë¡ì— ìˆëŠ” ì •ë³´ë¥¼ ì§ˆë¬¸ì— í¬í•¨ì‹œí‚¤ì„¸ìš”
- "ë‚´ ì´ë¦„"ì²˜ëŸ¼ ëŒ€í™” ë§¥ë½ì„ ì°¸ì¡°í•˜ëŠ” ë¶€ë¶„ì€ ì‹¤ì œ ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”
- ëŒ€í™” ê¸°ë¡ì—ë§Œ ë‹µì´ ìˆëŠ” ì§ˆë¬¸ì´ë¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”

ì´ì „ ëŒ€í™” ê¸°ë¡:
{chat_history}

í›„ì† ì§ˆë¬¸: {question}
ë…ë¦½ì ì¸ ì§ˆë¬¸:"""

        condense_question_prompt = PromptTemplate.from_template(condense_template)

        # QA Prompt - ë‹µë³€ ìƒì„±ìš© (ì´ì „ ëŒ€í™” ê¸°ë¡ í¬í•¨)
        qa_template = """ë‹¹ì‹ ì€ íƒˆëª¨ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ë‹µë³€ ê·œì¹™:
1. ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ì¡°í•˜ëŠ” ê²½ìš°, ëŒ€í™” ê¸°ë¡ì„ ìš°ì„ ì ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”
2. íƒˆëª¨ ê´€ë ¨ ì˜í•™ ì§ˆë¬¸ì€ ì œê³µëœ ì°¸ê³  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
3. ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ë‹µë³€í•˜ë˜, ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œì¥í•˜ì„¸ìš”
4. ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
5. ë‹µë³€ì€ 300ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        qa_prompt = PromptTemplate.from_template(qa_template)

        # ìƒˆ ì²´ì¸ ìƒì„± - get_chat_history ì¶”ê°€
        def get_chat_history(inputs) -> str:
            """ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜"""
            res = []
            for msg in inputs:
                if hasattr(msg, 'content'):
                    role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                    res.append(f"{role}: {msg.content}")
            return "\n".join(res) if res else "ì´ì „ ëŒ€í™” ì—†ìŒ"

        chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.retriever,
            memory=memory,
            return_source_documents=True,
            condense_question_prompt=condense_question_prompt,
            combine_docs_chain_kwargs={"prompt": qa_prompt},
            get_chat_history=get_chat_history,
            verbose=True
        )

        self.user_chains[user_id] = chain
        logger.info(f"ğŸ†• ìƒˆ ì²´ì¸ ìƒì„±: {user_id}")

        return chain

    def is_hair_related_question(self, message: str, source_docs: List) -> bool:
        """ì§ˆë¬¸ì´ íƒˆëª¨ ê´€ë ¨ì¸ì§€ íŒë³„"""
        # 1. ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìˆê³  ìœ ì‚¬ë„ ì ìˆ˜ê°€ ì¶©ë¶„íˆ ë†’ì€ì§€ í™•ì¸
        if source_docs and len(source_docs) > 0:
            # ë¬¸ì„œì— score ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(source_docs[0], 'metadata'):
                # ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸ (ë³´í†µ 0.7 ì´ìƒì´ë©´ ê´€ë ¨ì„±ì´ ë†’ìŒ)
                # LangChain DocumentëŠ” scoreë¥¼ ì§ì ‘ ê°€ì§€ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ë¡œ íŒë‹¨
                return True
        
        # 2. íƒˆëª¨ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        hair_keywords = [
            'íƒˆëª¨', 'ëª¨ë°œ', 'ë¨¸ë¦¬', 'í—¤ì–´', 'ëª¨ë‚­', 'ë‘í”¼', 
            'ë¯¸ë…¹ì‹œë”œ', 'í”¼ë‚˜ìŠ¤í…Œë¦¬ë“œ', 'í”„ë¡œí˜ì‹œì•„', 'ì•„ë³´ë‹¤íŠ¸',
            'ëª¨ë°œì´ì‹', 'aga', 'fphl', 'dht', 'ì•ˆë“œë¡œê²',
            'ì›í˜•íƒˆëª¨', 'ì§€ë£¨ì„±', 'ë¹„ë“¬', 'ê°€ë¥´ë§ˆ', 'ì •ìˆ˜ë¦¬',
            'hair', 'baldness', 'alopecia', 'finasteride', 'minoxidil'
        ]
        
        message_lower = message.lower()
        if any(keyword in message_lower for keyword in hair_keywords):
            return True
        
        # 3. ê²€ìƒ‰ ê²°ê³¼ë„ ì—†ê³  í‚¤ì›Œë“œë„ ì—†ìœ¼ë©´ íƒˆëª¨ ê´€ë ¨ ì•„ë‹˜
        return False

    def chat(self, message: str, conversation_id: str = None, user_id: str = None) -> Dict:
        """ì±—ë´‡ ëŒ€í™” - ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ìœ ì§€"""
        try:
            # conversation_idê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            if not conversation_id:
                conversation_id = "default"
            
            # user_idê°€ ì—†ìœ¼ë©´ anonymousë¡œ ì„¤ì •
            if not user_id:
                user_id = "anonymous"

            logger.info(f"ğŸ’¬ [{conversation_id}] ì‚¬ìš©ì ì§ˆë¬¸: {message}")

            # ì‚¬ìš©ìë³„ ì²´ì¸ ê°€ì ¸ì˜¤ê¸°
            chain = self.get_or_create_chain(user_id)

            # ëŒ€í™” ê¸°ë¡ í™•ì¸
            memory = self.user_memories.get(user_id)
            if memory and hasattr(memory, 'chat_memory'):
                msg_count = len(memory.chat_memory.messages)
                logger.info(f"ğŸ“š [{user_id}] ëŒ€í™” ê¸°ë¡: {msg_count}ê°œ ë©”ì‹œì§€")

            # LangChain Chain ì‹¤í–‰
            result = chain.invoke({"question": message})

            # ì‘ë‹µ ì¶”ì¶œ
            answer = result.get("answer", "")
            source_docs = result.get("source_documents", [])

            # íƒˆëª¨ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
            is_hair_related = self.is_hair_related_question(message, source_docs)
            
            # ì†ŒìŠ¤ ì •ë³´ - íƒˆëª¨ ê´€ë ¨ì¼ ë•Œë§Œ í‘œì‹œ
            sources = []
            if is_hair_related:
                for doc in source_docs[:3]:
                    metadata = doc.metadata
                    title = metadata.get('title', metadata.get('source', 'Unknown'))
                    if title not in sources:
                        sources.append(title)

            logger.info(f"âœ… [{user_id}] ë‹µë³€ ìƒì„± ì™„ë£Œ")
            logger.info(f"ğŸ” íƒˆëª¨ ê´€ë ¨ ì§ˆë¬¸: {is_hair_related}")
            logger.info(f"ğŸ“– ì¶œì²˜: {sources}")

            # ì‘ë‹µ í›„ ë©”ëª¨ë¦¬ ì¹´ìš´íŠ¸ (ì²´ì¸ì´ ë©”ëª¨ë¦¬ì— ì €ì¥í•œ í›„)
            final_memory = self.user_memories.get(user_id)
            final_count = len(final_memory.chat_memory.messages) if final_memory and hasattr(final_memory, 'chat_memory') else 0

            logger.info(f"ğŸ’¾ [{user_id}] ìµœì¢… ë©”ì‹œì§€ ìˆ˜: {final_count}")

            return {
                "response": answer,
                "sources": sources,
                "conversation_id": conversation_id,
                "timestamp": datetime.now().isoformat(),
                "context_used": len(source_docs) > 0 and is_hair_related,
                "message_count": final_count,
                "is_hair_related": is_hair_related
            }

        except Exception as e:
            logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "conversation_id": conversation_id or "default",
                "timestamp": datetime.now().isoformat(),
                "context_used": False,
                "message_count": 0
            }

    def clear_conversation(self, conversation_id: str, user_id: str):
        """íŠ¹ì • ì‚¬ìš©ìì˜ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
        if user_id in self.user_chains:
            del self.user_chains[user_id]
        if user_id in self.user_memories:
            del self.user_memories[user_id]
        logger.info(f"ğŸ—‘ï¸  ì‚¬ìš©ì {user_id} ëŒ€í™” ê¸°ë¡ ì‚­ì œ: {conversation_id}")

    def get_health_status(self) -> Dict:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        return {
            "status": "healthy",
            "vectorstores": list(self.vectorstores.keys()),
            "active_conversations": len(self.user_chains),
            "conversation_ids": list(self.user_chains.keys()),
            "apis": {
                "pinecone": bool(self.pinecone_api_key),
                "openai": bool(self.openai_api_key),
                "gemini": bool(self.google_api_key)
            },
            "features": {
                "multi_user_memory": True,
                "langchain_chain": "ConversationalRetrievalChain",
                "retriever": "MergerRetriever",
                "memory_per_user": True
            }
        }

# ê¸€ë¡œë²Œ ì‹±ê¸€í†¤ (ì²´ì¸ íŒ©í† ë¦¬ ì—­í• )
_chatbot_instance = None

def get_final_rag_chatbot() -> HairLossRAGChatbotWithMemory:
    """RAG ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬)"""
    global _chatbot_instance
    if _chatbot_instance is None:
        _chatbot_instance = HairLossRAGChatbotWithMemory()
    return _chatbot_instance

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    try:
        print("=" * 60)
        print("ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬ RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸")
        print("=" * 60)

        chatbot = HairLossRAGChatbotWithMemory()

        # ì‚¬ìš©ì 1 ëŒ€í™”
        print("\n[ì‚¬ìš©ì 1] ëŒ€í™” ì‹œì‘")
        result1 = chatbot.chat("ë‚¨ì„±í˜• íƒˆëª¨ì˜ ì›ì¸ì€?", "user-1")
        print(f"ë‹µë³€: {result1['response'][:100]}...")
        print(f"ë©”ì‹œì§€ ìˆ˜: {result1['message_count']}")

        result2 = chatbot.chat("ë°©ê¸ˆ ë§í•œ ì›ì¸ ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ê±´ ë­ì•¼?", "user-1")
        print(f"ë‹µë³€: {result2['response'][:100]}...")
        print(f"ë©”ì‹œì§€ ìˆ˜: {result2['message_count']}")

        # ì‚¬ìš©ì 2 ëŒ€í™”
        print("\n[ì‚¬ìš©ì 2] ëŒ€í™” ì‹œì‘")
        result3 = chatbot.chat("ì—¬ì„±í˜• íƒˆëª¨ëŠ” ì–´ë–»ê²Œ ê´€ë¦¬í•˜ë‚˜ìš”?", "user-2")
        print(f"ë‹µë³€: {result3['response'][:100]}...")
        print(f"ë©”ì‹œì§€ ìˆ˜: {result3['message_count']}")

        # ìƒíƒœ í™•ì¸
        status = chatbot.get_health_status()
        print(f"\ní™œì„± ëŒ€í™”: {status['active_conversations']}ê°œ")
        print(f"ëŒ€í™” ID: {status['conversation_ids']}")

    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()