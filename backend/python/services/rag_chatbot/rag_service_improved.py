"""
RAG ê¸°ë°˜ íƒˆëª¨ ì „ë¬¸ ì±—ë´‡ ì„œë¹„ìŠ¤ (LangChain ì™„ì „ í™œìš©)
LangChainì˜ ConversationalRetrievalChainê³¼ MultiQueryRetriever ì‚¬ìš©
"""
import os
import logging
from typing import List, Dict, Optional
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv("../../../../.env")
load_dotenv("../../.env")

# LangChain imports
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.retrievers import MergerRetriever
from langchain.schema import Document

# Pinecone imports
from pinecone import Pinecone

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImprovedHairLossRAGChatbot:
    def __init__(self):
        """RAG ì±—ë´‡ ì´ˆê¸°í™” - LangChain ì™„ì „ í™œìš©"""
        self.setup_apis()
        self.setup_vectorstores()
        self.setup_memory()
        self.setup_llm()
        self.setup_chain()

    def setup_apis(self):
        """API í‚¤ ì„¤ì •"""
        # Pinecone ì„¤ì •
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        if not self.pinecone_api_key:
            raise ValueError("PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # OpenAI ì„¤ì • (ì„ë² ë”©ìš©)
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # Google Gemini ì„¤ì • (ìƒì„±ìš©)
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        if not self.google_api_key:
            raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info(f"âœ… API í‚¤ ì„¤ì • ì™„ë£Œ")

    def setup_vectorstores(self):
        """Pinecone ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • - LangChain VectorStore ì‚¬ìš©"""
        try:
            # Pinecone ì´ˆê¸°í™”
            pc = Pinecone(api_key=self.pinecone_api_key)

            # OpenAI ì„ë² ë”© ëª¨ë¸
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=self.openai_api_key,
                model="text-embedding-ada-002"
            )

            # ì¸ë±ìŠ¤ ì´ë¦„ë“¤
            index_names = {
                'papers': os.getenv("PINECONE_INDEX_NAME1", "hair-loss-papers"),
                'encyclopedia': os.getenv("PINECONE_INDEX_NAME3", "hair-encyclopedia")
            }

            # LangChain VectorStore ìƒì„±
            self.vectorstores = {}
            for name, index_name in index_names.items():
                try:
                    if index_name in pc.list_indexes().names():
                        vectorstore = PineconeVectorStore(
                            index_name=index_name,
                            embedding=self.embeddings,
                            pinecone_api_key=self.pinecone_api_key
                        )
                        self.vectorstores[name] = vectorstore
                        logger.info(f"âœ… {name} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ: {index_name}")
                    else:
                        logger.warning(f"âš ï¸  ì¸ë±ìŠ¤ ì—†ìŒ: {index_name}")
                except Exception as e:
                    logger.error(f"âŒ {name} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì‹¤íŒ¨: {e}")

            if not self.vectorstores:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # MergerRetrieverë¡œ ì—¬ëŸ¬ ë²¡í„°ìŠ¤í† ì–´ í†µí•©
            retrievers = [
                vs.as_retriever(search_kwargs={"k": 5})
                for vs in self.vectorstores.values()
            ]

            # ì—¬ëŸ¬ retrieverë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            self.retriever = MergerRetriever(retrievers=retrievers)

            logger.info("âœ… ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • ì™„ë£Œ (MergerRetriever ì‚¬ìš©)")

        except Exception as e:
            logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def setup_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì •"""
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        logger.info("âœ… ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì • ì™„ë£Œ")

    def setup_llm(self):
        """LLM ì„¤ì • - ChatGoogleGenerativeAI ì‚¬ìš©"""
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=self.google_api_key,
            temperature=0.3,
            convert_system_message_to_human=True
        )
        logger.info("âœ… Gemini LLM ì„¤ì • ì™„ë£Œ")

    def setup_chain(self):
        """ConversationalRetrievalChain ì„¤ì •"""

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_template = """ë‹¹ì‹ ì€ íƒˆëª¨ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì˜í•™ ë…¼ë¬¸ê³¼ ì „ë¬¸ ìë£Œë¥¼ **ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬** ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ê·œì¹™:
1. **ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ** êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ ì¸ìš©í•˜ê³ , ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
3. ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ì œê³µëœ ìë£Œì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
4. ì˜í•™ì  ì¡°ì–¸ì€ ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œì¥í•˜ê³ , ì—°êµ¬ ê¸°ë°˜ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
5. ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
6. ë‹µë³€ì€ 300ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”

ì°¸ê³  ë¬¸ì„œ (ì˜í•™ ë…¼ë¬¸ ë° ì—°êµ¬ ìë£Œ):
{context}"""

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        messages = [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template("{question}")
        ]
        qa_prompt = ChatPromptTemplate.from_messages(messages)

        # ConversationalRetrievalChain ìƒì„±
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.retriever,
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": qa_prompt},
            verbose=False
        )

        logger.info("âœ… ConversationalRetrievalChain ì„¤ì • ì™„ë£Œ")

    def chat(self, message: str, conversation_id: str = None) -> Dict:
        """ì±—ë´‡ ëŒ€í™” ë©”ì¸ í•¨ìˆ˜ - LangChain Chain ì‚¬ìš©"""
        try:
            logger.info(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸: {message}")

            # LangChain Chain ì‹¤í–‰
            result = self.chain({"question": message})

            # ì‘ë‹µ ì¶”ì¶œ
            answer = result.get("answer", "")
            source_docs = result.get("source_documents", [])

            # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
            sources = []
            for doc in source_docs[:3]:  # ìµœëŒ€ 3ê°œ
                metadata = doc.metadata
                title = metadata.get('title', metadata.get('source', 'Unknown'))
                if title not in sources:
                    sources.append(title)

            logger.info(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {answer[:100]}...")
            logger.info(f"ğŸ“š ì‚¬ìš©ëœ ë¬¸ì„œ ìˆ˜: {len(source_docs)}")
            logger.info(f"ğŸ“– ì¶œì²˜: {sources}")

            return {
                "response": answer,
                "sources": sources,
                "conversation_id": conversation_id or "default",
                "timestamp": datetime.now().isoformat(),
                "context_used": len(source_docs) > 0
            }

        except Exception as e:
            logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "conversation_id": conversation_id or "default",
                "timestamp": datetime.now().isoformat(),
                "context_used": False
            }

    def get_health_status(self) -> Dict:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        return {
            "status": "healthy",
            "vectorstores": list(self.vectorstores.keys()),
            "memory_messages": len(self.memory.chat_memory.messages) if hasattr(self.memory, 'chat_memory') else 0,
            "apis": {
                "pinecone": bool(self.pinecone_api_key),
                "openai": bool(self.openai_api_key),
                "gemini": bool(self.google_api_key)
            },
            "langchain_components": {
                "llm": "ChatGoogleGenerativeAI",
                "chain": "ConversationalRetrievalChain",
                "retriever": "MergerRetriever",
                "memory": "ConversationBufferMemory"
            }
        }

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
_chatbot_instance = None

def get_improved_rag_chatbot() -> ImprovedHairLossRAGChatbot:
    """ê°œì„ ëœ RAG ì±—ë´‡ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _chatbot_instance
    if _chatbot_instance is None:
        _chatbot_instance = ImprovedHairLossRAGChatbot()
    return _chatbot_instance

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    try:
        print("=" * 60)
        print("ê°œì„ ëœ RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸ (LangChain ì™„ì „ í™œìš©)")
        print("=" * 60)

        chatbot = ImprovedHairLossRAGChatbot()

        # ìƒíƒœ í™•ì¸
        status = chatbot.get_health_status()
        print(f"\nìƒíƒœ: {status}")

        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        test_questions = [
            "ë‚¨ì„±í˜• íƒˆëª¨(AGA)ì˜ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "í”¼ë‚˜ìŠ¤í…Œë¦¬ë“œì™€ ë¯¸ë…¹ì‹œë”œì˜ íš¨ê³¼ ì°¨ì´ëŠ”?",
            "ì—¬ì„±í˜• íƒˆëª¨ëŠ” ì–´ë–»ê²Œ ê´€ë¦¬í•˜ë‚˜ìš”?"
        ]

        for question in test_questions:
            print(f"\nì§ˆë¬¸: {question}")
            result = chatbot.chat(question)
            print(f"ë‹µë³€: {result['response']}")
            print(f"ì¶œì²˜: {result['sources']}")

    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()