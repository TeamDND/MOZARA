"""
RAG ê¸°ë°˜ íƒˆëª¨ ì „ë¬¸ ì±—ë´‡ ì„œë¹„ìŠ¤
LangChainê³¼ Pineconeì„ í™œìš©í•œ ê²€ìƒ‰ ì¦ê°• ìƒì„±
"""
import os
import logging
from typing import List, Dict, Optional
from datetime import datetime
from dotenv import load_dotenv

# LangChain imports
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Pinecone imports
from pinecone import Pinecone

# Google Gemini imports
import google.generativeai as genai

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
current_file = Path(__file__)
project_root = current_file.parent.parent.parent.parent.parent  # backend/python/services/rag_chatbot/ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ
env_path = project_root / ".env"

load_dotenv(str(env_path))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HairLossRAGChatbot:
    def __init__(self):
        """RAG ì±—ë´‡ ì´ˆê¸°í™”"""
        self.setup_apis()
        self.setup_vectorstore()
        self.setup_memory()
        self.setup_chains()

    def setup_apis(self):
        """API í‚¤ ì„¤ì •"""
        # Pinecone ì„¤ì •
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        if not self.pinecone_api_key:
            raise ValueError("PINECONE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # OpenAI ì„¤ì • (ì„ë² ë”©ìš©)
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # Google Gemini ì„¤ì • (ìƒì„±ìš©) - GOOGLE_API_KEY ì‚¬ìš©
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        if not self.google_api_key:
            raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info(f"ğŸ”‘ ì‚¬ìš©í•  Gemini API í‚¤: {self.google_api_key[:20]}... (ê¸¸ì´: {len(self.google_api_key)})")

        # Gemini ì„¤ì •
        genai.configure(api_key=self.google_api_key)
        self.gemini_model = genai.GenerativeModel('gemini-2.5-flash-lite')

        logger.info("âœ… API í‚¤ ì„¤ì • ì™„ë£Œ")

    def setup_vectorstore(self):
        """Pinecone ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • - ê° ì¸ë±ìŠ¤ë§ˆë‹¤ ë§ëŠ” ì„ë² ë”© ì‚¬ìš©"""
        try:
            # Pinecone ì´ˆê¸°í™”
            pc = Pinecone(api_key=self.pinecone_api_key)

            # ì¸ë±ìŠ¤ ì´ë¦„ë“¤ (papers + encyclopedia)
            self.index_names = {
                'papers': os.getenv("PINECONE_INDEX_NAME1", "hair-loss-papers"),
                'encyclopedia': os.getenv("PINECONE_INDEX_NAME3", "hair-encyclopedia")  # 47ê°œ ë²¡í„°
            }

            # ê° ì¸ë±ìŠ¤ë³„ Pinecone ì¸ë±ìŠ¤ ê°ì²´ì™€ ì„ë² ë”© ëª¨ë¸ ì €ì¥
            self.indexes = {}
            self.embeddings_map = {}

            # OpenAI ì„ë² ë”© (1536 dimension - papersìš©)
            self.openai_embeddings = OpenAIEmbeddings(
                openai_api_key=self.openai_api_key,
                model="text-embedding-ada-002"
            )

            # ê° ì¸ë±ìŠ¤ ì—°ê²°
            for name, index_name in self.index_names.items():
                try:
                    if index_name in pc.list_indexes().names():
                        index = pc.Index(index_name)
                        stats = index.describe_index_stats()
                        dimension = stats.get('dimension', 0)

                        self.indexes[name] = {
                            'index': index,
                            'dimension': dimension,
                            'name': index_name
                        }

                        logger.info(f"âœ… {name} ì¸ë±ìŠ¤ ì—°ê²° ì„±ê³µ: {index_name} (dimension: {dimension})")
                    else:
                        logger.warning(f"âš ï¸  ì¸ë±ìŠ¤ ì—†ìŒ: {index_name}")
                except Exception as e:
                    logger.error(f"âŒ {name} ì¸ë±ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")

            if not self.indexes:
                raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def setup_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì •"""
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        logger.info("âœ… ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì • ì™„ë£Œ")

    def setup_chains(self):
        """RAG ì²´ì¸ ì„¤ì •"""
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt_template = PromptTemplate(
            template="""ë‹¹ì‹ ì€ íƒˆëª¨ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì˜í•™ ë…¼ë¬¸ê³¼ ì „ë¬¸ ìë£Œë¥¼ **ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬** ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ê·œì¹™:
1. **ì œê³µëœ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ** êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ ì¸ìš©í•˜ê³ , ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
3. ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ì œê³µëœ ìë£Œì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
4. ì˜í•™ì  ì¡°ì–¸ì€ ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œì¥í•˜ê³ , ì—°êµ¬ ê¸°ë°˜ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
5. ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
6. ë‹µë³€ì€ 300ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”

ì°¸ê³  ë¬¸ì„œ (ì˜í•™ ë…¼ë¬¸ ë° ì—°êµ¬ ìë£Œ):
{context}

ëŒ€í™” ê¸°ë¡:
{chat_history}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€ (ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±):""",
            input_variables=["context", "chat_history", "question"]
        )

        logger.info("âœ… RAG ì²´ì¸ ì„¤ì • ì™„ë£Œ")

    def search_relevant_docs(self, query: str, k: int = 10) -> List[Dict]:
        """ì—¬ëŸ¬ ì¸ë±ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ - ê° ì¸ë±ìŠ¤ì— ë§ëŠ” ì„ë² ë”© ì‚¬ìš©"""
        all_results = []

        for name, index_info in self.indexes.items():
            try:
                index = index_info['index']
                dimension = index_info['dimension']

                # dimensionì— ë§ëŠ” ì„ë² ë”© ìƒì„±
                if dimension == 1536:
                    # OpenAI ì„ë² ë”© ì‚¬ìš©
                    query_embedding = self.openai_embeddings.embed_query(query)
                elif dimension == 512:
                    # HuggingFace ì„ë² ë”© ì‚¬ìš© (512 dimension)
                    from langchain_huggingface import HuggingFaceEmbeddings
                    hf_embeddings = HuggingFaceEmbeddings(
                        model_name="sentence-transformers/all-mpnet-base-v2"  # 768 -> 512ë¡œ ì¡°ì • í•„ìš”
                    )
                    query_embedding = hf_embeddings.embed_query(query)
                    # 512ë¡œ ìë¥´ê¸°
                    query_embedding = query_embedding[:512]
                else:
                    logger.warning(f"{name} ì¸ë±ìŠ¤ì˜ dimension({dimension})ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                # Pinecone ì§ì ‘ ê²€ìƒ‰
                results = index.query(
                    vector=query_embedding,
                    top_k=k,
                    include_metadata=True
                )

                # ê²°ê³¼ ì¶”ê°€
                for match in results.get('matches', []):
                    metadata = match.get('metadata', {})
                    text = metadata.get('text', metadata.get('content', ''))

                    if text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                        all_results.append({
                            'content': text,
                            'metadata': metadata,
                            'source': name,
                            'score': match.get('score', 0)
                        })

            except Exception as e:
                logger.error(f"{name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                import traceback
                logger.error(traceback.format_exc())

        # ì ìˆ˜ìˆœ ì •ë ¬ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬) í›„ ìƒìœ„ kê°œ ë°˜í™˜
        all_results.sort(key=lambda x: x.get('score', 0), reverse=True)
        return all_results[:k]

    def generate_answer_with_gemini(self, question: str, context: str, chat_history: str = "") -> str:
        """Geminië¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
        try:
            prompt = self.prompt_template.format(
                context=context,
                chat_history=chat_history,
                question=question
            )

            logger.info(f"ğŸ” Gemini API í˜¸ì¶œ ì‹œì‘")
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.info(f"ğŸ“¦ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")

            response = self.gemini_model.generate_content(prompt)

            logger.info(f"âœ… Gemini API ì‘ë‹µ ë°›ìŒ")
            logger.info(f"ğŸ“¤ ì‘ë‹µ í…ìŠ¤íŠ¸: {response.text[:100] if response.text else 'None'}...")

            if response.text:
                return response.text
            else:
                logger.warning("âš ï¸ Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            logger.error(f"âŒ Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def chat(self, message: str, conversation_id: str = None) -> Dict:
        """ì±—ë´‡ ëŒ€í™” ë©”ì¸ í•¨ìˆ˜"""
        try:
            logger.info(f"ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸: {message}")

            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            logger.info("ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘...")
            relevant_docs = self.search_relevant_docs(message, k=10)
            logger.info(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")

            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ì ìˆ˜ì™€ í•¨ê»˜ ì¶œë ¥
            context_parts = []
            sources = []

            for idx, doc in enumerate(relevant_docs):
                score = doc.get('score', 0)
                logger.info(f"ğŸ“„ ë¬¸ì„œ {idx+1}: [{score:.4f}] {doc['source']} - {doc['content'][:100]}...")

                # ì œëª©ê³¼ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì‘ì„±
                title = doc['metadata'].get('title', 'ì œëª© ì—†ìŒ')
                content = doc['content']
                context_parts.append(f"[ë…¼ë¬¸ {idx+1}] ì œëª©: {title}\në‚´ìš©: {content}")

                # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                source_info = doc['metadata'].get('title', doc['source'])
                if source_info not in sources:
                    sources.append(source_info)

            context = "\n\n".join(context_parts)
            logger.info(f"ğŸ“¦ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ (ì´ {len(context)} ë¬¸ì)")
            logger.info(f"ğŸ“– ì°¸ê³  ìë£Œ: {sources}")

            # 3. ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            chat_history = ""
            if hasattr(self.memory, 'chat_memory') and self.memory.chat_memory.messages:
                history_messages = []
                for msg in self.memory.chat_memory.messages[-6:]:  # ìµœê·¼ 3í„´
                    if hasattr(msg, 'content'):
                        history_messages.append(f"{msg.__class__.__name__}: {msg.content}")
                chat_history = "\n".join(history_messages)
                logger.info(f"ğŸ’­ ëŒ€í™” ê¸°ë¡: {len(history_messages)}ê°œ ë©”ì‹œì§€")

            # 4. Geminië¡œ ë‹µë³€ ìƒì„±
            logger.info("ğŸ¤– Gemini ë‹µë³€ ìƒì„± ì‹œì‘...")
            answer = self.generate_answer_with_gemini(message, context, chat_history)
            logger.info(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {answer[:100]}...")

            # 5. ë©”ëª¨ë¦¬ì— ì €ì¥
            self.memory.save_context(
                {"question": message},
                {"answer": answer}
            )

            return {
                "response": answer,
                "sources": sources[:3],  # ìµœëŒ€ 3ê°œ ì†ŒìŠ¤
                "conversation_id": conversation_id or "default",
                "timestamp": datetime.now().isoformat(),
                "context_used": len(relevant_docs) > 0
            }

        except Exception as e:
            logger.error(f"âŒ ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "conversation_id": conversation_id or "default",
                "timestamp": datetime.now().isoformat(),
                "context_used": False
            }

    def get_health_status(self) -> Dict:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        return {
            "status": "healthy",
            "indexes": list(self.indexes.keys()),
            "index_details": {name: {"dimension": info["dimension"], "name": info["name"]} for name, info in self.indexes.items()},
            "memory_messages": len(self.memory.chat_memory.messages) if hasattr(self.memory, 'chat_memory') else 0,
            "apis": {
                "pinecone": bool(self.pinecone_api_key),
                "openai": bool(self.openai_api_key),
                "gemini": bool(self.google_api_key)
            }
        }

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
_chatbot_instance = None

def get_rag_chatbot() -> HairLossRAGChatbot:
    """RAG ì±—ë´‡ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _chatbot_instance
    if _chatbot_instance is None:
        _chatbot_instance = HairLossRAGChatbot()
    return _chatbot_instance

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    try:
        chatbot = HairLossRAGChatbot()

        # ìƒíƒœ í™•ì¸
        status = chatbot.get_health_status()
        print(f"ìƒíƒœ: {status}")

        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        test_questions = [
            "íƒˆëª¨ì˜ ì£¼ìš” ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë¯¸ë…¹ì‹œë”œì˜ íš¨ê³¼ëŠ” ì–´ë–¤ê°€ìš”?",
            "ì—¬ì„± íƒˆëª¨ì™€ ë‚¨ì„± íƒˆëª¨ì˜ ì°¨ì´ì ì€?"
        ]

        for question in test_questions:
            print(f"\nì§ˆë¬¸: {question}")
            result = chatbot.chat(question)
            print(f"ë‹µë³€: {result['response']}")
            print(f"ì¶œì²˜: {result['sources']}")

    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")