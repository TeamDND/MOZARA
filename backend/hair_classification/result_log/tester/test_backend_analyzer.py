#!/usr/bin/env python3
"""
ë°±ì—”ë“œ Hair Loss Analyzer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, classification_report,
    accuracy_score, precision_recall_fscore_support
)

# ë°±ì—”ë“œ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
backend_path = Path("C:/Users/301/Desktop/main_project/backend/hair_classification/hair_loss_rag_analyzer_v0/backend")
sys.path.append(str(backend_path))

# í•˜ë“œì½”ë”©ëœ ê²½ë¡œë¡œ config ìš°íšŒ
class HardcodedSettings:
    UPLOAD_DIR = "C:/Users/301/Desktop/main_project/backend/hair_classification/hair_loss_rag_analyzer_v0/backend/uploads"
    INDEX_NAME = "hair-loss-rag-analysis-convnext"
    EMBEDDING_DIMENSION = 1536
    MODEL_NAME = "convnext_large.fb_in22k_ft_in1k_384"

    # íƒˆëª¨ ë‹¨ê³„ ì„¤ëª…
    STAGE_DESCRIPTIONS = {
        2: "ê²½ë¯¸í•œ íƒˆëª¨ - Mì íƒˆëª¨ê°€ ì‹œì‘ë˜ê±°ë‚˜ ì´ë§ˆì„ ì´ ì•½ê°„ í›„í‡´",
        3: "ì´ˆê¸° íƒˆëª¨ - Mì íƒˆëª¨ê°€ ëšœë ·í•´ì§€ê³  ì •ìˆ˜ë¦¬ ë¶€ë¶„ ëª¨ë°œ ë°€ë„ ê°ì†Œ",
        4: "ì¤‘ê¸° íƒˆëª¨ - Mì íƒˆëª¨ ì§„í–‰, ì •ìˆ˜ë¦¬ íƒˆëª¨ ë³¸ê²©í™”",
        5: "ì§„í–‰ëœ íƒˆëª¨ - ì•ë¨¸ë¦¬ì™€ ì •ìˆ˜ë¦¬ íƒˆëª¨ê°€ ì—°ê²°ë˜ê¸° ì‹œì‘",
        6: "ì‹¬í•œ íƒˆëª¨ - ì•ë¨¸ë¦¬ì™€ ì •ìˆ˜ë¦¬ê°€ ì™„ì „íˆ ì—°ê²°ë˜ì–´ í•˜ë‚˜ì˜ í° íƒˆëª¨ ì˜ì—­ í˜•ì„±",
        7: "ë§¤ìš° ì‹¬í•œ íƒˆëª¨ - ì¸¡ë©´ê³¼ ë’·ë¨¸ë¦¬ë¥¼ ì œì™¸í•œ ëŒ€ë¶€ë¶„ì˜ ëª¨ë°œ ì†ì‹¤"
    }

# configë¥¼ ì§ì ‘ ë®ì–´ì“°ê¸°
import app.config
app.config.settings = HardcodedSettings()

from app.services.hair_loss_analyzer import HairLossAnalyzer
settings = HardcodedSettings()


def get_next_test_number(base_log_path: Path) -> int:
    """ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ë²ˆí˜¸ ê²°ì •"""
    if not base_log_path.exists():
        return 1

    existing_tests = [d for d in base_log_path.iterdir() if d.is_dir() and d.name.startswith('test')]
    if not existing_tests:
        return 1

    test_numbers = []
    for test_dir in existing_tests:
        try:
            num = int(test_dir.name.replace('test', ''))
            test_numbers.append(num)
        except ValueError:
            continue

    return max(test_numbers) + 1 if test_numbers else 1


class BackendAnalyzerTester:
    def __init__(self, test_data_path: str, base_log_path: str):
        """
        ë°±ì—”ë“œ ì• ë„ë¼ì´ì € í…ŒìŠ¤í„° ì´ˆê¸°í™”

        Args:
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê²½ë¡œ
            base_log_path: ê¸°ë³¸ ë¡œê·¸ ì €ì¥ ê²½ë¡œ
        """
        self.test_data_path = Path(test_data_path)
        self.base_log_path = Path(base_log_path)

        # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ë²ˆí˜¸ ê²°ì •
        self.test_number = get_next_test_number(self.base_log_path)
        self.result_log_path = self.base_log_path / f"test{self.test_number}"

        self.analyzer = None
        self.test_results = []

        # ê²°ê³¼ ë¡œê·¸ ë””ë ‰í„°ë¦¬ ìƒì„±
        self.result_log_path.mkdir(parents=True, exist_ok=True)

        print(f"í…ŒìŠ¤íŠ¸ ë²ˆí˜¸: {self.test_number}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {self.test_data_path}")
        print(f"ê²°ê³¼ ë¡œê·¸ ê²½ë¡œ: {self.result_log_path}")

    async def initialize_analyzer(self):
        """HairLossAnalyzer ì´ˆê¸°í™”"""
        try:
            print("HairLossAnalyzer ì´ˆê¸°í™” ì¤‘...")
            self.analyzer = HairLossAnalyzer()
            print("HairLossAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"HairLossAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def load_test_data(self) -> Dict[int, List[Path]]:
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ë ˆë²¨ 2-7)

        Returns:
            Dict[int, List[Path]]: {ë ˆë²¨: [ì´ë¯¸ì§€_ê²½ë¡œ_ë¦¬ìŠ¤íŠ¸]}
        """
        test_data = {}

        for level in range(2, 8):  # ë ˆë²¨ 2-7 í…ŒìŠ¤íŠ¸
            level_path = self.test_data_path / str(level)

            if not level_path.exists():
                print(f"ê²½ê³ : ë ˆë²¨ {level} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {level_path}")
                continue

            # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸° (ì¤‘ë³µ ì œê±°)
            image_files = []
            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                image_files.extend(list(level_path.glob(f"*{ext}")))
                image_files.extend(list(level_path.glob(f"*{ext.upper()}")))

            # ì¤‘ë³µ ì œê±° (ê°™ì€ íŒŒì¼ì˜ ëŒ€ì†Œë¬¸ì í™•ì¥ì ì¤‘ë³µ ì²˜ë¦¬)
            image_files = list(set(image_files))

            test_data[level] = image_files
            print(f"ë ˆë²¨ {level}: {len(image_files)}ê°œ ì´ë¯¸ì§€ íŒŒì¼")

        total_images = sum(len(files) for files in test_data.values())
        print(f"ì´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {total_images}ê°œ")

        return test_data

    async def test_single_image(self, image_path: Path, true_level: int) -> Dict:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸

        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            true_level: ì‹¤ì œ íƒˆëª¨ ë ˆë²¨

        Returns:
            Dict: í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path).convert('RGB')

            # ë¶„ì„ ì‹œì‘ ì‹œê°„
            start_time = time.time()

            # ë¶„ì„ ì‹¤í–‰
            result = await self.analyzer.analyze_image(image, image_path.name)

            # ë¶„ì„ ì¢…ë£Œ ì‹œê°„
            end_time = time.time()
            analysis_time = end_time - start_time

            if result['success']:
                return {
                    'image_path': str(image_path),
                    'filename': image_path.name,
                    'true_level': true_level,
                    'predicted_level': result['predicted_stage'],
                    'confidence': result['confidence'],
                    'analysis_time': analysis_time,
                    'stage_scores': result.get('stage_scores', {}),
                    'success': True,
                    'error': None
                }
            else:
                return {
                    'image_path': str(image_path),
                    'filename': image_path.name,
                    'true_level': true_level,
                    'predicted_level': None,
                    'confidence': 0.0,
                    'analysis_time': analysis_time,
                    'stage_scores': {},
                    'success': False,
                    'error': result.get('error', 'Unknown error')
                }

        except Exception as e:
            return {
                'image_path': str(image_path),
                'filename': image_path.name,
                'true_level': true_level,
                'predicted_level': None,
                'confidence': 0.0,
                'analysis_time': 0.0,
                'stage_scores': {},
                'success': False,
                'error': str(e)
            }

    async def run_tests(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ë°±ì—”ë“œ ì• ë„ë¼ì´ì € ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_data = self.load_test_data()

        if not test_data:
            print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì• ë„ë¼ì´ì € ì´ˆê¸°í™”
        if not await self.initialize_analyzer():
            return

        # ê° ë ˆë²¨ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for level, image_files in test_data.items():
            print(f"\në ˆë²¨ {level} í…ŒìŠ¤íŠ¸ ì¤‘... ({len(image_files)}ê°œ ì´ë¯¸ì§€)")

            for i, image_path in enumerate(image_files):
                print(f"  {i+1}/{len(image_files)}: {image_path.name}")

                result = await self.test_single_image(image_path, level)
                self.test_results.append(result)

                if not result['success']:
                    print(f"    ì‹¤íŒ¨: {result['error']}")
                else:
                    print(f"    ì˜ˆì¸¡: {result['predicted_level']} (ì‹ ë¢°ë„: {result['confidence']:.3f})")

        print(f"\nì´ {len(self.test_results)}ê°œ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    def calculate_metrics(self) -> Dict:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        print("\nì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")

        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ í•„í„°ë§
        successful_results = [r for r in self.test_results if r['success'] and r['predicted_level'] is not None]

        if not successful_results:
            print("ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        # ì‹¤ì œ ë ˆë²¨ê³¼ ì˜ˆì¸¡ ë ˆë²¨ ì¶”ì¶œ
        y_true = [r['true_level'] for r in successful_results]
        y_pred = [r['predicted_level'] for r in successful_results]

        # ê¸°ë³¸ ì§€í‘œ ê³„ì‚°
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='weighted')

        # í´ë˜ìŠ¤ë³„ ì§€í‘œ
        class_report = classification_report(y_true, y_pred, output_dict=True)

        # ì»¨í“¨ì „ ë©”íŠ¸ë¦­ìŠ¤
        cm = confusion_matrix(y_true, y_pred)

        # í‰ê·  ë¶„ì„ ì‹œê°„
        avg_analysis_time = np.mean([r['analysis_time'] for r in successful_results])

        # ì‹ ë¢°ë„ í†µê³„
        confidences = [r['confidence'] for r in successful_results]
        avg_confidence = np.mean(confidences)

        metrics = {
            'total_tests': len(self.test_results),
            'successful_tests': len(successful_results),
            'failed_tests': len(self.test_results) - len(successful_results),
            'success_rate': len(successful_results) / len(self.test_results),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'avg_analysis_time': avg_analysis_time,
            'avg_confidence': avg_confidence,
            'confusion_matrix': cm.tolist(),
            'class_report': class_report,
            'unique_labels': sorted(list(set(y_true + y_pred)))
        }

        return metrics

    def create_visualizations(self, metrics: Dict):
        """ì‹œê°í™” ìƒì„±"""
        print("\nì‹œê°í™” ìƒì„± ì¤‘...")

        # ì»¨í“¨ì „ ë©”íŠ¸ë¦­ìŠ¤ íˆíŠ¸ë§µ
        plt.figure(figsize=(10, 8))
        cm = np.array(metrics['confusion_matrix'])
        labels = metrics['unique_labels']

        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels)
        plt.title(f'Backend Analyzer Test{self.test_number} - Confusion Matrix')
        plt.xlabel('Predicted Level')
        plt.ylabel('True Level')
        plt.tight_layout()

        # ì €ì¥
        confusion_matrix_path = self.result_log_path / "confusion_matrix.png"
        plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')
        plt.close()

        # ì„±ëŠ¥ ì§€í‘œ ë°” ì°¨íŠ¸
        plt.figure(figsize=(10, 6))
        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        metrics_values = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1_score']]

        bars = plt.bar(metrics_names, metrics_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
        plt.ylim(0, 1)
        plt.title(f'Backend Analyzer Test{self.test_number} - Performance Metrics')
        plt.ylabel('Score')

        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, metrics_values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')

        plt.tight_layout()

        # ì €ì¥
        metrics_chart_path = self.result_log_path / "performance_metrics.png"
        plt.savefig(metrics_chart_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"ì»¨í“¨ì „ ë©”íŠ¸ë¦­ìŠ¤ ì €ì¥: {confusion_matrix_path}")
        print(f"ì„±ëŠ¥ ì§€í‘œ ì°¨íŠ¸ ì €ì¥: {metrics_chart_path}")

    def save_results(self, metrics: Dict):
        """ê²°ê³¼ ì €ì¥"""
        print("\nê²°ê³¼ ì €ì¥ ì¤‘...")

        # ë ˆë²¨ë³„ ì´ë¯¸ì§€ ìˆ˜ ê³„ì‚°
        level_counts = {}
        for result in self.test_results:
            level = result['true_level']
            level_counts[level] = level_counts.get(level, 0) + 1

        # ìƒì„¸ ê²°ê³¼ ì €ì¥ (JSON)
        detailed_results = {
            'test_info': {
                'test_type': 'backend_analyzer',
                'test_number': self.test_number,
                'timestamp': datetime.now().isoformat(),
                'test_data_path': str(self.test_data_path),
                'total_images': len(self.test_results),
                'level_counts': level_counts
            },
            'metrics': metrics,
            'detailed_results': self.test_results
        }

        json_path = self.result_log_path / "results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)

        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥ (í…ìŠ¤íŠ¸)
        report_path = self.result_log_path / "report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write(f"ë°±ì—”ë“œ Hair Loss Analyzer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ (Test{self.test_number})\n")
            f.write("=" * 60 + "\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.test_data_path}\n")
            f.write("\n")

            f.write("ğŸ“Š ì „ì²´ ê²°ê³¼\n")
            f.write("-" * 30 + "\n")
            f.write(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {metrics['total_tests']}\n")
            f.write(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {metrics['successful_tests']}\n")
            f.write(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {metrics['failed_tests']}\n")
            f.write(f"ì„±ê³µë¥ : {metrics['success_rate']:.1%}\n")
            f.write("\n")

            f.write("ğŸ“ ë ˆë²¨ë³„ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜\n")
            f.write("-" * 30 + "\n")
            for level in sorted(level_counts.keys()):
                f.write(f"ë ˆë²¨ {level}: {level_counts[level]}ê°œ íŒŒì¼\n")
            f.write("\n")

            f.write("ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ\n")
            f.write("-" * 30 + "\n")
            f.write(f"ì •í™•ë„ (Accuracy): {metrics['accuracy']:.3f}\n")
            f.write(f"ì •ë°€ë„ (Precision): {metrics['precision']:.3f}\n")
            f.write(f"ì¬í˜„ìœ¨ (Recall): {metrics['recall']:.3f}\n")
            f.write(f"F1-Score: {metrics['f1_score']:.3f}\n")
            f.write("\n")

            f.write("â±ï¸ ì„±ëŠ¥ í†µê³„\n")
            f.write("-" * 30 + "\n")
            f.write(f"í‰ê·  ë¶„ì„ ì‹œê°„: {metrics['avg_analysis_time']:.3f}ì´ˆ\n")
            f.write(f"í‰ê·  ì‹ ë¢°ë„: {metrics['avg_confidence']:.3f}\n")
            f.write("\n")

            f.write("ğŸ¯ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n")
            f.write("-" * 30 + "\n")
            for level in sorted(metrics['unique_labels']):
                level_str = str(level)
                if level_str in metrics['class_report']:
                    cr = metrics['class_report'][level_str]
                    f.write(f"ë ˆë²¨ {level}: ì •ë°€ë„={cr['precision']:.3f}, ì¬í˜„ìœ¨={cr['recall']:.3f}, F1={cr['f1-score']:.3f}\n")

        print(f"ìƒì„¸ ê²°ê³¼ ì €ì¥: {json_path}")
        print(f"ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")

        # CSVë¡œë„ ì €ì¥ (Excelì—ì„œ ì—´ê¸° ìœ„í•´)
        df = pd.DataFrame(self.test_results)
        csv_path = self.result_log_path / "results.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"CSV ë°ì´í„° ì €ì¥: {csv_path}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ë°±ì—”ë“œ Hair Loss Analyzer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    # ê²½ë¡œ ì„¤ì •
    test_data_path = "C:/Users/301/Desktop/test_data_set/test"
    base_log_path = "C:/Users/301/Desktop/main_project/backend/hair_classification/result_log/log"

    # í…ŒìŠ¤í„° ì´ˆê¸°í™” ë° ì‹¤í–‰
    tester = BackendAnalyzerTester(test_data_path, base_log_path)

    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await tester.run_tests()

        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        metrics = tester.calculate_metrics()

        if metrics:
            # ì‹œê°í™” ìƒì„±
            tester.create_visualizations(metrics)

            # ê²°ê³¼ ì €ì¥
            tester.save_results(metrics)

            print("\n" + "=" * 60)
            print(f"ë°±ì—”ë“œ ì• ë„ë¼ì´ì € ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (Test{tester.test_number})")
            print("=" * 60)
            print(f"ì •í™•ë„: {metrics['accuracy']:.1%}")
            print(f"F1-Score: {metrics['f1_score']:.3f}")
            print(f"í‰ê·  ë¶„ì„ ì‹œê°„: {metrics['avg_analysis_time']:.3f}ì´ˆ")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {tester.result_log_path}")
        else:
            print("\nì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")

    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    asyncio.run(main())