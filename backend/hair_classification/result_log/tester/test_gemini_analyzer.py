#!/usr/bin/env python3
"""
ì œë¯¸ë‚˜ì´ ë¶„ì„ê¸° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (ë¡œê·¸ì¸ ê¸°ëŠ¥ ì¶”ê°€)
"""

import os
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, classification_report,
    accuracy_score, precision_recall_fscore_support
)
import aiohttp
import aiofiles


def get_next_test_number(base_log_path: Path) -> int:
    """ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ë²ˆí˜¸ ê²°ì •"""
    if not base_log_path.exists():
        return 1

    existing_tests = [d for d in base_log_path.iterdir() if d.is_dir() and d.name.startswith('test')]
    if not existing_tests:
        return 1

    test_numbers = []
    for test_dir in existing_tests:
        try:
            num = int(test_dir.name.replace('test', ''))
            test_numbers.append(num)
        except ValueError:
            continue

    return max(test_numbers) + 1 if test_numbers else 1


class GeminiAnalyzerTester:
    def __init__(self, test_data_path: str, base_log_path: str, api_base_url: str = "http://localhost:8080"):
        """
        ì œë¯¸ë‚˜ì´ ì• ë„ë¼ì´ì € í…ŒìŠ¤í„° ì´ˆê¸°í™”
        """
        self.test_data_path = Path(test_data_path)
        self.base_log_path = Path(base_log_path)
        self.test_number = get_next_test_number(self.base_log_path)
        self.result_log_path = self.base_log_path / f"test{self.test_number}"
        self.api_base_url = api_base_url
        self.api_endpoint = f"{self.api_base_url}/api/ai/gemini-check/analyze"
        self.test_results = []
        self.jwt_token = None  # JWT í† í° ì €ì¥

        self.result_log_path.mkdir(parents=True, exist_ok=True)

        print(f"í…ŒìŠ¤íŠ¸ ë²ˆí˜¸: {self.test_number}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {self.test_data_path}")
        print(f"ê²°ê³¼ ë¡œê·¸ ê²½ë¡œ: {self.result_log_path}")
        print(f"API ì—”ë“œí¬ì¸íŠ¸: {self.api_endpoint}")

    def load_test_data(self) -> Dict[int, List[Path]]:
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ë ˆë²¨ 2-7)
        """
        test_data = {}
        for level in range(2, 8):  # ë ˆë²¨ 2-7 í…ŒìŠ¤íŠ¸
            level_path = self.test_data_path / str(level)
            if not level_path.exists():
                print(f"ê²½ê³ : ë ˆë²¨ {level} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {level_path}")
                continue
            image_files = []
            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                image_files.extend(list(level_path.glob(f"*{ext}")))
                image_files.extend(list(level_path.glob(f"*{ext.upper()}")))
            test_data[level] = image_files[:20]  # ë ˆë²¨ë‹¹ 20ê°œë¡œ ì œí•œ
            print(f"ë ˆë²¨ {level}: {len(image_files[:7])}ê°œ ì´ë¯¸ì§€ íŒŒì¼ (ì „ì²´ {len(image_files)}ê°œ ì¤‘)")
        total_images = sum(len(files) for files in test_data.values())
        print(f"ì´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {total_images}ê°œ")
        return test_data

    async def login_and_get_token(self, username, password) -> bool:
        """APIì— ë¡œê·¸ì¸í•˜ì—¬ JWT í† í°ì„ ë°œê¸‰ë°›ìŠµë‹ˆë‹¤."""
        login_url = f"{self.api_base_url}/api/login"
        credentials = {"username": username, "password": password}
        print(f"'{login_url}'ì— ë¡œê·¸ì¸ì„ ì‹œë„í•©ë‹ˆë‹¤...")

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(login_url, json=credentials, timeout=15) as response:
                    if response.status == 200:
                        auth_header = response.headers.get("Authorization")
                        if auth_header and auth_header.startswith("Bearer "):
                            self.jwt_token = auth_header
                            print("ë¡œê·¸ì¸ ì„±ê³µ. JWT í† í°ì„ ë°œê¸‰ë°›ì•˜ìŠµë‹ˆë‹¤.")
                            return True
                        else:
                            print("ë¡œê·¸ì¸ì— ì„±ê³µí–ˆì§€ë§Œ, ì‘ë‹µ í—¤ë”ì—ì„œ 'Authorization: Bearer <í† í°>'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            return False
                    else:
                        error_text = await response.text()
                        print(f"ë¡œê·¸ì¸ ì‹¤íŒ¨. ìƒíƒœ ì½”ë“œ: {response.status}, ë©”ì‹œì§€: {error_text}")
                        return False
            except Exception as e:
                print(f"ë¡œê·¸ì¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                return False

    async def test_single_image(self, session: aiohttp.ClientSession, image_path: Path, true_level: int) -> Dict:
        """ì¸ì¦ í—¤ë”ë¥¼ í¬í•¨í•˜ì—¬ ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        headers = {"Authorization": self.jwt_token}
        try:
            async with aiofiles.open(image_path, 'rb') as f:
                image_data = await f.read()

            data = aiohttp.FormData()
            data.add_field('image', image_data, filename=image_path.name, content_type='image/jpeg')

            start_time = time.time()
            async with session.post(self.api_endpoint, data=data, headers=headers, timeout=30) as response:
                end_time = time.time()
                analysis_time = end_time - start_time

                if response.status == 200:
                    result = await response.json()
                    return {
                        'image_path': str(image_path), 'filename': image_path.name, 'true_level': true_level,
                        'predicted_stage': result.get('stage'), 'gemini_title': result.get('title', ''),
                        'gemini_description': result.get('description', ''), 'gemini_advice': result.get('advice', []),
                        'analysis_time': analysis_time, 'success': True, 'error': None, 'api_status': response.status
                    }
                else:
                    error_text = await response.text()
                    return {
                        'image_path': str(image_path), 'filename': image_path.name, 'true_level': true_level,
                        'predicted_stage': None, 'analysis_time': analysis_time, 'success': False,
                        'error': f"HTTP {response.status}: {error_text}", 'api_status': response.status
                    }
        except Exception as e:
            return {
                'image_path': str(image_path), 'filename': image_path.name, 'true_level': true_level,
                'predicted_stage': None, 'analysis_time': 0.0, 'success': False,
                'error': str(e), 'api_status': None
            }

    async def run_tests(self, username, password):
        """ë¡œê·¸ì¸ ê³¼ì •ì„ í¬í•¨í•˜ì—¬ ì „ì²´ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ì œë¯¸ë‚˜ì´ ë¶„ì„ê¸° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        if not await self.login_and_get_token(username, password):
            print("ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ ì¸í•´ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

        test_data = self.load_test_data()
        if not test_data:
            print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        async with aiohttp.ClientSession() as session:
            for level, image_files in test_data.items():
                print(f"\në ˆë²¨ {level} í…ŒìŠ¤íŠ¸ ì¤‘... ({len(image_files)}ê°œ ì´ë¯¸ì§€)")
                for i, image_path in enumerate(image_files):
                    print(f"  {i+1}/{len(image_files)}: {image_path.name}")
                    result = await self.test_single_image(session, image_path, level)
                    self.test_results.append(result)
                    if not result['success']:
                        print(f"    ì‹¤íŒ¨: {result['error']}")
                    else:
                        print(f"    ì˜ˆì¸¡: Stage {result['predicted_stage']} - {result['gemini_title']}")
                    await asyncio.sleep(10)  # í• ë‹¹ëŸ‰ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ 10ì´ˆ ëŒ€ê¸°

        print(f"\nì´ {len(self.test_results)}ê°œ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    def map_gemini_to_backend_level(self, gemini_stage: int) -> int:
        """ì œë¯¸ë‚˜ì´ stageë¥¼ ë°±ì—”ë“œ ë ˆë²¨ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤."""
        mapping = {0: 2, 1: 3, 2: 5, 3: 7}
        return mapping.get(gemini_stage, gemini_stage + 2)

    def calculate_metrics(self) -> Dict:
        """ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        print("\nì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        successful_results = [r for r in self.test_results if r['success'] and r['predicted_stage'] is not None]
        if not successful_results:
            print("ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        y_true = [r['true_level'] for r in successful_results]
        y_pred_raw = [r['predicted_stage'] for r in successful_results]
        y_pred_mapped = [self.map_gemini_to_backend_level(stage) for stage in y_pred_raw]

        # ì›ë³¸ ì œë¯¸ë‚˜ì´ stageë¡œ ê³„ì‚°
        accuracy_raw, precision_raw, recall_raw, f1_raw = (0,0,0,0)
        cm_raw, class_report_raw = np.array([[0]]), {}
        try:
            accuracy_raw = accuracy_score(y_true, y_pred_raw)
            precision_raw, recall_raw, f1_raw, _ = precision_recall_fscore_support(y_true, y_pred_raw, average='weighted', zero_division=0)
            cm_raw = confusion_matrix(y_true, y_pred_raw)
            class_report_raw = classification_report(y_true, y_pred_raw, output_dict=True, zero_division=0)
        except Exception as e:
            print(f"ì›ë³¸ Stage ì§€í‘œ ê³„ì‚° ì˜¤ë¥˜: {e}")


        # ë§¤í•‘ëœ ë ˆë²¨ë¡œ ê³„ì‚°
        accuracy_mapped, precision_mapped, recall_mapped, f1_mapped = (0,0,0,0)
        cm_mapped, class_report_mapped = np.array([[0]]), {}
        try:
            accuracy_mapped = accuracy_score(y_true, y_pred_mapped)
            precision_mapped, recall_mapped, f1_mapped, _ = precision_recall_fscore_support(y_true, y_pred_mapped, average='weighted', zero_division=0)
            cm_mapped = confusion_matrix(y_true, y_pred_mapped)
            class_report_mapped = classification_report(y_true, y_pred_mapped, output_dict=True, zero_division=0)
        except Exception as e:
            print(f"ë§¤í•‘ëœ Level ì§€í‘œ ê³„ì‚° ì˜¤ë¥˜: {e}")

        status_counts = {}
        for result in self.test_results:
            status = result.get('api_status', 'unknown')
            status_counts[status] = status_counts.get(status, 0) + 1

        return {
            'total_tests': len(self.test_results),
            'successful_tests': len(successful_results),
            'failed_tests': len(self.test_results) - len(successful_results),
            'success_rate': len(successful_results) / len(self.test_results) if self.test_results else 0,
            'raw_metrics': {
                'accuracy': accuracy_raw, 'precision': precision_raw, 'recall': recall_raw, 'f1_score': f1_raw,
                'confusion_matrix': cm_raw.tolist(), 'class_report': class_report_raw,
                'unique_labels': sorted(list(set(y_true + y_pred_raw)))
            },
            'mapped_metrics': {
                'accuracy': accuracy_mapped, 'precision': precision_mapped, 'recall': recall_mapped, 'f1_score': f1_mapped,
                'confusion_matrix': cm_mapped.tolist(), 'class_report': class_report_mapped,
                'unique_labels': sorted(list(set(y_true + y_pred_mapped)))
            },
            'avg_analysis_time': np.mean([r['analysis_time'] for r in successful_results]) if successful_results else 0,
            'api_status_counts': status_counts
        }

    def create_visualizations(self, metrics: Dict):
        """ì‹œê°í™” ìë£Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("\nì‹œê°í™” ìƒì„± ì¤‘...")
        plt.figure(figsize=(12, 5))
        plt.subplot(1, 2, 1)
        cm_raw = np.array(metrics['raw_metrics']['confusion_matrix'])
        labels_raw = metrics['raw_metrics']['unique_labels']
        if cm_raw.size > 1:
            sns.heatmap(cm_raw, annot=True, fmt='d', cmap='Blues', xticklabels=labels_raw, yticklabels=labels_raw)
        plt.title(f'Gemini Test{self.test_number} - Raw Stage Prediction')
        plt.xlabel('Predicted Stage (Gemini 0-3)')
        plt.ylabel('True Level (Backend 2-7)')

        plt.subplot(1, 2, 2)
        cm_mapped = np.array(metrics['mapped_metrics']['confusion_matrix'])
        labels_mapped = metrics['mapped_metrics']['unique_labels']
        if cm_mapped.size > 1:
            sns.heatmap(cm_mapped, annot=True, fmt='d', cmap='Greens', xticklabels=labels_mapped, yticklabels=labels_mapped)
        plt.title(f'Gemini Test{self.test_number} - Mapped Level Prediction')
        plt.xlabel('Predicted Level (Mapped 2-7)')
        plt.ylabel('True Level (Backend 2-7)')

        plt.tight_layout()
        confusion_matrix_path = self.result_log_path / "confusion_matrix.png"
        plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ì»¨í“¨ì „ ë©”íŠ¸ë¦­ìŠ¤ ì €ì¥: {confusion_matrix_path}")

    def save_results(self, metrics: Dict):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        print("\nê²°ê³¼ ì €ì¥ ì¤‘...")

        # ë ˆë²¨ë³„ ì´ë¯¸ì§€ ìˆ˜ ê³„ì‚°
        level_counts = {}
        for result in self.test_results:
            level = result['true_level']
            level_counts[level] = level_counts.get(level, 0) + 1

        detailed_results = {
            'test_info': {
                'test_type': 'gemini_analyzer', 'test_number': self.test_number,
                'timestamp': datetime.now().isoformat(), 'test_data_path': str(self.test_data_path),
                'api_endpoint': self.api_endpoint, 'total_images': len(self.test_results),
                'level_counts': level_counts
            },
            'metrics': metrics, 'detailed_results': self.test_results
        }
        json_path = self.result_log_path / "results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        print(f"ìƒì„¸ ê²°ê³¼(JSON) ì €ì¥: {json_path}")

        report_path = self.result_log_path / "report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write(f"ì œë¯¸ë‚˜ì´ Hair Loss Analyzer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ (Test{self.test_number})\n")
            f.write("=" * 60 + "\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.test_data_path}\n")
            f.write(f"API ì—”ë“œí¬ì¸íŠ¸: {self.api_endpoint}\n")
            f.write("\n")

            f.write("ğŸ“Š ì „ì²´ ê²°ê³¼\n")
            f.write("-" * 30 + "\n")
            f.write(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {metrics['total_tests']}\n")
            f.write(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {metrics['successful_tests']}\n")
            f.write(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {metrics['failed_tests']}\n")
            f.write(f"ì„±ê³µë¥ : {metrics['success_rate']:.1%}\n")
            f.write("\n")

            f.write("ğŸ“ ë ˆë²¨ë³„ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜\n")
            f.write("-" * 30 + "\n")
            for level in sorted(level_counts.keys()):
                f.write(f"ë ˆë²¨ {level}: {level_counts[level]}ê°œ íŒŒì¼\n")
            f.write("\n")

            f.write("ğŸ“ˆ ì›ë³¸ Stage ì„±ëŠ¥ ì§€í‘œ\n")
            f.write("-" * 30 + "\n")
            f.write(f"ì •í™•ë„: {metrics['raw_metrics']['accuracy']:.3f}\n")
            f.write(f"ì •ë°€ë„: {metrics['raw_metrics']['precision']:.3f}\n")
            f.write(f"ì¬í˜„ìœ¨: {metrics['raw_metrics']['recall']:.3f}\n")
            f.write(f"F1-Score: {metrics['raw_metrics']['f1_score']:.3f}\n")
            f.write("\n")

            f.write("ğŸ“ˆ ë§¤í•‘ëœ Level ì„±ëŠ¥ ì§€í‘œ\n")
            f.write("-" * 30 + "\n")
            f.write(f"ì •í™•ë„: {metrics['mapped_metrics']['accuracy']:.3f}\n")
            f.write(f"ì •ë°€ë„: {metrics['mapped_metrics']['precision']:.3f}\n")
            f.write(f"ì¬í˜„ìœ¨: {metrics['mapped_metrics']['recall']:.3f}\n")
            f.write(f"F1-Score: {metrics['mapped_metrics']['f1_score']:.3f}\n")
            f.write("\n")

            f.write("â±ï¸ ì„±ëŠ¥ í†µê³„\n")
            f.write("-" * 30 + "\n")
            f.write(f"í‰ê·  ë¶„ì„ ì‹œê°„: {metrics['avg_analysis_time']:.3f}ì´ˆ\n")
            f.write("\n")
        print(f"ìš”ì•½ ë¦¬í¬íŠ¸(TXT) ì €ì¥: {report_path}")

        df = pd.DataFrame(self.test_results)
        csv_path = self.result_log_path / "results.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"CSV ë°ì´í„° ì €ì¥: {csv_path}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ì œë¯¸ë‚˜ì´ Hair Loss Analyzer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    test_data_path = "C:/Users/301/Desktop/test_data_set/test"
    base_log_path = "C:/Users/301/Desktop/main_project/backend/hair_classification/result_log/log"
    
    tester = GeminiAnalyzerTester(test_data_path, base_log_path)

    try:
        await tester.run_tests(username="aaaaaa", password="11111111")
        metrics = tester.calculate_metrics()
        if metrics:
            tester.create_visualizations(metrics)
            tester.save_results(metrics)
            print("\n" + "=" * 60)
            print(f"ì œë¯¸ë‚˜ì´ ì• ë„ë¼ì´ì € ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (Test{tester.test_number})")
            print("=" * 60)
            print(f"ì›ë³¸ Stage ì •í™•ë„: {metrics['raw_metrics']['accuracy']:.1%}")
            print(f"ì›ë³¸ Stage F1-Score: {metrics['raw_metrics']['f1_score']:.3f}")
            print(f"ë§¤í•‘ëœ Level ì •í™•ë„: {metrics['mapped_metrics']['accuracy']:.1%}")
            print(f"ë§¤í•‘ëœ Level F1-Score: {metrics['mapped_metrics']['f1_score']:.3f}")
            print(f"í‰ê·  ë¶„ì„ ì‹œê°„: {metrics['avg_analysis_time']:.3f}ì´ˆ")
            print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {tester.result_log_path}")
        else:
            print("\nì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
    except Exception as e:
        print(f"\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    asyncio.run(main())
