#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ConvNeXt v0 + FAISS ë°ì´í„°ì…‹ ê²€ì¦ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- analyzer_v0 í´ë”ì˜ FAISS ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
- 2+ ë ˆë²¨ ì°¨ì´ ì´ë¯¸ì§€ ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
- Geminiì™€ ë™ì¼í•œ ê²€ì¦ í•­ëª© ë¶„ì„
"""

import os
import sys
import json
import time
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from sklearn.metrics import (
    confusion_matrix, classification_report,
    accuracy_score, precision_recall_fscore_support
)

# ConvNeXt v0 ë°±ì—”ë“œ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
v0_backend_path = Path("C:/Users/301/Desktop/main_project/backend/hair_classification/hair_loss_rag_analyzer_v0/backend")
sys.path.append(str(v0_backend_path))

from app.services.hair_loss_analyzer import HairLossAnalyzer


def get_next_test_number(base_log_path: Path, prefix: str = "convnext_v0_validation") -> int:
    if not base_log_path.exists():
        return 1
    existing = [d for d in base_log_path.iterdir() if d.is_dir() and d.name.startswith(prefix)]
    if not existing:
        return 1
    nums = []
    for d in existing:
        try:
            nums.append(int(d.name.replace(prefix, '')))
        except:
            pass
    return max(nums) + 1 if nums else 1


class ConvNeXtV0ValidationTester:
    def __init__(self, test_data_path: str, base_log_path: str):
        self.test_data_path = Path(test_data_path)
        self.base_log_path = Path(base_log_path)
        self.test_number = get_next_test_number(self.base_log_path, "convnext_v0_validation")
        self.result_log_path = self.base_log_path / f"convnext_v0_validation{self.test_number}"
        self.test_results: List[Dict] = []
        self.analyzer = None
        self.result_log_path.mkdir(parents=True, exist_ok=True)

        print(f"ConvNeXt v0 ê²€ì¦ í…ŒìŠ¤íŠ¸ ë²ˆí˜¸: {self.test_number}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {self.test_data_path}")
        print(f"ê²°ê³¼ ë¡œê·¸ ê²½ë¡œ: {self.result_log_path}")
        print(f"ì‚¬ìš© ëª¨ë¸: ConvNeXt v0 + FAISS")

    def load_test_data(self) -> Dict[int, List[Path]]:
        """ê° ë ˆë²¨ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë¡œë“œ"""
        test_data: Dict[int, List[Path]] = {}

        for level in range(2, 8):
            level_path = self.test_data_path / str(level)
            if not level_path.exists():
                print(f"ê²½ê³ : ë ˆë²¨ {level} í´ë” ì—†ìŒ: {level_path}")
                continue

            images: List[Path] = []
            for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.JPG', '.JPEG', '.PNG', '.BMP']:
                images.extend(list(level_path.glob(f"*{ext}")))
            images = sorted(list(set(images)))

            test_data[level] = images
            print(f"ë ˆë²¨ {level}: {len(images)}ê°œ ì´ë¯¸ì§€")

        total = sum(len(v) for v in test_data.values())
        print(f"ì´ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {total}ê°œ")
        return test_data

    async def initialize_analyzer(self):
        """ConvNeXt v0 Analyzer ì´ˆê¸°í™”"""
        try:
            print("ConvNeXt v0 Analyzer ì´ˆê¸°í™” ì¤‘...")
            self.analyzer = HairLossAnalyzer()

            # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
            db_info = self.analyzer.get_database_info()
            if not db_info['success']:
                print(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {db_info.get('error')}")
                return False

            print(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {db_info['total_vectors']}ê°œ ë²¡í„°")
            return True

        except Exception as e:
            print(f"Analyzer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def test_single_image(self, image_path: Path, true_level: int) -> Dict:
        """ë‹¨ì¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path).convert('RGB')

            # ë¶„ì„ ì‹œì‘ ì‹œê°„
            start_time = time.time()

            # ConvNeXt v0 + FAISS ë¶„ì„ (LLM ë¹„í™œì„±í™”)
            result = await self.analyzer.analyze_image(image, image_path.name, use_llm=False)

            # ë¶„ì„ ì¢…ë£Œ ì‹œê°„
            end_time = time.time()
            analysis_time = end_time - start_time

            if result['success']:
                predicted_level = result['predicted_stage']
                level_difference = abs(true_level - predicted_level)

                return {
                    'success': True,
                    'filename': image_path.name,
                    'image_path': str(image_path),
                    'true_level': true_level,
                    'predicted_stage': predicted_level,
                    'confidence': result['confidence'],
                    'analysis_time': analysis_time,
                    'stage_scores': result.get('stage_scores', {}),
                    'similar_images': result.get('similar_images', []),
                    'level_difference': level_difference,
                    'stage_description': result.get('stage_description', ''),
                    'method': result.get('analysis_details', {}).get('method', 'faiss_only')
                }
            else:
                return {
                    'success': False,
                    'filename': image_path.name,
                    'image_path': str(image_path),
                    'true_level': true_level,
                    'predicted_stage': None,
                    'confidence': 0.0,
                    'analysis_time': analysis_time,
                    'error': result.get('error', 'Unknown error'),
                    'level_difference': None
                }

        except Exception as e:
            return {
                'success': False,
                'filename': image_path.name,
                'image_path': str(image_path),
                'true_level': true_level,
                'predicted_stage': None,
                'confidence': 0.0,
                'analysis_time': 0.0,
                'error': str(e),
                'level_difference': None
            }

    async def run_tests(self):
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ConvNeXt v0 ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        test_data = self.load_test_data()
        if not test_data:
            print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # Analyzer ì´ˆê¸°í™”
        if not await self.initialize_analyzer():
            print("Analyzer ì´ˆê¸°í™” ì‹¤íŒ¨. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

        count = 0
        total = sum(len(v) for v in test_data.values())

        for level, images in test_data.items():
            print(f"\në ˆë²¨ {level} í…ŒìŠ¤íŠ¸ ì§„í–‰ ({len(images)}ì¥)")
            for i, image_path in enumerate(images):
                count += 1
                print(f"  [{count}/{total}] {i+1}/{len(images)}: {image_path.name}")
                result = await self.test_single_image(image_path, level)
                self.test_results.append(result)

                if not result['success']:
                    print(f"    ì‹¤íŒ¨: {result.get('error')}")
                else:
                    diff = result['level_difference']
                    print(f"    ì˜ˆì¸¡: {result['predicted_stage']} (ì°¨ì´: {diff}, ì‹ ë¢°ë„: {result['confidence']:.3f})")

        print(f"\nì´ {len(self.test_results)}ì¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    def calculate_metrics(self) -> Dict:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        print("\nì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
        successful = [r for r in self.test_results if r['success'] and r.get('predicted_stage') is not None]

        if not successful:
            print("ì„±ê³µ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        y_true = [r['true_level'] for r in successful]
        y_pred = [r['predicted_stage'] for r in successful]

        acc = accuracy_score(y_true, y_pred)
        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)
        cm = confusion_matrix(y_true, y_pred, labels=[2,3,4,5,6,7])
        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
        avg_time = np.mean([r['analysis_time'] for r in successful])
        avg_confidence = np.mean([r['confidence'] for r in successful])

        return {
            'total_tests': len(self.test_results),
            'successful_tests': len(successful),
            'failed_tests': len(self.test_results) - len(successful),
            'success_rate': len(successful) / len(self.test_results) if self.test_results else 0,
            'accuracy': acc,
            'precision': prec,
            'recall': rec,
            'f1_score': f1,
            'confusion_matrix': cm.tolist(),
            'class_report': report,
            'unique_labels': sorted(list(set(y_true + y_pred))),
            'avg_analysis_time': avg_time,
            'avg_confidence': avg_confidence
        }

    def generate_validation_report(self) -> Dict:
        """2+ ë ˆë²¨ ì°¨ì´ ì´ë¯¸ì§€ ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± (Geminiì™€ ë™ì¼í•œ í˜•ì‹)"""
        print("\nê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

        successful = [r for r in self.test_results if r['success'] and r.get('predicted_stage') is not None]

        # 2+ ë ˆë²¨ ì°¨ì´ ì´ë¯¸ì§€ í•„í„°ë§
        problematic_images = []
        for result in successful:
            diff = result.get('level_difference', 0)
            if diff >= 2:
                # ì´ë¯¸ì§€ íŠ¹ì§• ë¶„ì„ (íŒŒì¼ëª… ê¸°ë°˜)
                filename = result['filename'].lower()
                suspected_features = []

                # íŒŒì¼ëª…ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
                if 'front' in filename:
                    suspected_features.append("ì •ë©´ê°ë„")
                if 'top' in filename:
                    suspected_features.append("ìƒë‹¨ê°ë„")
                if 'side' in filename or 'left' in filename or 'right' in filename:
                    suspected_features.append("ì¸¡ë©´ê°ë„")
                if 'back' in filename:
                    suspected_features.append("í›„ë©´ê°ë„")
                if 'male' in filename:
                    suspected_features.append("ë‚¨ì„±")
                if 'female' in filename:
                    suspected_features.append("ì—¬ì„±")

                # FAISS ìœ ì‚¬ ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
                similar_images_info = result.get('similar_images', [])
                if similar_images_info:
                    top_similar = similar_images_info[0]
                    suspected_features.append(f"ìµœìœ ì‚¬: Level {top_similar.get('stage')} (ê±°ë¦¬: {top_similar.get('distance', 0):.3f})")

                # ë ˆë²¨ ì°¨ì´ íŒ¨í„´ ë¶„ì„
                if result['true_level'] < result['predicted_stage']:
                    error_pattern = f"ê³¼ëŒ€í‰ê°€ ({result['true_level']} â†’ {result['predicted_stage']})"
                else:
                    error_pattern = f"ê³¼ì†Œí‰ê°€ ({result['true_level']} â†’ {result['predicted_stage']})"

                problematic_images.append({
                    'filename': result['filename'],
                    'image_path': result['image_path'],
                    'true_level': result['true_level'],
                    'predicted_level': result['predicted_stage'],
                    'level_difference': diff,
                    'error_pattern': error_pattern,
                    'confidence': result['confidence'],
                    'stage_description': result.get('stage_description', ''),
                    'suspected_features': suspected_features,
                    'analysis_time': result['analysis_time'],
                    'similar_images_count': len(similar_images_info),
                    'stage_scores': result.get('stage_scores', {})
                })

        # ì—ëŸ¬ íŒ¨í„´ í†µê³„
        error_patterns = {}
        feature_patterns = {}
        confidence_stats = {'low_confidence': 0, 'high_confidence': 0}

        for img in problematic_images:
            pattern = img['error_pattern']
            error_patterns[pattern] = error_patterns.get(pattern, 0) + 1

            for feature in img['suspected_features']:
                feature_patterns[feature] = feature_patterns.get(feature, 0) + 1

            # ì‹ ë¢°ë„ ë¶„ì„
            if img['confidence'] < 0.5:
                confidence_stats['low_confidence'] += 1
            else:
                confidence_stats['high_confidence'] += 1

        validation_report = {
            'total_problematic': len(problematic_images),
            'problematic_rate': len(problematic_images) / len(successful) if successful else 0,
            'problematic_images': problematic_images,
            'error_patterns': error_patterns,
            'feature_patterns': feature_patterns,
            'confidence_stats': confidence_stats,
            'validation_guidelines': self.get_validation_guidelines()
        }

        return validation_report

    def get_validation_guidelines(self) -> Dict:
        """ë°ì´í„°ì…‹ ê²€ì¦ ê°€ì´ë“œë¼ì¸ (ConvNeXt íŠ¹í™”)"""
        return {
            "ìˆ˜ë™_ê²€ì¦_ì²´í¬ë¦¬ìŠ¤íŠ¸": [
                "1. í—¤ì–´ë¼ì¸ íŒ¨í„´: Mì íƒˆëª¨ ì •ë„ê°€ ë ˆë²¨ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?",
                "2. ì •ìˆ˜ë¦¬ ìƒíƒœ: ì •ìˆ˜ë¦¬ íƒˆëª¨ ì§„í–‰ë„ê°€ ì ì ˆí•œê°€?",
                "3. ì „ì²´ ëª¨ë°œ ë°€ë„: ì „ë°˜ì ì¸ ëª¨ë°œ ì–‘ì´ ë ˆë²¨ì— ë§ëŠ”ê°€?",
                "4. ì´ë¯¸ì§€ í’ˆì§ˆ: ì¡°ëª…, ê°ë„, í•´ìƒë„ê°€ ë¶„ì„ì— ì í•©í•œê°€?",
                "5. ì„±ë³„ ì¼ì¹˜: ë‚¨ì„±/ì—¬ì„± ë¶„ë¥˜ê°€ ì •í™•í•œê°€?",
                "6. ë·°í¬ì¸íŠ¸ ì¼ì¹˜: ì´¬ì˜ ê°ë„ê°€ ë¼ë²¨ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?",
                "7. FAISS ìœ ì‚¬ë„: ê²€ìƒ‰ëœ ìœ ì‚¬ ì´ë¯¸ì§€ë“¤ì´ ì ì ˆí•œê°€?"
            ],
            "ì˜ì‹¬_ì¼€ì´ìŠ¤_ìš°ì„ ìˆœìœ„": [
                "1. 3ë ˆë²¨ ì´ìƒ ì°¨ì´ë‚˜ëŠ” ì´ë¯¸ì§€ (ìµœìš°ì„ )",
                "2. ë‚®ì€ ì‹ ë¢°ë„(<0.5)ì—ì„œ í° ì°¨ì´ ë‚˜ëŠ” ì´ë¯¸ì§€",
                "3. ê³¼ëŒ€í‰ê°€ëœ ì´ë¯¸ì§€ (ì‹¤ì œë³´ë‹¤ ë†’ê²Œ í‰ê°€)",
                "4. FAISS ìœ ì‚¬ ì´ë¯¸ì§€ê°€ ì ì€ ì¼€ì´ìŠ¤",
                "5. ì •ë©´ê°ë„ì—ì„œ ì˜¤ë¶„ë¥˜ëœ ì´ë¯¸ì§€"
            ],
            "ConvNeXt_íŠ¹í™”_ë¶„ì„": [
                "1. ì„ë² ë”© í’ˆì§ˆ: ConvNeXtê°€ ì¶”ì¶œí•œ íŠ¹ì§•ì´ íƒˆëª¨ íŒ¨í„´ì„ ì˜ í‘œí˜„í•˜ëŠ”ê°€?",
                "2. FAISS ê²€ìƒ‰ ê²°ê³¼: ìœ ì‚¬í•œ íƒˆëª¨ íŒ¨í„´ì˜ ì´ë¯¸ì§€ë“¤ì´ ê²€ìƒ‰ë˜ëŠ”ê°€?",
                "3. ë‹¨ê³„ë³„ ë¶„í¬: stage_scoresê°€ ì‹¤ì œ íƒˆëª¨ ì§„í–‰ë„ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?",
                "4. ê²½ê³„ ì¼€ì´ìŠ¤: ë ˆë²¨ ê°„ ê²½ê³„ì— ìˆëŠ” ì• ë§¤í•œ ì¼€ì´ìŠ¤ë“¤ì˜ ë¶„ë¥˜ í’ˆì§ˆ"
            ],
            "ì¬ë¼ë²¨ë§_ê¸°ì¤€": [
                "1. ì „ë¬¸ê°€ 2ëª… ì´ìƒì˜ í•©ì˜",
                "2. ë…¸ìš°ë“œ ë¶„ë¥˜ ê¸°ì¤€í‘œ ì¬ì°¸ì¡°",
                "3. ConvNeXtê°€ ì¼ê´€ë˜ê²Œ ë‹¤ë¥´ê²Œ ë¶„ë¥˜í•˜ëŠ” íŒ¨í„´ ë¶„ì„",
                "4. FAISS ìœ ì‚¬ ì´ë¯¸ì§€ë“¤ê³¼ì˜ ì¼ê´€ì„± í™•ì¸"
            ],
            "ë°ì´í„°ì…‹_ê°œì„ _ì œì•ˆ": [
                "1. ConvNeXt ì„ë² ë”© í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ë°ì´í„° ì¦ê°•",
                "2. ê²½ê³„ ì¼€ì´ìŠ¤ ì¶”ê°€ ìˆ˜ì§‘",
                "3. ê°ë„ë³„ ê· ë“±í•œ ë¶„í¬ í™•ë³´",
                "4. FAISS ì¸ë±ìŠ¤ ìµœì í™” ê³ ë ¤"
            ]
        }

    def create_visualizations(self, metrics: Dict, validation_report: Dict):
        """ì‹œê°í™” ìƒì„±"""
        print("\nì‹œê°í™” ìƒì„± ì¤‘...")

        # 1. Confusion Matrix
        plt.figure(figsize=(10, 8))
        cm = np.array(metrics['confusion_matrix'])
        labels = [2,3,4,5,6,7]
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
        plt.xlabel('Predicted Level')
        plt.ylabel('True Level')
        plt.title(f'ConvNeXt v0 Validation Test{self.test_number} - Confusion Matrix')
        confusion_path = self.result_log_path / 'confusion_matrix.png'
        plt.tight_layout()
        plt.savefig(confusion_path, dpi=300, bbox_inches='tight')
        plt.close()

        # 2. ì„±ëŠ¥ ì§€í‘œ
        plt.figure(figsize=(10, 6))
        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        metrics_values = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1_score']]
        bars = plt.bar(metrics_names, metrics_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
        plt.ylim(0, 1)
        plt.title(f'ConvNeXt v0 Validation Test{self.test_number} - Performance Metrics')
        plt.ylabel('Score')
        for bar, value in zip(bars, metrics_values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')
        plt.tight_layout()
        metrics_path = self.result_log_path / 'performance_metrics.png'
        plt.savefig(metrics_path, dpi=300, bbox_inches='tight')
        plt.close()

        # 3. ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
        if validation_report['error_patterns']:
            plt.figure(figsize=(12, 6))
            patterns = list(validation_report['error_patterns'].keys())
            counts = list(validation_report['error_patterns'].values())
            plt.bar(patterns, counts, color='lightcoral')
            plt.title(f'Error Patterns in Problematic Images (ConvNeXt v0 Test{self.test_number})')
            plt.ylabel('Count')
            plt.xticks(rotation=45)
            plt.tight_layout()
            error_path = self.result_log_path / 'error_patterns.png'
            plt.savefig(error_path, dpi=300, bbox_inches='tight')
            plt.close()

        # 4. ì‹ ë¢°ë„ ë¶„ì„
        plt.figure(figsize=(8, 5))
        conf_stats = validation_report['confidence_stats']
        labels = ['Low Confidence\n(<0.5)', 'High Confidence\n(â‰¥0.5)']
        values = [conf_stats['low_confidence'], conf_stats['high_confidence']]
        colors = ['#ff6b6b', '#4ecdc4']
        plt.bar(labels, values, color=colors)
        plt.title(f'Confidence Distribution in Problematic Images (Test{self.test_number})')
        plt.ylabel('Count')
        plt.tight_layout()
        conf_path = self.result_log_path / 'confidence_analysis.png'
        plt.savefig(conf_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {self.result_log_path}")

    def save_results(self, metrics: Dict, validation_report: Dict):
        """ê²°ê³¼ ì €ì¥"""
        print("\nê²°ê³¼ ì €ì¥ ì¤‘...")

        # ë ˆë²¨ë³„ í†µê³„
        level_counts = {}
        for r in self.test_results:
            level = r['true_level']
            level_counts[level] = level_counts.get(level, 0) + 1

        # ìƒì„¸ ê²°ê³¼ JSON
        detailed_results = {
            'test_info': {
                'test_type': 'convnext_v0_validation',
                'test_number': self.test_number,
                'timestamp': datetime.now().isoformat(),
                'test_data_path': str(self.test_data_path),
                'model_type': 'ConvNeXt v0 + FAISS',
                'total_images': len(self.test_results),
                'level_counts': level_counts
            },
            'metrics': metrics,
            'validation_report': validation_report,
            'detailed_results': self.test_results
        }

        json_path = self.result_log_path / 'results.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)

        # ê²€ì¦ ë¦¬í¬íŠ¸ (í…ìŠ¤íŠ¸)
        report_path = self.result_log_path / 'validation_report.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ConvNeXt v0 + FAISS ë°ì´í„°ì…‹ ê²€ì¦ ë¦¬í¬íŠ¸ (Test{self.test_number})\n")
            f.write("=" * 80 + "\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.test_data_path}\n")
            f.write(f"ì‚¬ìš© ëª¨ë¸: ConvNeXt v0 + FAISS\n\n")

            f.write("ğŸ“Š ì „ì²´ ì„±ëŠ¥\n")
            f.write("-" * 40 + "\n")
            f.write(f"ì´ í…ŒìŠ¤íŠ¸: {metrics['total_tests']}\n")
            f.write(f"ì„±ê³µ: {metrics['successful_tests']}\n")
            f.write(f"ì‹¤íŒ¨: {metrics['failed_tests']}\n")
            f.write(f"ì •í™•ë„: {metrics['accuracy']:.3f}\n")
            f.write(f"F1-Score: {metrics['f1_score']:.3f}\n")
            f.write(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {metrics['avg_analysis_time']:.3f}ì´ˆ\n")
            f.write(f"í‰ê·  ì‹ ë¢°ë„: {metrics['avg_confidence']:.3f}\n\n")

            f.write("ğŸš¨ ê²€ì¦ í•„ìš” ì´ë¯¸ì§€\n")
            f.write("-" * 40 + "\n")
            f.write(f"ë¬¸ì œ ì´ë¯¸ì§€ ìˆ˜: {validation_report['total_problematic']}\n")
            f.write(f"ë¬¸ì œ ë¹„ìœ¨: {validation_report['problematic_rate']:.1%}\n\n")

            f.write("ğŸ“‹ ê²€ì¦ ëŒ€ìƒ ìƒì„¸ ëª©ë¡\n")
            f.write("-" * 40 + "\n")
            for img in validation_report['problematic_images']:
                f.write(f"íŒŒì¼ëª…: {img['filename']}\n")
                f.write(f"  ì‹¤ì œ: Level {img['true_level']} â†’ ì˜ˆì¸¡: Level {img['predicted_level']}\n")
                f.write(f"  ì°¨ì´: {img['level_difference']}ë ˆë²¨\n")
                f.write(f"  íŒ¨í„´: {img['error_pattern']}\n")
                f.write(f"  ì‹ ë¢°ë„: {img['confidence']:.3f}\n")
                f.write(f"  ìœ ì‚¬ ì´ë¯¸ì§€ ìˆ˜: {img['similar_images_count']}\n")
                f.write(f"  ì˜ì‹¬ íŠ¹ì§•: {', '.join(img['suspected_features'])}\n")
                f.write(f"  ê²½ë¡œ: {img['image_path']}\n\n")

            f.write("ğŸ“ˆ ì—ëŸ¬ íŒ¨í„´ ë¶„ì„\n")
            f.write("-" * 40 + "\n")
            for pattern, count in validation_report['error_patterns'].items():
                f.write(f"{pattern}: {count}ê±´\n")
            f.write("\n")

            f.write("ğŸ” ì‹ ë¢°ë„ ë¶„ì„\n")
            f.write("-" * 40 + "\n")
            conf_stats = validation_report['confidence_stats']
            f.write(f"ë‚®ì€ ì‹ ë¢°ë„(<0.5): {conf_stats['low_confidence']}ê±´\n")
            f.write(f"ë†’ì€ ì‹ ë¢°ë„(â‰¥0.5): {conf_stats['high_confidence']}ê±´\n\n")

            f.write("ğŸ” ConvNeXt v0 ë°ì´í„°ì…‹ ê²€ì¦ ê°€ì´ë“œë¼ì¸\n")
            f.write("-" * 40 + "\n")
            guidelines = validation_report['validation_guidelines']

            f.write("ìˆ˜ë™ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n")
            for item in guidelines['ìˆ˜ë™_ê²€ì¦_ì²´í¬ë¦¬ìŠ¤íŠ¸']:
                f.write(f"  {item}\n")
            f.write("\n")

            f.write("ì˜ì‹¬ ì¼€ì´ìŠ¤ ìš°ì„ ìˆœìœ„:\n")
            for item in guidelines['ì˜ì‹¬_ì¼€ì´ìŠ¤_ìš°ì„ ìˆœìœ„']:
                f.write(f"  {item}\n")
            f.write("\n")

            f.write("ConvNeXt íŠ¹í™” ë¶„ì„:\n")
            for item in guidelines['ConvNeXt_íŠ¹í™”_ë¶„ì„']:
                f.write(f"  {item}\n")
            f.write("\n")

            f.write("ì¬ë¼ë²¨ë§ ê¸°ì¤€:\n")
            for item in guidelines['ì¬ë¼ë²¨ë§_ê¸°ì¤€']:
                f.write(f"  {item}\n")
            f.write("\n")

            f.write("ë°ì´í„°ì…‹ ê°œì„  ì œì•ˆ:\n")
            for item in guidelines['ë°ì´í„°ì…‹_ê°œì„ _ì œì•ˆ']:
                f.write(f"  {item}\n")

        # CSV ì €ì¥
        df = pd.DataFrame(self.test_results)
        csv_path = self.result_log_path / 'results.csv'
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # ë¬¸ì œ ì´ë¯¸ì§€ë§Œ ë³„ë„ CSV
        if validation_report['problematic_images']:
            problem_df = pd.DataFrame(validation_report['problematic_images'])
            problem_csv_path = self.result_log_path / 'problematic_images.csv'
            problem_df.to_csv(problem_csv_path, index=False, encoding='utf-8-sig')
            print(f"ë¬¸ì œ ì´ë¯¸ì§€ ëª©ë¡: {problem_csv_path}")

        print(f"ìƒì„¸ ê²°ê³¼: {json_path}")
        print(f"ê²€ì¦ ë¦¬í¬íŠ¸: {report_path}")
        print(f"ê²°ê³¼ CSV: {csv_path}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ConvNeXt v0 + FAISS ë°ì´í„°ì…‹ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 80)
    print("ëª©ì : 2+ ë ˆë²¨ ì°¨ì´ ì´ë¯¸ì§€ ê²€ì¦ ë° ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„")
    print("ëª¨ë¸: ConvNeXt v0 + FAISS (analyzer_v0)")
    print("=" * 80)

    # ê²½ë¡œ ì„¤ì •
    test_data_path = "C:/Users/301/Desktop/test_data_set/test"
    base_log_path = "C:/Users/301/Desktop/main_project/backend/hair_classification/result_log/log"

    # í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = ConvNeXtV0ValidationTester(test_data_path, base_log_path)

    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì „ì²´)
        await tester.run_tests()

        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        metrics = tester.calculate_metrics()

        if metrics:
            # ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
            validation_report = tester.generate_validation_report()

            # ì‹œê°í™” ìƒì„±
            tester.create_visualizations(metrics, validation_report)

            # ê²°ê³¼ ì €ì¥
            tester.save_results(metrics, validation_report)

            print("\n" + "=" * 80)
            print(f"ConvNeXt v0 ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! (Test{tester.test_number})")
            print("=" * 80)
            print(f"ì •í™•ë„: {metrics['accuracy']:.1%}")
            print(f"ë¬¸ì œ ì´ë¯¸ì§€: {validation_report['total_problematic']}ê°œ ({validation_report['problematic_rate']:.1%})")
            print(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {metrics['avg_analysis_time']:.3f}ì´ˆ")
            print(f"í‰ê·  ì‹ ë¢°ë„: {metrics['avg_confidence']:.3f}")
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {tester.result_log_path}")
            print("\nğŸš¨ ìˆ˜ë™ ê²€ì¦ í•„ìš” ì´ë¯¸ì§€ë“¤ì„ problematic_images.csvì—ì„œ í™•ì¸í•˜ì„¸ìš”!")
        else:
            print("\nì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")

    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    asyncio.run(main())