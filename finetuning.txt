# 파인튜닝 (Fine-tuning) - ImageNet-22K Transfer Learning

## 핵심 개념
이미 학습된 Swin Transformer 모델을 우리의 탈모 분석 문제에 맞게 재활용하는 기법

---

## 1. 사전학습 모델 (Pre-trained Model)

### 사용 모델
- **모델명**: Swin Transformer (Shifted Window Transformer)
- **사전학습 데이터**: ImageNet-22K
  - 22,000개 클래스 (일반 객체 인식)
  - 약 14,000,000개의 이미지
  - 일반적인 시각적 패턴 학습 완료 (엣지, 텍스처, 형태 등)

### 왜 Swin Transformer를 선택했나요?
- **Vision Transformer 기반**: CNN보다 장거리 의존성 학습에 유리
- **계층적 구조**: 다양한 스케일의 패턴 인식 가능
- **Window Attention**: 계산 효율성 우수
- **전이학습 성능**: ImageNet 사전학습 모델의 전이학습 성능이 검증됨

---

## 2. 파인튜닝 vs 처음부터 학습

### 성능 비교표
| 방식 | 학습 시간 | 정확도 | 필요 데이터 |
|------|----------|--------|------------|
| 처음부터 학습 | **20시간** | 89.3% | 매우 많음 (수만장) |
| 파인튜닝 | **8시간** | **97.95%** | 적음 (수천장) |

### 파인튜닝의 장점
1. **학습 시간 60% 단축** (20h → 8h)
2. **정확도 8.65%p 향상** (89.3% → 97.95%)
3. **데이터 효율성**: 적은 데이터로도 높은 성능
4. **과적합 방지**: 일반화된 특징 활용

---

## 3. 기술적 도전과제: 3채널 → 6채널 확장

### 문제 상황
- 기존 Swin Transformer는 **RGB 3채널** 입력만 처리 가능
- 우리 프로젝트는 **RGB 3채널 + 헤어 마스크 3채널 = 6채널** 필요

### 해결 방법: 가중치 복사 및 스케일링
1. 기존 3채널 가중치를 복사하여 6채널로 확장
2. 각 가중치를 0.5배로 스케일링하여 균형 유지
3. 이유: 입력이 2배가 되었으므로 활성화 값의 크기를 원래대로 유지해야 안정적인 학습 가능

### 왜 0.5배를 곱하나요?
- RGB 3채널에서 학습된 가중치가 특정 크기의 활성화 값을 만들도록 학습됨
- 6채널로 단순 복사하면 활성화 값이 2배로 커짐
- 0.5배 스케일링으로 원래의 활성화 크기 유지 → 안정적인 학습

---

## 4. Layer-wise Learning Rate (레이어별 학습률 전략)

### 핵심 아이디어
모델의 각 부분을 **다른 학습률**로 학습

### 학습률 설정
- **앞쪽 레이어**: 0.00001 (매우 낮음)
  - 엣지, 코너, 텍스처 같은 범용 특징 추출
  - ImageNet에서 이미 잘 학습됨 → 크게 바꿀 필요 없음

- **중간 레이어**: 0.00005 (중간)
  - 중간 수준의 패턴 인식

- **뒤쪽 레이어**: 0.001 (높음)
  - 탈모 패턴, 모발 밀도 같은 도메인 특화 특징
  - 우리 데이터로 새로 학습 필요

### 왜 이렇게 설정하나요?
- **앞부분**: 선의 방향, 색상 대비 등 기본 형태는 이미 완성
- **뒷부분**: M자 헤어라인, 정수리 패턴 등 탈모 특화 특징은 새로 배워야 함

---

## 5. 학습 스케줄링: Cosine Annealing with Warmup

### Warmup (5 Epoch)
- **목적**: 갑작스러운 가중치 변화 방지
- **과정**: 학습률을 매우 낮게 시작해서 점진적으로 목표 학습률까지 증가
  - Epoch 1: 0.00001 (매우 낮게 시작)
  - Epoch 5: 0.001 (목표 학습률 도달)

### Cosine Annealing (Epoch 6~100)
- **목적**: 학습률을 부드럽게 감소시켜 정밀 조정
- **과정**: 코사인 곡선을 따라 학습률이 점진적으로 감소
  - Epoch 6: 최대 (0.001)
  - Epoch 50: 중간 (0.00005)
  - Epoch 100: 최소 (0.000001)

### 왜 Cosine 곡선을 사용하나요?
- **부드러운 감소**: 급격한 학습률 변화 없이 점진적 감소
- **미세 조정 가능**: 마지막에 매우 작은 학습률로 정밀 조정
- **지역 최적점 탈출**: 초기 높은 학습률로 넓게 탐색 → 후기 낮은 학습률로 정밀 조정

---

## 6. 학습 결과

### TOP 모델 (정수리 분석)
- **Best Epoch**: 68/100
- **정확도**: **97.95%**
- **학습 시간**: 약 8시간
- **데이터**: 3,200장 (4개 클래스)

### SIDE 모델 (M자 분석)
- **Best Epoch**: 72/100
- **정확도**: **83.61%**
- **학습 시간**: 약 7.5시간
- **데이터**: 2,800장 (4개 클래스)

### 왜 SIDE 모델이 더 낮나요?
1. **패턴 복잡도**: M자 헤어라인은 개인차가 큼
2. **데이터 불균형**: 정수리보다 데이터 수집 어려움
3. **애매한 경계**: M자 진행 단계가 주관적

---

## 7. 예상 질문 & 답변

### Q1: 파인튜닝과 전이학습의 차이는?
**A**: 사실상 같은 의미입니다. 전이학습이 큰 개념이고, 파인튜닝은 전이학습을 구현하는 구체적인 방법입니다.

### Q2: ImageNet으로 학습한 모델이 탈모 분석에도 효과적인 이유는?
**A**:
- ImageNet에서 학습한 저수준 특징(엣지, 텍스처)은 모든 이미지 분석에 공통적으로 필요
- 뒷부분 레이어만 재학습하여 탈모 특화 고수준 특징 추출
- 실험 결과, 처음부터 학습(89.3%)보다 8.65%p 높은 성능

### Q3: 6채널로 확장할 때 왜 단순 복사를 사용했나요?
**A**:
- 단순 복사 + 스케일링: 기존 학습된 패턴 활용 → 빠른 수렴
- 랜덤 초기화: 처음부터 학습 필요 → 학습 시간 증가
- 실험 결과, 복사 방식이 5-10%p 더 높은 성능

### Q4: Warmup을 왜 사용하나요?
**A**:
- 사전학습 모델의 가중치는 ImageNet에 최적화되어 있음
- 갑자기 높은 학습률로 학습하면 좋은 가중치가 망가질 수 있음
- Warmup으로 천천히 시작하여 우리 데이터에 부드럽게 적응

### Q5: Best Epoch이 68인데 왜 100 Epoch까지 학습했나요?
**A**:
- Early Stopping 미사용: 과적합 방지를 위해 Best 모델 저장
- 최적점 탐색: 후반부에 더 나은 성능이 나올 가능성 대비
- 학습 곡선 분석: 전체 학습 과정 관찰 필요
- 실제로는 Epoch 68 모델을 사용

### Q6: 다른 모델은 시도해봤나요?
**A**: 네, 여러 모델을 비교했습니다.
- ResNet-50: 91.2% (CNN 기반, 장거리 의존성 약함)
- EfficientNet-B3: 93.7% (효율적이지만 탈모 패턴 인식 부족)
- Swin Transformer: 97.95% (Window Attention이 머리카락 패턴 분석에 최적)

### Q7: 학습 데이터는 몇 장이고 어떻게 구성되어 있나요?
**A**:
- TOP 모델: 3,200장 (Stage 0~3 각각 700-850장)
- SIDE 모델: 2,800장 (비슷한 비율)
- 데이터 증강: 회전, 반전, 색상 변화, 랜덤 크롭 등

### Q8: 과적합은 어떻게 방지했나요?
**A**:
1. 데이터 증강으로 다양성 확보
2. Dropout 0.2 적용
3. Weight Decay (L2 정규화 0.01)
4. Layer-wise LR로 앞부분 레이어 거의 고정
5. Validation 모니터링으로 Best 모델 저장

---

## 코드 위치
- 모델 로딩 및 6채널 확장: `swin_transformer.py:150-180`
- Layer-wise LR 설정: `train_model.py:220-250`
- Cosine Annealing: `train_model.py:280-310`
- 학습 루프: `train_model.py:350-450`

---

## 참고 자료
- Swin Transformer 논문: "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" (ICCV 2021)
- ImageNet-22K: https://www.image-net.org/
- PyTorch Transfer Learning Guide: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html
